[{"categories":["Kubernetes"],"content":"本篇文章主要介绍k8s集群中证书的作用位置，并且以创建pod和exec为例说明证书调用，最后写了部署监控证书有效期的步骤。 ","date":"2023-05-09","objectID":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/:0:0","tags":["Kubernetes"],"title":"k8s证书说明及更新方式","uri":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/"},{"categories":["Kubernetes"],"content":"证书概述 在 Kubernetes 中包含多个以独立进程形式运行的组件，这些组件之间通过 HTTPS/gRPC 相互通信，以协同完成集群中应用的部署和管理工作。 k8s多master架构 从图中可以看到，Kubernetes控制平面中包含了etcd，kube-apiserver，kube-scheduler，kube-controller-manager等组件，这些组件会相互进行远程调用，例如 kube-apiserver会调用etcd接口查询或数据，kube-controller-manager会调用kube-apiserver接口查询集群中的对象状态；同时kube-apiserver也会和在工作节点上的kubelet和kube-proxy进行通信，以在工作节点上部署和管理应用。 ","date":"2023-05-09","objectID":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/:0:1","tags":["Kubernetes"],"title":"k8s证书说明及更新方式","uri":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/"},{"categories":["Kubernetes"],"content":"证书路径及用途 集群CA证书 签发者(CN) 路径 作用 kubernetes /etc/kubernetes/ssl/ca.crt Kubernetes 通用 CA etcd-ca /etc/ssl/etcd/ssl/ca.pem 与 etcd 相关的所有功能 front-proxy-ca /etc/kubernetes/ssl/front-proxy-ca.crt 用于前端代理证书签署 服务端证书 签发者 证书路径 启动命令 相关参数或环境变量 用途 etcd-ca /etc/ssl/etcd/ssl/member-.pem /usr/local/bin/etcd ETCD_CERT_FILE 提供etcd集群的加密访问 kubernetes /etc/kubernetes/ssl/apiserver.crt kube-apiserver --tls-cert-file 提供kube-apiserver的加密访问 -ca@[UnixTimeStamp] /var/lib/kubelet/pki/kubelet.crt 无 无 节点自签证书，提供kubelet的加密访问，过期不影响集群运行 客户端证书 签发者 证书路径 启动命令 相关参数或环境变量 用途 etcd-ca /etc/ssl/etcd/ssl/node-[NodeName].pem kube-apiserver --etcd-certfile kube-apiserver访问etcd的客户端证书 kubernetes /etc/kubernetes/ssl/apiserver-kubelet-client.crt kube-apiserver --kubelet-client-certificate kube-apiserver访问kubelet的客户端证书 front-proxy-ca /etc/kubernetes/ssl/front-proxy-client.crt kube-apiserver --proxy-client-cert-file 前端代理的客户端证书，只有需要支持扩展API服务器时才需要该证书 服务账户证书(另一种形式的客户端证书) 签发者 文件路径 启动命令 相关参数或环境变量 用途 kubernetes /etc/kubernetes/admin.conf kubectl 无 提供给集群管理员操作集群时使用，一般需要复制到$HOME/.kube/config kubernetes /etc/kubernetes/controller-manager.conf kube-controller-manager --authentication-kubeconfig 提供给kube-controller-manager组件访问kube-apiserver时使用 kubernetes /etc/kubernetes/scheduler.conf kube-scheduler --authentication-kubeconfig 提供给kube-scheduler组件访问kube-apiserver时使用 kubernetes /etc/kubenernetes/kubelet.conf kubelet --kubeconfig 提供给kubelet组件访问kube-apiserver时使用 注：kubelet.conf中的证书文件实际指向/var/lib/kubelet/pki/kubelet-client-current.pem，该文件是链接文件在kubelet证书滚动更新后自动指向新的文件。 其他证书 签发者 文件路径 启动命令 相关参数或环境变量 用途 etcd-ca /etc/ssl/etcd/ssl/admin-.pem etcdctl --cert 提供给管理员直接访问/操作etcd的证书 ","date":"2023-05-09","objectID":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/:0:2","tags":["Kubernetes"],"title":"k8s证书说明及更新方式","uri":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/"},{"categories":["Kubernetes"],"content":"证书交互过程 以创建Pod和执行kubectl exec为例说明证书交互的过程 以创建Pod为例 步骤解析： 用户通过kubectl提交资源创建请求，此时使用的集群管理证书即$HOME/.kube/config访问kube-apiserver kube-apiserver使用客户端证书保存资源到etcd中，此时使用的证书为/etc/ssl/etcd/ssl/node-.pem kube-controller-manager检查更新，此时使用kube-controller-manager的客户端证书，即/etc/kubernetes/controller-manager.conf中内嵌的证书 kube-scheduler观察到需要调度新的Pod，使用客户端证书连接到kube-apiserver，证书位于/etc/kubernetes/scheduler.conf中内嵌的证书 kube-scheduler通过各种算法计算出调度的节点，计算完成后提交给kube-apiserver，此时使用的证书同第4步中证书 kube-apiserver保存pod与节点的映射到etcd中，此时使用的证书同第2步中的证书 kubelet观察到需要在本节点创建新的Pod，证书位于/etc/kubernetes/kubelet.conf中内嵌的证书 kube-apiserver绑定pod到对应节点的kubelet，此时使用的证书位于/etc/kubernetes/ssl/apiserver-kubelet-client.crt kubelet通知容器运行时创建pod 创建完成后kubelet向kube-apiserver更新状态，证书同第8步 kube-apiserver保存最新的状态到etcd中，证书同第2步 以执行kubectl exec为例 步骤解析： 用户执行kubectl exec子命令请求进入容器中执行指令，此时使用的集群管理证书即$HOME/.kube/config访问kube-apiserver kube-apiserver使用客户端证书访问kubelet，证书位于/etc/kubernetes/ssl/apiserver-kubelet-client.crt kubelet通过gRPC接口访问容器运行时，无需证书 容器运行时返回执行结果给kubelet kubelet返回执行结果给kube-apiserver，此时使用kubelet客户端证书访问kube-apiserver，证书位于/etc/kubernetes/kubelet.conf中内嵌的证书 呈现执行的结果 ","date":"2023-05-09","objectID":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/:0:3","tags":["Kubernetes"],"title":"k8s证书说明及更新方式","uri":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/"},{"categories":["Kubernetes"],"content":"证书有效期监控 为防止证书过期造成集群不可用，可部署exporter采集各个证书的有效期并且对小于7天的证书进行告警及时通知集群管理员更新证书。 部署ssl_exporter 部署类型为daemonset apiVersion: apps/v1 kind: DaemonSet metadata: labels: app: ssl-exporter name: ssl-exporter spec: selector: matchLabels: app: ssl-exporter template: metadata: labels: app: ssl-exporter spec: containers: - image: reg.kolla.org/library/ssl-exporter:2.4.2 imagePullPolicy: IfNotPresent name: ssl-exporter resources: limits: cpu: 250m memory: 256Mi requests: cpu: 100m memory: 128Mi volumeMounts: - mountPath: /host/etc/kubernetes name: k8s-certs readOnly: true - mountPath: /host/var/lib/kubelet/pki name: kubelet-certs readOnly: true hostNetwork: true volumes: - hostPath: path: /etc/kubernetes type: \"\" name: k8s-certs - hostPath: path: /var/lib/kubelet/pki type: \"\" name: kubelet-certs 配置Prometheus抓取规则 由于服务证书暂不能使用通配符，各个服务账户证书需手动配置策略。 scrape_configs: - job_name: \"ssl-k8s-file\" metrics_path: /probe params: module: [\"file\"] target: [\"/host/etc/kubernetes/ssl/*.crt\"] kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: ^(.*):(.*)$ target_label: __address__ replacement: ${1}:9219 - job_name: \"ssl-kubelet-file\" metrics_path: /probe params: module: [\"file\"] target: - \"/host/var/lib/kubelet/pki/*.crt\" kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: ^(.*):(.*)$ target_label: __address__ replacement: ${1}:9219 - job_name: \"kubeconfig-admin\" metrics_path: /probe params: module: [\"kubeconfig\"] target: [\"/host/etc/kubernetes/admin.conf\"] kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: ^(.*):(.*)$ target_label: __address__ replacement: ${1}:9219 - job_name: \"kubeconfig-controller-manager\" metrics_path: /probe params: module: [\"kubeconfig\"] target: [\"/host/etc/kubernetes/controller-manager.conf\"] kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: ^(.*):(.*)$ target_label: __address__ replacement: ${1}:9219 - job_name: \"kubeconfig-kubelet\" metrics_path: /probe params: module: [\"kubeconfig\"] target: [\"/host/etc/kubernetes/kubelet.conf\"] kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: ^(.*):(.*)$ target_label: __address__ replacement: ${1}:9219 - job_name: \"kubeconfig-scheduler\" metrics_path: /probe params: module: [\"kubeconfig\"] target: [\"/host/etc/kubernetes/scheduler.conf\"] kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__address__] regex: ^(.*):(.*)$ target_label: __address__ replacement: ${1}:9219 配置Prometheus告警策略 groups: - name: check_ssl_validity rules: - alert: \"K8S集群证书在7天后过期\" expr: (ssl_file_cert_not_after - time()) / 86400 \u003c 7 for: 1h labels: severity: critical annotations: description: 'K8S集群的证书还有{{ printf \"%.1f\" $value }}天就过期了,请尽快更新证书' summary: \"K8S集群证书证书过期警告\" 配置Grafana面板 服务账户证书展示 ssl证书展示 ","date":"2023-05-09","objectID":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/:0:4","tags":["Kubernetes"],"title":"k8s证书说明及更新方式","uri":"/posts/k8s%E8%AF%81%E4%B9%A6%E8%AF%B4%E6%98%8E%E5%8F%8A%E6%9B%B4%E6%96%B0%E6%96%B9%E5%BC%8F/"},{"categories":["Linux"],"content":"概述 堡垒机4A模型 ","date":"2022-05-10","objectID":"/posts/jumpserver%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/:0:1","tags":["Linux"],"title":"Jumpserver使用手册","uri":"/posts/jumpserver%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"},{"categories":["Linux"],"content":"安装 需求配置 4核/16G,推荐200G以上硬盘。主要用来存储审计录像，审计录像每小时大约为10M，按每天操作4小时并且保留30天计算为127GB。 虚拟机方式 直接安装官网链接:https://docs.jumpserver.org/zh/master/install/setup_by_fast/ -\u003e 在线安装 -\u003e 手动部署 k8s方式 官网链接：https://docs.jumpserver.org/zh/master/install/setup_by_fast/ -\u003e 在线安装 -\u003e helm 部署外置MySQL 简化流程，采用单节点部署 apiVersion: v1 kind: Secret metadata: name: mysql-pass namespace: jumpserver type: Opaque data: rootpassword: \"dl50WkRlYmNTQmhrRU5CIw==\" --- apiVersion: v1 kind: Service metadata: name: mysql-for-jumpserver namespace: jumpserver labels: app: mysql spec: ports: - protocol: TCP port: 3306 targetPort: 3306 selector: app: mysql --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim namespace: jumpserver labels: app: mysql spec: storageClassName: rbd-immediate accessModes: - ReadWriteOnce resources: requests: storage: 10Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql-for-jumpserver namespace: jumpserver labels: app: mysql spec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: reg.kolla.org/library/mysql:5.7.39 name: mysql livenessProbe: exec: command: - /bin/bash - -c - 'mysqladmin ping -p${MYSQL_ROOT_PASSWORD}' initialDelaySeconds: 30 timeoutSeconds: 1 periodSeconds: 10 readinessProbe: exec: command: - /bin/bash - -c - 'mysql -p${MYSQL_ROOT_PASSWORD} -e \"SELECT 1\"' initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 1 lifecycle: postStart: exec: command: [\"/bin/sh\",\"-c\",\"rm -rf /var/lib/mysql/lost+found\"] env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: rootpassword ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim 等Pod处于Ready状态后，进入Pod创建Databasejumpserver: create database jumpserver default charset 'utf8' collate 'utf8_bin'; 部署外置Redis apiVersion: v1 kind: ConfigMap metadata: name: redis-for-jumpserver-config namespace: jumpserver data: redis.conf: | bind 0.0.0.0 protected-mode yes tcp-backlog 1024 requirepass \"sw@1Wq(jN\" timeout 0 tcp-keepalive 300 databases 16 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename dump.rdb rdb-del-sync-files no dir /var/lib/redis oom-score-adj no oom-score-adj-values 0 200 800 disable-thp yes appendonly yes appendfilename \"appendonly.aof\" appendfsync everysec --- apiVersion: v1 kind: Service metadata: name: redis-for-jumpserver namespace: jumpserver labels: app: redis spec: ports: - protocol: TCP port: 6379 targetPort: 6379 selector: app: redis --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: redis-pv-claim namespace: jumpserver labels: app: redis spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi --- apiVersion: apps/v1 kind: Deployment metadata: name: redis-for-jumpserver namespace: jumpserver labels: app: redis spec: selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - image: reg.kolla.org/library/redis:7.0.4 name: redis command: [\"redis-server\"] args: - /opt/redis.conf ports: - containerPort: 6379 name: redis volumeMounts: - name: redis-persistent-storage mountPath: /var/lib/redis - name: config mountPath: \"/opt\" volumes: - name: redis-persistent-storage persistentVolumeClaim: claimName: redis-pv-claim - name: config configMap: name: redis-for-jumpserver-config items: - key: redis.conf path: redis.conf 安装jumpserver 添加helm仓库:helm repo add jumpserver https://jumpserver.github.io/helm-charts 下载jumpserver的chart文件:helm pull jumpserver/jumpserver --untar 按需修改values.yaml 推送镜像到内网仓库 修改ReadWriteMany-\u003eReadWriteOnce,如果storageclass支持多读多写可不做修改。审计视频存储于 按需添加资源限制 ingress实现若不是使用的nginx-ingress，则按需修改 在values.yaml同级目录执行安装: helm install jumpserver . -n jumpserver -f value.yaml ","date":"2022-05-10","objectID":"/posts/jumpserver%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/:0:2","tags":["Linux"],"title":"Jumpserver使用手册","uri":"/posts/jumpserver%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"},{"categories":["Linux"],"content":"使用 创建用户 管理员：特权用户。创建各类账户、资产、MFA、授权等等，也可用于审计操作 普通用户：日常登陆到jumpserver进行资产访问的用户 审计员：对普通用户操作进行审计的专用账户 点击创建,创建两个账户: 普通用户testuser:testuser123与审计员testauditor:testauditor123 创建特权用户 特权用户 是资产已存在的, 并且拥有 高级权限 的系统用户， 如 root 或 拥有 NOPASSWD: ALL sudo 权限的用户。 JumpServer 使用该用户来 推送系统用户、获取资产硬件信息 等。 管理用户：资产上的root用户或者是具有NOPASSWD:ALL权限的普通用户，jumpserver使用该用户创建系统用户或者获取硬件信息。 系统用户：在实际运维工作中，给使用人员所分配的登陆资产的系统用户，可由jumpserver自动推送，由jumpserver统一进行密码管理与更新。 创建资产 创建授权 将用户与资产进行权限关联 其中节点为一种类似于标签选择器的匹配方式，所有新建的节点均位于Default组下 测试登陆 webterminal方式 使用新建的普通用户testuser登陆到jumpserver中,右上角有一个webterminal选项，点击后跳转到webterminal界面。由于在授权时仅授予了172.16.200.123一个节点，故在登陆后仅能看到这一台节点。 ssh方式 ssh方式需要将koko组件的2222端口暴露到公网，采用ingress或者nodeport方式。使用ssh testuser@[ip] -p [port]方式登陆 密码管理 审计 审计可对日常的操作进行回放审计 登陆管理员或者审计员账户，切换到审计台视图进行操作审计 ","date":"2022-05-10","objectID":"/posts/jumpserver%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/:0:3","tags":["Linux"],"title":"Jumpserver使用手册","uri":"/posts/jumpserver%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"},{"categories":["Go"],"content":"前言 最近在学习gRPC，在生成服务端证书与客户端实现双向加密中遇到了证书错误，特此记录下过程及解决方法。 ","date":"2021-04-28","objectID":"/posts/grpc%E4%BD%BF%E7%94%A8%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6%E5%AE%9E%E7%8E%B0%E5%8F%8C%E5%90%91%E5%8A%A0%E5%AF%86/:0:1","tags":["Go"],"title":"gRPC使用自签证书实现双向加密","uri":"/posts/grpc%E4%BD%BF%E7%94%A8%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6%E5%AE%9E%E7%8E%B0%E5%8F%8C%E5%90%91%E5%8A%A0%E5%AF%86/"},{"categories":["Go"],"content":"错误 transport: authentication handshake failed: x509: certificate relies on legacy Common Name field, use S ANs or temporarily enable Common Name matching with GODEBUG=x509ignoreCN=0。从错误来看是因为证书中依赖传统的CN(Common Name)字段，而Go自1.15版本废弃了common name方式认证，需要使用SANS证书进行认证，或者设置Go环境变量GODEBUG=x509ignoreCN=0。 ","date":"2021-04-28","objectID":"/posts/grpc%E4%BD%BF%E7%94%A8%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6%E5%AE%9E%E7%8E%B0%E5%8F%8C%E5%90%91%E5%8A%A0%E5%AF%86/:0:2","tags":["Go"],"title":"gRPC使用自签证书实现双向加密","uri":"/posts/grpc%E4%BD%BF%E7%94%A8%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6%E5%AE%9E%E7%8E%B0%E5%8F%8C%E5%90%91%E5%8A%A0%E5%AF%86/"},{"categories":["Go"],"content":"解决 生成CA证书 生成CA私钥，长度为4096位 $ openssl genrsa -out ca.key 4096 生成CA公钥 $ openssl req -x509 -new -nodes -sha512 -days 3650 \\ -subj \"/O=example/CN=test.com\" \\ -key ca.key \\ -out ca.crt 生成服务端证书 生成服务端私钥 $ openssl genrsa -out server.key 4096 生成服务端证书认证请求 $ openssl req -sha512 -new \\ -subj \"/O=example/CN=test.com\" \\ -key server.key \\ -out server.csr 生成服务端x509 v3扩展文件 $ cat \u003e v3-server.ext \u003c\u003c-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [alt_names] DNS.1=test.com DNS.2=www.test.com EOF 生成服务端公钥 $ openssl x509 -req -sha512 -days 3650 \\ -extfile v3-server.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in server.csr \\ -out server.crt 生成客户端证书 生成客户端私钥 $ openssl genrsa -out client.key 4096 生成客户端证书认证请求 $ openssl req -sha512 -new \\ -subj \"/O=example/CN=test.com\" \\ -key client.key \\ -out client.csr 生成客户端x509 v3扩展文件 $ cat \u003e v3-client.ext \u003c\u003c-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = clientAuth subjectAltName = @alt_names [alt_names] DNS.1=test.com DNS.2=www.test.com EOF 该文件与客户端证书唯一不同就在于extendedKeyUsage，服务端是serverAuth,客户端为clientAuth。 生成客户端公钥 $ openssl x509 -req -sha512 -days 3650 \\ -extfile v3-client.ext \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in client.csr \\ -out client.crt ","date":"2021-04-28","objectID":"/posts/grpc%E4%BD%BF%E7%94%A8%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6%E5%AE%9E%E7%8E%B0%E5%8F%8C%E5%90%91%E5%8A%A0%E5%AF%86/:0:3","tags":["Go"],"title":"gRPC使用自签证书实现双向加密","uri":"/posts/grpc%E4%BD%BF%E7%94%A8%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6%E5%AE%9E%E7%8E%B0%E5%8F%8C%E5%90%91%E5%8A%A0%E5%AF%86/"},{"categories":["Go"],"content":"应用 拷贝上述生成的ca.crt,server.crt,server.key,client.crt,client.key文件到项目的certs目录中。服务端及客户端代码示例： 服务端 cert,err:=tls.LoadX509KeyPair(\"certs/server.crt\",\"certs/server.key\") if err!=nil{ log.Fatal(err) } certPool:=x509.NewCertPool() ca,err:=ioutil.ReadFile(\"certs/ca.crt\") if err!=nil{ log.Fatal(err) } certPool.AppendCertsFromPEM(ca) cred:=credentials.NewTLS(\u0026tls.Config{ Certificates: []tls.Certificate{cert}, ClientAuth: tls.RequireAndVerifyClientCert, ClientCAs: certPool, }) 客户端 cert,_:=tls.LoadX509KeyPair(\"certs/client.crt\",\"certs/client.key\") certPool:=x509.NewCertPool() ca,_:=ioutil.ReadFile(\"certs/ca.crt\") certPool.AppendCertsFromPEM(ca) cred:=credentials.NewTLS(\u0026tls.Config{ Certificates: []tls.Certificate{cert}, ServerName: \"test.com\", //alt_names文件中的任意域名一致 RootCAs: certPool, }) ","date":"2021-04-28","objectID":"/posts/grpc%E4%BD%BF%E7%94%A8%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6%E5%AE%9E%E7%8E%B0%E5%8F%8C%E5%90%91%E5%8A%A0%E5%AF%86/:0:4","tags":["Go"],"title":"gRPC使用自签证书实现双向加密","uri":"/posts/grpc%E4%BD%BF%E7%94%A8%E8%87%AA%E7%AD%BE%E8%AF%81%E4%B9%A6%E5%AE%9E%E7%8E%B0%E5%8F%8C%E5%90%91%E5%8A%A0%E5%AF%86/"},{"categories":["Kubernetes"],"content":"概念 相对于水平自动扩缩容(HPA)在pod资源紧张时扩充pod个数来平衡负载。Pod的垂直扩容会自动调整Pod资源申请的requests值及limits值，它会依据pod当前运行状况动态地为Pod资源申请CPU及内存使用量。解放了手动设置request值及limits值的难点，使Pod运行更加智能。目前为Beta阶段 垂直扩缩容项目地址位于：https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler ","date":"2021-03-11","objectID":"/posts/%E5%88%9D%E8%AF%95pod%E5%9E%82%E7%9B%B4%E6%89%A9%E7%BC%A9%E5%AE%B9vpa/:0:1","tags":["Kubernetes"],"title":"初试Pod垂直扩缩容VPA","uri":"/posts/%E5%88%9D%E8%AF%95pod%E5%9E%82%E7%9B%B4%E6%89%A9%E7%BC%A9%E5%AE%B9vpa/"},{"categories":["Kubernetes"],"content":"安装VPA控制器 与HPA一样，VPA在运行时的指标同样是由Metrics Server 提供,所以安装VPA控制器前首先先运行好Metrics Server 目前最新版本为0.9，考虑到部署0.9版本需要升级OpenSSL。本次实验采用0.8版本的部署包。 下载部署文件 $ wget https://codeload.github.com/kubernetes/autoscaler/tar.gz/vertical-pod-autoscaler-0.8.0 解压后进入vertical-pod-autoscaler目录 $ cd vertical-pod-autoscaler 批量修改镜像地址为registry.cn-shanghai.aliyuncs.com/ltzhang $ sed -i 's@k8s.gcr.io/@registry.cn-shanghai.aliyuncs.com/ltzhang/@g' `egrep -r \"\\\u003cimage\\\u003e\" deploy/ | awk -F: '{print $1}'` 安装VPA控制器 $ ./hack/vpa-up.sh customresourcedefinition.apiextensions.k8s.io/verticalpodautoscalers.autoscaling.k8s.io created customresourcedefinition.apiextensions.k8s.io/verticalpodautoscalercheckpoints.autoscaling.k8s.io created clusterrole.rbac.authorization.k8s.io/system:metrics-reader created clusterrole.rbac.authorization.k8s.io/system:vpa-actor created ... deployment.apps/vpa-admission-controller created service/vpa-webhook created 安装完成后会生成自定义API资源autoscaling.k8s.io并在其下生成两个资源VerticalPodAutoscalerCheckpoint及VerticalPodAutoscaler 创建测试应用 $ kubectl apply -f examples/hamster.yaml verticalpodautoscaler.autoscaling.k8s.io/hamster-vpa created deployment.apps/hamster created 测试pod会申请100m的CPU及50M内存并且其中会运行shell命令不断来消耗CPU及内存： resources: requests: cpu: 100m memory: 50Mi command: [\"/bin/sh\"] args: - \"-c\" - \"while true; do timeout 0.5s yes \u003e/dev/null; sleep 0.5s; done\" 在pod运行中，VPA控制器会发现应用所需要的CPU及内存不断增加。根据当前的updatePolicy会不断重建pod给予Pod更多的CPU及内存申请值 Pod中的资源申请已经上调为587m个CPU及256MiB内存了，这都是VPA控制器自动帮我们完成的。在应用负载降低时，同样会删除Pod并给予相对资源申请值。 配置文件解析 回到测试Pod配置文件查看有关VPA的定义 --- apiVersion: \"autoscaling.k8s.io/v1beta2\" kind: VerticalPodAutoscaler metadata: name: hamster-vpa spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hamster resourcePolicy: containerPolicies: - containerName: '*' minAllowed: cpu: 100m memory: 50Mi maxAllowed: cpu: 1 memory: 500Mi controlledResources: [\"cpu\", \"memory\"] targetRef：定义了VPA控制器作用的资源名称即相应的类型及API组 resourcePolicy：定义指定资源(这里是CPU及内存)的计算策略。CPU在100m-1间浮动，内存在50MiB-500MiB之间浮动 updatePolicy：这个字段可以通过describe vpa hamster-vpa看到，用于定义更新策略,目前有以下四种更新策略： 1、Off：仅会在VPA控制器中发现建议的资源申请值，Pod中的依然是原来的值。相当于dry run模式 2、Initial：仅会在Pod被创建时给予一个推荐的值，在Pod运行过程中不会修改。 3、Recreate：默认模式。除了在Pod创建时给予一个推荐值外，在Pod运行过程中会反复调整。调整的方式就是删除旧Pod新建新的Pod 4、Auto：相当于Recreate 已知问题 https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#known-limitations ","date":"2021-03-11","objectID":"/posts/%E5%88%9D%E8%AF%95pod%E5%9E%82%E7%9B%B4%E6%89%A9%E7%BC%A9%E5%AE%B9vpa/:0:2","tags":["Kubernetes"],"title":"初试Pod垂直扩缩容VPA","uri":"/posts/%E5%88%9D%E8%AF%95pod%E5%9E%82%E7%9B%B4%E6%89%A9%E7%BC%A9%E5%AE%B9vpa/"},{"categories":["Kubernetes"],"content":"概念 Kubernetes自1.11版本开始引入了名为\"HorizontalPodAutoscaler\"的控制器用于完成Pod基于CPU使用率进行水平扩展。所谓水平扩展是在Pod中CPU的使用率达到设定的某个值时，HPA告知Pod对应的高层控制器(如Deployment、RS等控制器)。高层控制器随即创建出多个pod副本平衡负载使Pod的平均CPU使用率低于我们设定的值。当Pod的平均CPU使用率变小时，又能回收多余的Pod以避免资源浪费。 除了CPU使用率这个指标外，还可以采用自定义的指标。通常自定义的指标需要容器以某种方式提供，如URL路径\"/metrics\"提供，即http://:/metrics获取相应的指标数据。当然如果kubernetes构建在公有云上，亦可以采用公有云服务商提供的指标数据如负载均衡器的QPS等作为HPA控制器的指标来源。 ","date":"2021-03-10","objectID":"/posts/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E7%9A%84hpa%E5%AE%9E%E6%88%98/:0:1","tags":["Kubernetes"],"title":"基于自定义指标的HPA实战","uri":"/posts/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E7%9A%84hpa%E5%AE%9E%E6%88%98/"},{"categories":["Kubernetes"],"content":"HPA控制器配置 目前HPA(HorizontalPodAutoscaler)资源对象处于Kubernetes的API组\"autoscaling\"中，包括v1、v2beta1及v2beta2三个API版本，其中\"autoscaling/v1\"仅支持基于CPU的自动扩缩容，而\"autoscaling/v2\"则支持基于自定义指标类型的数据。 为了使用HPA需要实现部署Metrics-Server。项目地址https://github.com/kubernetes-sigs/metrics-server，由于我本地环境是用Operator部署的Prometheus ，部署完自带metrics-server,所以不再手动部署。 部署完成metrics-server后稍等一会后就可以使用kubectl top node/pod命令查看节点或者pod的CPU及内存使用率了。 (1)基于autoscaling/v1版本的HPA配置，仅可以设置CPU使用率 apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache miniReplicas: 1 maxReplicas: 10 targetCPUUtilizationPercentage: 50 scaleTargetRef:定义目标作用对象，可以是Deployment、RS targetCPUUtilizationPercentage：目标pod的CPU阈值，超过这个值即触发扩容操作 miniReplicas及maxReplicas：最小及最大副本数量，HPA会自动在这个范围内进行Pod的自动伸缩同时维持各个Pod的CPU使用率为50% (2)基于autoscaling/v2beta2的HPA配置 type=Resource type=Resource的指标数据由metrics-server采集并提供给HPA控制器 kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2beta1 metadata: name: sample-app spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: sample-app minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu / memory target: type: Utilization averageUtilization: 50 / averageValue: 200MiB 其中type字段可为： Resource：基于资源的指标值，指标数据可以通过metrics.k8s.io这个API查询。 Pods：基于Pod的指标 Object：基于某种资源对象,如ingress或者任意自定义的指标 Pods与Object是自定义指标，需要搭建自定义的Metrics Server和监控工具进行指标采集。指标数据通过custom.metrics.k8s.io进行查询，必须先启动自定义的Metrics Server。 type=pods(本次采用的类型) 类型为Pods的类型指标数据来源于Pod对象本身，target指标只能是AverageValue。 metrics: - type: Pods pods: metrics: name: packet-per-second target: type: AverageValue averageValue: 1k type=object type=object的指标数据来自其他资源或者是自定义的指标。target指标为Value或者是AverageValue #指标名称为requests-per-second，其值来源于Ingress\"main-route\"的目标值，即Ingress每秒请求量达到2000时触发扩容操作 metrics： - type: Object object: metric: name: requests-per-second describeObject: apiVersion: extensions/v1beta1 kind: Ingress name: main-route target: type: Value value: 2k #指标名称为requests-per-second，其值来源于Ingress\"main-route\"的目标值，即Ingress每秒请求量达到2000时触发扩容操作 metrics： - type: Object object: metric: name: 'http_requests' selector: 'verb=GET' target: type: AverageValue averageValue: 500 ","date":"2021-03-10","objectID":"/posts/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E7%9A%84hpa%E5%AE%9E%E6%88%98/:0:2","tags":["Kubernetes"],"title":"基于自定义指标的HPA实战","uri":"/posts/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E7%9A%84hpa%E5%AE%9E%E6%88%98/"},{"categories":["Kubernetes"],"content":"基于自定义指标的HPA实战 整体概述 按照《Kubernetes权威指南(第四版)》所述的架构图，由Prometheus 采集各个pod的指标通过prometheus-adapter这个适配器将数据提交给指标聚合器，并最终注册到api-server中，以/apis/custom.metrics.k8s.io的路径提供指标数据。HPA控制器从该路径下获取数据作为水平自动扩缩容的依据。 配置controller-manager 一般而言通过kubeadm部署的k8s集群不需要进行api-server的手动配置，所需要的配置已经默认开启。我们只需要配置kube-controller-manager的参数即可。 编辑/etc/kubernetes/manifests/kube-controller-manager.yaml添加上图所示的启动参数。为了加快试验的效果，这里缩短了默认的配置。生产环境推荐以官方默认配置为准。 创建对象pod 该pod会在/metrics路径下提供名为http_requests_total的指标。 apiVersion: v1 kind: Service metadata: name: sample-app labels: app: sample-app spec: selector: app: sample-app ports: - name: http port: 80 targetPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: sample-app labels: app: sample-app spec: replicas: 1 selector: matchLabels: app: sample-app template: metadata: labels: app: sample-app spec: containers: - image: luxas/autoscale-demo:v0.1.2 name: metrics-provider ports: - name: http containerPort: 8080 创建ServiceMonitor对象，用于监控程序提供的指标 $ cat service-monitor.yaml kind: ServiceMonitor apiVersion: monitoring.coreos.com/v1 metadata: name: sample-app labels: app: sample-app spec: selector: matchLabels: app: sample-app endpoints: - port: http 注意:这里的port:http一定要和上面部署的Service对象中port的名称一致。应用该对象后稍等2~3分钟打开promethes的页面，即可发现相应的监控项已经是up状态： 配置Adapter 以上配置的指标作http_requests_total为一种持续增长的值，不能反映单位时间内的增长量，不能用作HPA的依据。需要进行适当地转换，打开adapter的配置，添加如下配置并应用： config.yaml: | rules: #sum(rate(http_requests_total{namespace=\"xx\",pod=\"xx\"}[1m])) by pod:1分钟内全部pod指标http_requests_total的总和的每秒平均值 - metricsQuery: sum(rate(\u003c\u003c.Series\u003e\u003e{\u003c\u003c.LabelMatchers\u003e\u003e}[1m])) by (\u003c\u003c.GroupBy\u003e\u003e) #将metricsQuery计算的结果赋给新的指标`http_requests`并提供给HPA控制器 name: as: \"${1}\" matches: ^(.*)_total$ resources: template: \u003c\u003c.Resource\u003e\u003e seriesFilters: - isNot: .*_seconds_total$ seriesQuery: '{namespace!=\"\",__name__=~\"http_requests_.*\"}' 应用configmap完成后删除adapter Pod，使新的配置生效。 自定义API资源 创建名为v1beta1.custom.metrics.k8s.io的自定义聚合API资源 apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1beta1.custom.metrics.k8s.io spec: service: name: prometheus-adapter namespace: monitoring group: custom.metrics.k8s.io version: v1beta1 insecureSkipTLSVerify: true groupPriorityMinimum: 100 versionPriority: 100 创建HPA控制器资源 在以上资源创建完成后即可以创建HPA控制器了 kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2beta2 metadata: name: sample-app spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: sample-app minReplicas: 1 maxReplicas: 10 metrics: - type: Pods pods: metric: name: http_requests target: type: AverageValue averageValue: 500m type=pods:表示从Pods自身获取指标 name=http_requests：即adapter中的配置，有http_requests_total计算而来 averageValue=500m：即http_requests的目标值为500m。 通过聚合API查询指标数据,pod指标成功采集 验证动态扩缩容 在其他终端请求sample-app的地址进行压测： $ for i in {1..100000};do wget -q -O- 10.233.38.160 \u003e /dev/null;done 自动扩容成功 停止访问请求一分钟后(kube-controller-manager中设定的参数--horizontal-pod-autoscaler-downscale-stabilization=1min进行缩容操作)pod数量成功回落到1个 ","date":"2021-03-10","objectID":"/posts/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E7%9A%84hpa%E5%AE%9E%E6%88%98/:0:3","tags":["Kubernetes"],"title":"基于自定义指标的HPA实战","uri":"/posts/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E7%9A%84hpa%E5%AE%9E%E6%88%98/"},{"categories":["Kubernetes"],"content":"参考 《Kubernetes权威指南(第四版)》（p241-p264） ","date":"2021-03-10","objectID":"/posts/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E7%9A%84hpa%E5%AE%9E%E6%88%98/:0:4","tags":["Kubernetes"],"title":"基于自定义指标的HPA实战","uri":"/posts/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E7%9A%84hpa%E5%AE%9E%E6%88%98/"},{"categories":["Kubernetes","Monitoring"],"content":"前言：最近客户需要监控ingress流量，在查阅资料后成功部署，记录下部署的过程及遇到的问题。 ","date":"2021-03-07","objectID":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/:0:0","tags":["Kubernetes","Monitoring"],"title":"Prometheus-Operator监控Nginx Ingress","uri":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/"},{"categories":["Kubernetes","Monitoring"],"content":"暴露ingress的监控端口 默认情况下nginx-ingress的监控指标端口为10254，监控路径为其下的/metrics。调整配置ingress-nginx的配置文件，打开service及pod的10254端口。 在官方的部署文件中的Service段中添加一段监听10254端口的配置，命名为https-metrics spec: type: ClusterIP ports: - name: https-webhook port: 443 targetPort: webhook - name: https-metrics port: 10254 targetPort: 10254 同时在deployment段中开放pod的10254端口,命名为metrics ports: - name: http containerPort: 80 protocol: TCP - name: https containerPort: 443 protocol: TCP - name: webhook containerPort: 8443 protocol: TCP - name: metrics containerPort: 10254 protocol: TCP ","date":"2021-03-07","objectID":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/:0:1","tags":["Kubernetes","Monitoring"],"title":"Prometheus-Operator监控Nginx Ingress","uri":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/"},{"categories":["Kubernetes","Monitoring"],"content":"静态抓取 在prometheus-prometheus.yaml的配置文件中，官方内置了一个字段additionalScrapeConfigs用于添加自定义的抓取目标 https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#PrometheusSpec 所以只需要按照官方的要求写入配置即可。新建额外抓取的信息prometheus-additional.yaml： - job_name: nginx-ingress metrics_path: /metrics scrape_interval: 5s static_configs: - targets: - 172.16.200.102:10254 - 172.16.200.103:10254 - 172.16.200.104:10254 抓取的目标制定为targets列表，创建secret对象ingress-nginx-additional-configs引用该配置 $ kubectl create secret generic ingress-nginx-additional-configs --from-file=./prometheus-additional.yaml -n monitoring 在promethes-prometheus.yaml中添加additionalScrapeConfigs引用创建的secret serviceAccountName: prometheus-k8s serviceMonitorNamespaceSelector: {} serviceMonitorSelector: {} version: v2.11.0 additionalScrapeConfigs: name: ingress-nginx-additional-configs key: prometheus-additional.yaml 重建promethes配置: $ kubectl delete -f ./prometheus-prometheus.yaml \u0026\u0026 kubectl apply -f ./prometheus-prometheus.yaml 打开promethes的UI页面，访问/targets路径发现相关的节点的状态已经为up状态 在grafana中导入9614号监控模板,数据显示正常： 静态配置的方式比较固定，不能针对主机列表动态变化。新增加边缘节点需要重建secretingress-nginx-additional-configs,十分不便。 ","date":"2021-03-07","objectID":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/:0:2","tags":["Kubernetes","Monitoring"],"title":"Prometheus-Operator监控Nginx Ingress","uri":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/"},{"categories":["Kubernetes","Monitoring"],"content":"servicemonitors抓取 针对使用Operator模式部署的prometheus，有一个名为servicemonitors.monitoring.coreos.com的CRD资源，可以仿照这个资源中某个项进行ingress-nginx的数据抓取 [root@master-1 ~]# kubectl -n monitoring get servicemonitors.monitoring.coreos.com NAME AGE alertmanager 4d22h coredns 4d22h grafana 4d22h kube-apiserver 4d22h kube-controller-manager 4d22h kube-scheduler 4d22h kube-state-metrics 4d22h kubelet 4d22h node-exporter 4d22h prometheus 4d22h prometheus-operator 4d22h 定义ingress-nginx的配置文件： $ cat ingress-nginx.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: nginx-ingress namespace: monitoring labels: app.kubernetes.io/component: controller spec: jobLabel: app.kubernetes.io/component endpoints: - port: https-metrics #之前定义的ingress的监控端口，一定要用名称。不能使用数字 interval: 10s selector: matchLabels: app.kubernetes.io/component: controller #该标签匹配ingress-nginx-controller的pod namespaceSelector: matchNames: - ingress-nginx #指定ingress控制器的名称空间为ingress-nginx 创建该资源 $ kubectl apply -f ./ingress-nginx.yaml 打开prometheues的监控面板下的/targets,相关资源已经生成： 参考静态抓取的方式导入监控面板即可。 ","date":"2021-03-07","objectID":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/:0:3","tags":["Kubernetes","Monitoring"],"title":"Prometheus-Operator监控Nginx Ingress","uri":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/"},{"categories":["Kubernetes","Monitoring"],"content":"参考文档 https://www.amd5.cn/atang_4421.html https://www.cnblogs.com/lvcisco/p/12574532.html https://prometheus.io/docs/prometheus/latest/configuration/configuration ","date":"2021-03-07","objectID":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/:0:4","tags":["Kubernetes","Monitoring"],"title":"Prometheus-Operator监控Nginx Ingress","uri":"/posts/prometheus-operator%E7%9B%91%E6%8E%A7nginx-ingress/"},{"categories":["Kubernetes","Etcd"],"content":"kubectl get cs时发现所有的etcd均返回503报错，查看etcd的告警发现有NO SPACE的信息且etcdctl endpoints status中的DB SIZE大于2GiB时。表示etcd的空间超过最大值需进行压缩。不进行压缩api-server无法继续写入数据，影响集群正常工作。 注意：修复会导致所有节点Not Ready，需重启各个节点的kubelet kubernetes版本：v1.17.0 etcd版本：3.3.10 etcd运行方式：docker容器 ","date":"2021-01-17","objectID":"/posts/etcdno-space%E7%9A%84%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D%E6%AD%A5%E9%AA%A4/:0:0","tags":["Kubernetes","Etcd"],"title":"Etcd：NO SPACE的问题修复步骤","uri":"/posts/etcdno-space%E7%9A%84%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D%E6%AD%A5%E9%AA%A4/"},{"categories":["Kubernetes","Etcd"],"content":"解决步骤 1、将API版本调整为3 $ export ETCDCTL_API=3 2、声明变量ETCD_ENDPOINT $ ETCD_ENDPOINT=(https://xx:xx:xx:xx:2379,https://xx:xx:xx:xx:2379,https://xx:xx:xx:xx:2379) $ ETCD_CAFILE=(/etc/ssl/etcd/ssl/ca.pem) $ ETCD_CERTFILE=(/etc/ssl/etcd/ssl/node-master-1.pem) $ ETCD_KEYFILE=(/etc/ssl/etcd/ssl/node-master-1-key.pem) 如果不清楚etcd证书存放的位置可以使用ps命令查看 $ ps -ef | grep -v grep | grep apiserver |sed 's/ /\\n/g' | grep etcd --etcd-cafile=/etc/ssl/etcd/ssl/ca.pem --etcd-certfile=/etc/ssl/etcd/ssl/node-master-1.pem --etcd-keyfile=/etc/ssl/etcd/ssl/node-master-1-key.pem --etcd-servers=https://172.16.200.101:2379,https://172.16.200.102:2379,https://172.16.200.103:2379 --storage-backend=etcd3 3、备份etcd $ etcdctl --endpoints=${ETCD_ENDPOINT} --cert=${ETCD_CERTFILE} --key=${ETCD_KEYFILE} \\ --cacert=${ETCD_CAFILE} snapshot save /var/lib/etcd/my-snapshot.db 4、获取当前的修订编号,选取需要截断的历史 $ rev=$(etcdctl --cert=${ETCD_CERTFILE} --key=${ETCD_KEYFILE} --cacert=${ETCD_CAFILE} \\ endpoint status -w json | egrep -o '\"revision\":[0-9]*' | \\ egrep -o '[0-9]*'| awk 'NR==1{print $1}') 5、按第四步得到的编号进行压缩 $ etcdctl --endpoints=${ETCD_ENDPOINT} --cert=${ETCD_CERTFILE} \\ --key=${ETCD_KEYFILE} --cacert=${ETCD_CAFILE} compact $rev 6、释放空间 $ etcdctl --endpoints=${ETCD_ENDPOINT} --cert=${ETCD_CERTFILE} \\ --key=${ETCD_KEYFILE} --cacert=${ETCD_CAFILE} defrag 此步骤最好将endpoints列表中的节点拆开依次执行，否则容易引起集群震荡 7、解除告警 $ etcdctl --endpoints=${ETCD_ENDPOINT} --cert=${ETCD_CERTFILE} \\ --key=${ETCD_KEYFILE} --cacert=${ETCD_CAFILE} alarm disarm 此命令可能需执行两次，确保etcdctl --endpoints=${ETCD_ENDPOINT} --cert=${ETCD_CERTFILE} --key=${ETCD_KEYFILE} --cacert=${ETCD_CAFILE} alarm list再无输出 8、重启kubelet 经过一段时候后，如果发现所有节点都是Not Ready状态,只需要将每个节点的kubelet重启即可 $ for node in `kubectl get node --no-headers | awk '{print $1}' | \\ xargs`;do ssh ${node} 'systemctl restart kubelet';done 9、观察一段时间的DBsize，确保无大幅度增长 $ etcdctl --endpoints=${ETCD_ENDPOINT} --cert=${ETCD_CERTFILE} \\ --key=${ETCD_KEYFILE} --cacert=${ETCD_CAFILE} endpoints status -w table 参考：压缩完DB SIZE是30MiB，一段时间涨到132MiB后持续不动 KeySpace与存储空间 在解决步骤的第6步，会执行defrag命令，这是用来释放碎片文件的。碎片文件是etcd进行compaction操作后历史的数据，这部分的空间是可以被用来存放新键的。所以在进行完compaction操作后是没有必要进行defrag的。只有当etcd的空间满了进行释放空间的操作时才需要进行defrag操作。 如何查看etcd空间的实际使用量? 通过etcdctl命令查看的DB SIZE是etcd历史最大的数据量大小，不是目前的数据量。etcd对外暴露了mertics提供监控信息，其中名为etcd_mvcc_db_total_size_in_use_in_bytes的值即是目前数据量的大小。查询方法可使用下面的命令: curl -s --cacert ${ETCD_CA_FILE} --cert ${ETCD_CERT_FILE} --key ${ETCD_KEY_FILE} `echo ${ETCD_ENDPOINTS} \\ | awk -F, '{print $1}'`/metrics -o-| egrep ^etcd_mvcc_db_total_size_in_use_in_bytes | awk '{print $2}' | awk '{printf(\"%.2fMiB\",$0/1024/1024)}' ","date":"2021-01-17","objectID":"/posts/etcdno-space%E7%9A%84%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D%E6%AD%A5%E9%AA%A4/:0:1","tags":["Kubernetes","Etcd"],"title":"Etcd：NO SPACE的问题修复步骤","uri":"/posts/etcdno-space%E7%9A%84%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D%E6%AD%A5%E9%AA%A4/"},{"categories":["Kubernetes","Etcd"],"content":"参考文档 https://etcd.io/docs/v3.3.12/op-guide/maintenance/ ","date":"2021-01-17","objectID":"/posts/etcdno-space%E7%9A%84%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D%E6%AD%A5%E9%AA%A4/:0:2","tags":["Kubernetes","Etcd"],"title":"Etcd：NO SPACE的问题修复步骤","uri":"/posts/etcdno-space%E7%9A%84%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D%E6%AD%A5%E9%AA%A4/"},{"categories":["Kubernetes","TroubleShooting"],"content":"前言：本文主要分析了针对podcustom-metrics-apiserver的驱逐事件，分析相关成因并给出解决措施。 ","date":"2021-01-09","objectID":"/posts/kubernetes%E9%A9%B1%E9%80%90%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90/:0:0","tags":["Kubernetes","TroubleShooting"],"title":"Kubernetes驱逐事件分析","uri":"/posts/kubernetes%E9%A9%B1%E9%80%90%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes","TroubleShooting"],"content":"问题 在学习HPA自动伸缩时，部署完custom-metrics-apiserver这个的Deployment后资源后经过一段时间总能观察到大量的驱逐事件: $ kubectl get pods -n custom-metrics NAME READY STATUS RESTARTS AGE custom-metrics-apiserver-f884776d4-9x9br 0/1 Evicted 0 6h34m custom-metrics-apiserver-f884776d4-cztcr 0/1 Evicted 0 8h custom-metrics-apiserver-f884776d4-fpr25 0/1 Evicted 0 4h46m custom-metrics-apiserver-f884776d4-lcvtt 1/1 Running 0 33m custom-metrics-apiserver-f884776d4-pb58n 0/1 Evicted 0 5h28m custom-metrics-apiserver-f884776d4-plzr5 0/1 Evicted 0 99m custom-metrics-apiserver-f884776d4-q7fpz 0/1 Evicted 0 10h custom-metrics-apiserver-f884776d4-tllrm 0/1 Evicted 0 154m custom-metrics-apiserver-f884776d4-wqnw9 0/1 Evicted 0 7h41m custom-metrics-apiserver-f884776d4-xh9wz 0/1 Evicted 0 8h custom-metrics-apiserver-f884776d4-xrs44 0/1 Evicted 0 3h40m 一开始并不了解是什么情况也是第一次见到STATUS=Evicted的状态，只是简单的删除被驱逐的podkubectl delete pods --all -n custom-metrics。但是经过一段时间后又会出现同样的情况，本着发生驱逐就意味着节点资源紧缺的判断，有必要分析下驱逐的原因并且梳理下避免驱逐的措施。 ","date":"2021-01-09","objectID":"/posts/kubernetes%E9%A9%B1%E9%80%90%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90/:0:1","tags":["Kubernetes","TroubleShooting"],"title":"Kubernetes驱逐事件分析","uri":"/posts/kubernetes%E9%A9%B1%E9%80%90%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes","TroubleShooting"],"content":"分析 从kubelet日志可以发现，是由于ephemeral-storage不足而引发的驱逐。那么这个ephemeral-storage是什么呢？带着问题去[kubernetes官网](Managing Resources for Containers | Kubernetes )搜索，发现这是类似于cpu、内存的一种资源，暂时翻译为临时性存储。 node-1 kubelet[7082]: W 7082 eviction_manager.go:330] eviction manager: attempting to reclaim ephemeral-storage node-1 kubelet[7082]: I 7082 container_gc.go:85] attempting to delete unused containers node-1 kubelet[7082]: I 7082 image_gc_manager.go:317] attempting to delete unused images node-1 kubelet[7082]: I 7082 eviction_manager.go:341] eviction manager: must evict pod(s) to reclaim ephemeral-storage node-1 kubelet[7082]: I 7082 eviction_manager.go:359] eviction manager: pods ranked for eviction: custom-metrics-apiserver-747fd5d6f6-9dg8t_custom-metrics(e8ea2efe-7e5f-4098-a9a6-359f7e79cdef), node-exporter-khwxn_monitoring(5f8ffd48-e9fe-4273-9937-1e4e6788e437), nginx-proxy-node-1_kube-system(1e396843ff781cfa662c1e9f2423a988), rbd-provisioner-bc84fd48f-nckhq_kube-system(ec731329-782f-4768-b989-b07f04724c0f), ceph-pod_default(bd698243-bea0-4175-993c-00de365a9d19), metrics-server-cc9d976bf-h8jqh_kube-system(d06ea89f-f8ea-474d-9864-82d336621a8b), nodelocaldns-8d9ml_kube-system(ccfcf424-3f70-4a3e-8525-f0fddebc301f), calico-node-kkfsx_kube-system(386b8857-3354-43e9-827d-b7245a4a6e81), kube-proxy-vcw5x_kube-system(a7db076a-a9ff-41b6-9bf5-0f190b99abc9) node-1 kubelet[7082]: I 7082 kubelet_node_status.go:486] Recording NodeHasDiskPressure event message for node node-1 node-1 kubelet[7082]: I 7082 kubelet_pods.go:1102] Killing unwanted pod \"custom-metrics-apiserver-747fd5d6f6-9dg8t\" node-1 kubelet[7082]: I 7082 eviction_manager.go:566] eviction manager: pod custom-metrics-apiserver-747fd5d6f6-9dg8t_custom-metrics(e8ea2efe-7e5f-4098-a9a6-359f7e79cdef) is evicted successfully #清理残留数据过程... node-1 kubelet[7082]: I 7082 eviction_manager.go:403] eviction manager: pods custom-metrics-apiserver-747fd5d6f6-9dg8t_custom-metrics(e8ea2efe-7e5f-4098-a9a6-359f7e79cdef) successfully cleaned up 日志中提到了需要回收ephemeral-storage,并且尝试删除未使用的容器及镜像无果，发现必须要驱逐pod才能完成对于ephemeral-storage的回收操作。然后对所有运行中的pod进行打分，custom-metrics-apiserver-747fd5d6f6-9dg8t_custom-metrics的分数最高从而被驱逐。在此之间kubelet向apiserver报告了NodeHasDiskPressure的事件，并且kubelet在驱逐完pod后对节点资源进行回收等操作。 关于驱逐的误区 一开始我以为驱逐是把k8s把资源紧张的节点上的某个pod移动到另外的节点上，但后来发现所谓的驱逐仅仅是终止pod。在别的节点拉起的动作是由Deployment、ReplicaSet等高层控制器完成的。 定义测试pod $ cat test-pod.yaml apiVersion: v1 kind: Pod metadata: name: pod spec: hostNetwork: true containers: - name: nginx image: reg.kolla.org/library/nginx:1.18 imagePullPolicy: IfNotPresent ports: - name: test-pod containerPort: 80 $ kubectl get pods NAME READY STATUS RESTARTS AGE pod 1/1 Running 0 118s 定义驱逐文件 $ cat eviction.json { \"apiVersion\": \"policy/v1beta1\", \"kind\": \"Eviction\", \"metadata\": { \"name\": \"pod\", \"namespace\": \"default\" } } 测试驱逐 $ kubectl proxy Starting to serve on 127.0.0.1:8001 #复制终端后执行 $ curl -X POST -d @eviction.json -H 'Content-type: application/json' http://127.0.0.1:8001/api/v1/namespaces/default/pods/pod/eviction { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Success\", \"code\": 201 } 查看结果 $ kubectl get pods No resources found in default namespace. 从结果来看，由于pod资源上层没有更高级的副本控制器，在pod被驱逐后pod就无法运行了。所以驱逐仅是完成对pod的终止操作。 临时存储的问题 我们知道volume中的emptyDir就是一种临时存储，这种存储不能用来存放持久化数据。当pod被删除后，emptyDir中的数据就会被清空，所以emptyDir这种类型的volume只是用来存储pod运行过程中产生的临时数据，或者是pod内多个container共享临时数据的一种方式。那么是否有可能是emptyDir的问题呢？查看配置发现，配置中的确有emptyDir类型的卷： $ cat custom-metrics-apiserver-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: custom-metrics-apiserver name: custom-metrics-apiserver namespace: custom-metrics spec: replicas: 1 selector: matchLabels: app: custom-metrics-apiserver template: metadata: labels: app: custom-metrics-apiserver name: custom-metrics-apiserver spec: serviceAccountName: custom-metrics-apiserver containers: - name: custom-metrics-apiserver image: reg.kolla.org/prometheus/k8s-prometheus-adapter-amd64 args: - --secure-port=6443 - --tls-cert-file=/var/run/serving-cert/serving.crt - --tls-privat","date":"2021-01-09","objectID":"/posts/kubernetes%E9%A9%B1%E9%80%90%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90/:0:2","tags":["Kubernetes","TroubleShooting"],"title":"Kubernetes驱逐事件分析","uri":"/posts/kubernetes%E9%A9%B1%E9%80%90%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes","TroubleShooting"],"content":"解决 联系上文的分析，断定是日志的问题，解决方法也很简单：1、修改日志的级别 2、docker中配置日志相关参数 将日志的级别修改为1后重新应用，运行一天后不再观察到驱逐事件： $ kubectl get pods -n custom-metrics NAME READY STATUS RESTARTS AGE custom-metrics-apiserver-6594b9d597-t84q2 1/1 Running 0 28h 虽然调整日志级别的方法很管用，但是当容器长期运行时随着日志量的逐步累积仍然可能被驱逐。并且日志级别太高不能很好的分析问题。 docker启动参数中添加最大日志及日志数量的限制： $ cat \u003e\u003e /etc/docker/daemon.json \u003c\u003cEOF { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"500m\", \"max-file\": \"3\" } } $ systemctl daemon-reload $ systemctl restart docker 限制最多有3个日志并且每个日志最大500M。长期运行后未发现驱逐事件。其实在部署k8s集群就应该将日志的限制加到docker的配置项中，但我这个是测试环境懒得加，直到出现这个问题才发现是这个原因，生产环境应添加日志配置的参数。 限制前单个日志不断增大造成磁盘紧张引发驱逐： $ ls -lh total 1.0G -rw-r-----. 1 root root 610M Jan 9 17:01 a9d63a1a2c48765ac9a962d269b5ab59aae2617f6954280826dbdc8cfd22fcd0-json.log drwx------. 2 root root 6 Jan 9 16:50 checkpoints -rw-------. 1 root root 7.2K Jan 9 16:50 config.v2.json -rw-r--r--. 1 root root 2.4K Jan 9 16:50 hostconfig.json drwx------. 2 root root 6 Jan 9 16:50 mounts 限制后日志不断回滚，最大不超过1.5G： $ ls -lh total 970M -rw-r-----. 1 root root 15M Jan 9 17:02 86acee999a12760622fc7983944ecfe44d8e944b5cdd5905d9d999ff21f9aa4e-json.log -rw-r-----. 1 root root 477M Jan 9 17:01 86acee999a12760622fc7983944ecfe44d8e944b5cdd5905d9d999ff21f9aa4e-json.log.1 -rw-r-----. 1 root root 477M Jan 9 16:53 86acee999a12760622fc7983944ecfe44d8e944b5cdd5905d9d999ff21f9aa4e-json.log.2 drwx------. 2 root root 6 Jan 9 16:10 checkpoints -rw-------. 1 root root 7.2K Jan 9 16:10 config.v2.json -rw-r--r--. 1 root root 2.4K Jan 9 16:10 hostconfig.json drwx------. 2 root root 6 Jan 9 16:10 mounts ","date":"2021-01-09","objectID":"/posts/kubernetes%E9%A9%B1%E9%80%90%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90/:0:3","tags":["Kubernetes","TroubleShooting"],"title":"Kubernetes驱逐事件分析","uri":"/posts/kubernetes%E9%A9%B1%E9%80%90%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes","TroubleShooting","Calico"],"content":"集群中两个节点中的calico-node启动后一直处于crashloopbackoff的分析处理 ","date":"2020-12-30","objectID":"/posts/%E4%B8%80%E6%AC%A1calico-node%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:0:0","tags":["Kubernetes","TroubleShooting","Calico"],"title":"一次calico-node无法启动的问题记录","uri":"/posts/%E4%B8%80%E6%AC%A1calico-node%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Kubernetes","TroubleShooting","Calico"],"content":"问题 生产集群中node-4及node-9两个节点的calico-node无法启动一直处于crashloopbackoff。calicoctl node status中的INFO列这两个节点显示为Active Socket: Connection refused。kubectl describe发现Event中有报错： Liveness probe failed: Get http://127.0.0.1:9099/liveness: dial tcp 192.168.99.7:9099: getsockopt: connection refused ","date":"2020-12-30","objectID":"/posts/%E4%B8%80%E6%AC%A1calico-node%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:0:1","tags":["Kubernetes","TroubleShooting","Calico"],"title":"一次calico-node无法启动的问题记录","uri":"/posts/%E4%B8%80%E6%AC%A1calico-node%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Kubernetes","TroubleShooting","Calico"],"content":"追踪 尝试将两个pod重启 $ kubectl get pods -n kube-system --no-headers --field-selector=status.phase!=Running | grep \"calico-node\" | awk '{print $1}' | xargs kubectl delete pod -n kube-system 重启完成后pod仍然处于crashloopbackoff状态，失败。 进入Github搜索相关issue，基本都是因为calico自动获取的ip不是目前在用的ip，但检查日志发现日志中获取的ip就是本地bond0网卡的地址。失败。 检查日志发现日志很短,并且两个节点的calico-node都是这种情况:执行到Using node name就结束了。与正常节点的calico-node启动日志对比，这部分日志并没有什么区别。 2020-12-30 12:50:55.337 [INFO][10] startup.go 256: Early log level set to info 2020-12-30 12:50:55.337 [INFO][10] startup.go 272: Using NODENAME environment for node name 2020-12-30 12:50:55.337 [INFO][10] startup.go 284: Determined node name: node-4 2020-12-30 12:50:55.436 [INFO][10] startup.go 97: Skipping datastore connection test 2020-12-30 12:50:55.438 [INFO][10] startup.go 476: Using IPv4 address from environment: IP=172.16.200.103 2020-12-30 12:50:55.440 [INFO][10] startup.go 509: IPv4 address 172.16.200.103 discovered on interface bond0 2020-12-30 12:50:55.440 [INFO][10] startup.go 647: No AS number configured on node resource, using global value 2020-12-30 12:50:55.440 [INFO][10] startup.go 149: Setting NetworkUnavailable to False 2020-12-30 12:50:55.639 [INFO][10] startup.go 530: FELIX_IPV6SUPPORT is false through environment variable 2020-12-30 12:50:55.644 [INFO][10] startup.go 181: Using node name: node-4 灵感：在某个github issue 中提到虽然就绪探针readinessProbe不会导致crashloopbackoff,但是livenessProbe可能。检查calico-node的配置文件中有关探针的部分 $ kubectl get daemonsets.apps -n kube-system calico-node -o yaml ... livenessProbe: httpGet: host: 127.0.0.1 path: /liveness port: 9099 scheme: HTTP initialDelaySeconds: 5 periodSeconds: 10 successThreshold: 1 failureThreshold: 6 timeoutSeconds: 1 ... liveness探针自pod启动后5s后开始探测 每隔10s进行一次探测 检查失败后进入成功状态最少连续探测次数为1次 检查成功后进入失败状态最少连续探测次数为6次 探测检测超时为1s 如果是配置文件有问题，那么所有的calico-node都会有问题，但现在只有node-4及node-9上的calico-node有问题。会不会是这两个节点异常导致calico-node启动后5s没有pod并没有启动完成，存活探针无法访问9099端口触发pod自动重启，不断反复导致pod处于crashloopbackoff？登录node-4及node-9发现两个节点的负载竟然比正常节点高了几十倍。原因就显而易见了：高负载导致pod启动缓慢，存活探针判断失败触发pod不断自动重启引发pod状态异常。 ","date":"2020-12-30","objectID":"/posts/%E4%B8%80%E6%AC%A1calico-node%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:0:2","tags":["Kubernetes","TroubleShooting","Calico"],"title":"一次calico-node无法启动的问题记录","uri":"/posts/%E4%B8%80%E6%AC%A1calico-node%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Kubernetes","TroubleShooting","Calico"],"content":"解决 驱逐node-4及node-9上的pod $ kubectl drain node-4 --delete-local-data --ignore-daemonsets --force $ kubectl drain node-9 --delete-local-data --ignore-daemonsets --force 重启两个节点 $ reboot 重启完成后两个节点上的calico-node处于Running状态 ","date":"2020-12-30","objectID":"/posts/%E4%B8%80%E6%AC%A1calico-node%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:0:3","tags":["Kubernetes","TroubleShooting","Calico"],"title":"一次calico-node无法启动的问题记录","uri":"/posts/%E4%B8%80%E6%AC%A1calico-node%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Kubernetes"],"content":"在一套k8s集群中会存在多个名称空间，各个名称空间由其项目组进行维护。为达到名称空间隔离，防止其他用户操作本空间下的资源，需要创建特定的config文件。使用对应的config仅能对相应名称空间下的资源进行管理。本例以k8s-api用户为例，该用户仅能操作k8s-api名称空间下的部分资源。 ","date":"2020-12-26","objectID":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/:0:0","tags":["Kubernetes"],"title":"Kubernetes创建名称空间账户","uri":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/"},{"categories":["Kubernetes"],"content":"创建证书 创建私钥 openssl genrsa -out k8s-api.key 2048 创建证书签署请求CSR openssl req -new -key k8s-api.key -out k8s-api.csr -subj \"/CN=k8s-api\" 使用k8s集群的ca证书及私钥签发用户证书 openssl x509 -req -in k8s-api.csr \\ -CA /etc/kubernetes/ssl/ca.crt \\ -CAkey /etc/kubernetes/ssl/ca.key \\ -CAcreateserial \\ -out k8s-api.crt \\ -days 3560 注：签发周期为10年，实际环境取决于需求。 ","date":"2020-12-26","objectID":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/:0:1","tags":["Kubernetes"],"title":"Kubernetes创建名称空间账户","uri":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/"},{"categories":["Kubernetes"],"content":"生成k8s-api用户要用到的config文件 config文件存放着kubectl用来操作k8s集群用的到证书、集群信息等，位于$HOME/.kube/目录下。 生成默认的config(此步骤结束会生成k8s-api.conf) kubectl config set-cluster k8s-api \\ --certificate-authority=/etc/kubernetes/ssl/ca.crt \\ --embed-certs=true \\ --server=https://172.16.200.101:6443 \\ --kubeconfig=k8s-api.conf 注：1. --embed-certs=true: 将证书文件内嵌到config中 2. --server=https://172.16.200.101:6443:指定apiserver的地址 将创建的用户公钥及私钥加入到config文件中 kubectl config set-credentials k8s-api \\ --client-certificate=k8s-api.crt \\ --client-key=k8s-api.key \\ --embed-certs=true \\ --kubeconfig=k8s-api.conf 配置上下文参数 kubectl config set-context k8s-api \\ --cluster=k8s-api \\ --user=k8s-api \\ --kubeconfig=k8s-api.conf 查看生成好的config文件 apiVersion: v1 clusters: - cluster: certificate-authority-data: ignore... (ca.crt | base64) server: https://172.16.200.101:6443 name: k8s-api contexts: - context: cluster: k8s-api namespace: k8s-api user: k8s-api name: k8s-api current-context: \"\" (进入相应用户后设置该字段) kind: Config preferences: {} users: - name: k8s-api user: client-certificate-data: ignore... (cat k8s-api.crt | base64) client-key-data: ignore... (cat k8s-api.key | base64) ","date":"2020-12-26","objectID":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/:0:2","tags":["Kubernetes"],"title":"Kubernetes创建名称空间账户","uri":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/"},{"categories":["Kubernetes"],"content":"创建名称空间 kubectl create ns k8s-api ","date":"2020-12-26","objectID":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/:0:3","tags":["Kubernetes"],"title":"Kubernetes创建名称空间账户","uri":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/"},{"categories":["Kubernetes"],"content":"创建RBAC规则 创建role定义k8s-api用户拥有的权限 注：该角色配置仅作为参考，实际环境按需调整 cat k8s-api-rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: k8s-api-role namespace: k8s-api rules: - apiGroups: - \"\" resources: - pods verbs: - create - get - list - watch - update - apiGroups: - apps resources: - deployments verbs: - create - get - delete $ kubectl create -f k8s-api-role.yaml 创建rolebinding cat k8s-api-rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: k8s-api-rolebinding namespace: k8s-api roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: k8s-api-role subjects: - kind: User name: k8s-api kubectl create -f k8s-api-rolebinding.yaml ","date":"2020-12-26","objectID":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/:0:4","tags":["Kubernetes"],"title":"Kubernetes创建名称空间账户","uri":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/"},{"categories":["Kubernetes"],"content":"创建用户并测试权限 创建系统用户k8s-api并设置密码 $ useradd k8s-api $ passwd k8s-api $ mkdir /home/k8s-api/.kube $ cp ./k8s-api.conf /home/k8s-api/.kube/config $ chown -Rf k8s-api.k8s-api /home/k8s-api/ 切换到k8s-api用户并测试相关权限 $ su - k8s-api $ kubectl config use-context k8s-api $ kubectl get ns Error from server (Forbidden): namespaces is forbidden: User \"k8s-api\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope $ kubectl get pods -n kube-system Error from server (Forbidden): pods is forbidden: User \"k8s-api\" cannot list resource \"pods\" in API group \"\" in the namespace \"kube-system\" $ kubectl get pods -n k8s-api (可不加名称空间,默认就是k8s-api) NAME READY STATUS RESTARTS AGE busybox-deployment-5ddc5f454f-76c7d 1/1 Running 0 19h busybox-deployment-5ddc5f454f-gqbdb 1/1 Running 0 19h nginx-deployment-2-5957d54f6d-5cj4r 1/1 Running 0 19h nginx-deployment-2-5957d54f6d-8bdfx 1/1 Running 0 19h $ kubectl get deployments -n k8s-api (可不加名称空间,默认就是k8s-api) Error from server (Forbidden): deployments.apps is forbidden: User \"k8s-api\" cannot list resource \"deployments\" in API group \"apps\" in the namespace \"k8s-api\" $ kubectl create deployment k8s-api-rbac-test --image=reg.kolla.org/library/nginx:1.19 --namespace=k8s-api --replicas=2 deployment.apps/k8s-api-rbac-test created 可以看出k8s-api用户仅拥有对k8s-api名称空间下pod及deployment的相应权限，保证了隔离性。 ","date":"2020-12-26","objectID":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/:0:5","tags":["Kubernetes"],"title":"Kubernetes创建名称空间账户","uri":"/posts/kubernetes%E5%88%9B%E5%BB%BA%E5%90%8D%E7%A7%B0%E7%A9%BA%E9%97%B4%E8%B4%A6%E6%88%B7/"},{"categories":["Ceph"],"content":"前言:CRUSH算法是Ceph的核心算法，全称为可扩展散列下的智能分发机制(Controlled Replication Under Scalable Hashing)。是整个Ceph数据存储机制的核心。默认安装的Ceph集群会根据当前集群自动生成一套CRUSH map规则，但是默认的CRUSH map可能不符合预期，可以手动修改以满足实际需求。 生产环境中，我们的应用场景可能需要在多种类型设备上创建多个存储池。如基于SSD磁盘创建池创建CephFS的metadata存储池，基于HDD磁盘创建CephFS的data存储池。分而治之，实现Ceph集群的最佳性能。 ","date":"2020-12-20","objectID":"/posts/crush-map%E5%AE%9E%E8%B7%B5/:0:0","tags":["Ceph"],"title":"CRUSH map实践","uri":"/posts/crush-map%E5%AE%9E%E8%B7%B5/"},{"categories":["Ceph"],"content":"主机列表 外部ip 集群ip host 角色 172.16.200.101 10.16.200.101 ceph-mon1 mon、osd节点 172.16.200.102 10.16.200.102 ceph-mon2 mon、osd节点 172.16.200.103 10.16.200.103 ceph-mon3 mon、osd节点 172.16.200.104 10.16.200.104 ceph-osd4 osd节点 这里假设ceph-osd4节点上的磁盘是SSD磁盘，其余节点是HDD磁盘。创建两个存储池：ssd及sata,分别应用到应用到SSD磁盘和HDD磁盘上。需要手动修改CRUSH map以应用此效果。 ","date":"2020-12-20","objectID":"/posts/crush-map%E5%AE%9E%E8%B7%B5/:0:1","tags":["Ceph"],"title":"CRUSH map实践","uri":"/posts/crush-map%E5%AE%9E%E8%B7%B5/"},{"categories":["Ceph"],"content":"调整CRUSH map 获取现有CRUSH map 登录任一mon节点导出现有的CRUSH map root /etc/ceph \u003e\u003e\u003e ceph osd getcrushmap -o crushmap_compiled_file #反编译 root /etc/ceph \u003e\u003e\u003e crushtool -d crushmap_compiled_file -o crushmap_decompiled_file #经过反编译后的CRUSH map可以使用任意的编辑器打开 #为防止错误修改CRUSH map导致集群故障，首先备份CRUSH map root /etc/ceph \u003e\u003e\u003e cp crushmap_decompiled_file crushmap_fix 修改CRUSH map 打开导出的CRUSH map文件，当前的CRUSH map如下： ... # buckets root default { id -1 # do not change unnecessarily id -2 class hdd # do not change unnecessarily # weight 0.156 alg straw2 hash 0 # rjenkins1 item ceph-mon1 weight 0.039 item ceph-mon2 weight 0.039 item ceph-mon3 weight 0.039 item ceph-osd4 weight 0.039 } # rules rule replicated_rule { id 0 type replicated min_size 1 max_size 10 step take default step chooseleaf firstn 0 type host step emit } root default bucket中定义了所有的节点，每个节点的权重一致。创建两个bucket取代default bucket。注意id不要与之前出现的id重复。 root sata { id -11 alg straw2 hash 0 item ceph-mon1 weight 0.039 item ceph-mon2 weight 0.039 item ceph-mon3 weight 0.039 } root ssd { id -12 alg straw2 hash 0 item ceph-osd4 weight 0.039 } 创建两条规则sata_rule及ssd_rule取代默认的crush_rule。 rule sata_rule { id 0 type replicated min_size 3 max_size 10 step take sata step chooseleaf firstn 0 type host step emit } rule ssd_rule { id 1 type replicated min_size 3 max_size 10 step take ssd step chooseleaf firstn 0 type host step emit } 注意：sata_rule的id是0需要与之前默认的rule的id保持一致，否则无法应用修改后的CRUSH map。因为之前创建的池找不到对应的crush_rule。 应用新的CRUSH map 编译修改后的CRUSH map root /etc/ceph \u003e\u003e\u003e crushtool -c crushmap_fix -o crushmap_fix_compiled 应用新的CRUSH map root /etc/ceph \u003e\u003e\u003e ceph osd setcrushmap -i crushmap_fix_compiled 应用新的CRUSH map后。集群将开始数据调整和数据恢复，过一段时间后集群才会重新回到HEALTH_OK状态。 ","date":"2020-12-20","objectID":"/posts/crush-map%E5%AE%9E%E8%B7%B5/:0:2","tags":["Ceph"],"title":"CRUSH map实践","uri":"/posts/crush-map%E5%AE%9E%E8%B7%B5/"},{"categories":["Ceph"],"content":"新建存储池 一旦Ceph集群回到HEALTH_OK状态，就可以创建存储池了。这里定义两个存储池：sata池和ssd池，并且分别定义crush_rule为sata_rule和ssd_rule，即CRUSH map中新定义的sata_rule和ssd_rule。 创建存储池 创建存储池sata及ssd root /etc/ceph \u003e\u003e\u003e ceph osd pool create sata 64 64 pool 'sata' created root /etc/ceph \u003e\u003e\u003e ceph osd pool create ssd 64 64 pool 'ssd' created 为存储池指定crush rule 分别为存储池sata及ssd指定crush_rule root /etc/ceph \u003e\u003e\u003e ceph osd pool set sata crush_rule sata_rule set pool 11 crush_rule to sata_rule root /etc/ceph \u003e\u003e\u003e ceph osd pool set ssd crush_rule ssd_rule set pool 12 crush_rule to ssd_rule 查看现有的池规则 root /etc/ceph \u003e\u003e\u003e ceph osd dump | egrep -i \"sata|ssd\" pool 11 'sata' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 last_change 106 flags hashpspool stripe_width 0 pool 12 'ssd' replicated size 3 min_size 2 crush_rule 1 object_hash rjenkins pg_num 64 pgp_num 64 last_change 107 flags hashpspool stripe_width 0 sata池与ssd池对应的crush_rule分别为0和1，就是修改后的CRUSH map中sata_rule和ssd_rule中的id。 ","date":"2020-12-20","objectID":"/posts/crush-map%E5%AE%9E%E8%B7%B5/:0:3","tags":["Ceph"],"title":"CRUSH map实践","uri":"/posts/crush-map%E5%AE%9E%E8%B7%B5/"},{"categories":["Ceph"],"content":"测试存储效果 创建新文件 root /etc/ceph \u003e\u003e\u003e dd if=/dev/zero of=sata.file bs=1M count=64 conv=fsync 64+0 records in 64+0 records out 67108864 bytes (67 MB) copied, 0.167555 s, 401 MB/s root /etc/ceph \u003e\u003e\u003e dd if=/dev/zero of=ssd.file bs=1M count=64 conv=fsync 64+0 records in 64+0 records out 67108864 bytes (67 MB) copied, 0.151092 s, 444 MB/s 上传文件到指定的池 rados为对象存储的命令行工具，上传对象的子命令为rados put \u003cobj-name\u003e \u003cinfile\u003e -p \u003cpool-name\u003e root /etc/ceph \u003e\u003e\u003e rados put sata.pool.object sata.file -p sata root /etc/ceph \u003e\u003e\u003e rados put ssd.pool.object ssd.file -p ssd 检查池中的对象信息 root /etc/ceph \u003e\u003e\u003e ceph osd map sata sata.pool.object osdmap e110 pool 'sata' (11) object 'sata.pool.object' -\u003e pg 11.f71bcbc2 (11.2) -\u003e up ([0,2,4], p0) acting ([0,2,4], p0) root /etc/ceph \u003e\u003e\u003e ceph osd map ssd ssd.pool.object osdmap e110 pool 'ssd' (12) object 'ssd.pool.object' -\u003e pg 12.82fd0527 (12.27) -\u003e up ([6], p6) acting ([6,0,3], p6) 由于副本数为3。查看两个对象acting字段中的值，sata.pool.object的第一副本位于osd.0,第二第三副本位于osd.2,osd.4上，符合预期。ssd.pool.object的第一副本为osd.6为SSD磁盘，其余两个副本位于HDD磁盘上。 ","date":"2020-12-20","objectID":"/posts/crush-map%E5%AE%9E%E8%B7%B5/:0:4","tags":["Ceph"],"title":"CRUSH map实践","uri":"/posts/crush-map%E5%AE%9E%E8%B7%B5/"},{"categories":["Ceph"],"content":"前言:Ceph能提供三大存储接口，即块存储、文件存储和对象存储。本篇博客主要介绍Ceph实现三种存储的步骤。 ","date":"2020-12-19","objectID":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:0","tags":["Ceph"],"title":"Ceph存储基本使用","uri":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"主机列表 外部ip 集群ip host 角色 172.16.200.101 10.16.200.101 ceph-mon1 mon、osd节点 172.16.200.102 10.16.200.102 ceph-mon2 mon、osd节点 172.16.200.103 10.16.200.103 ceph-mon3 mon、osd节点 172.16.200.104 10.16.200.104 ceph-osd4 osd节点 ","date":"2020-12-19","objectID":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:1","tags":["Ceph"],"title":"Ceph存储基本使用","uri":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"块存储 块存储可以为客户端提供基于块的持久化存储，通常是格式化成单独的磁盘使用。客户可以灵活的使用这个磁盘，就像新加的磁盘一样，直接作为裸设备使用或者格式化成文件系统挂载到特定的目录。 基本使用 1、创建存储池 $ ceph osd pool create mypool 128 128 2、创建RBD镜像，名称为myimage，大小为10G。feature为layering $ rbd create myimage -s 10G --image-feature layering -p mypool $ rbd ls -p mypool myimage 3、查看myimage的详细信息 $ root ~ \u003e\u003e\u003e rbd info myimage -p mypool rbd image 'myimage': size 10 GiB in 2560 objects order 22 (4 MiB objects) id: 16966b8b4567 block_name_prefix: rbd_data.16966b8b4567 format: 2 features: layering op_features: flags: create_timestamp: Thu Dec 17 16:26:28 2020 4、将myimage映射为块设备 $ $ rbd map myimage -p mypool /dev/rbd0 myimage映射为/dev/rbd0块设备 同时showmapped子命令可以查看所有已经映射的卷 $ $ rbd showmapped id pool image snap device 0 mypool myimage - /dev/rbd0 5、一旦RBD的image映射到了系统上，就应该在其上创建文件系统以使用这个设备 $ mkfs.xfs /dev/rbd0 #格式化为xfs格式 Discarding blocks...Done. meta-data=/dev/rbd0 isize=512 agcount=16, agsize=163840 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=2621440, imaxpct=25 = sunit=1024 swidth=1024 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=8 blks, lazy-count=1 realtime =none $ mkdir /ceph/rbd -p #创建挂载点 $ mount /dev/rbd0 /ceph/rbd #挂载 $ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 3.9G 0 3.9G 0% /dev tmpfs 3.9G 0 3.9G 0% /dev/shm tmpfs 3.9G 74M 3.9G 2% /run tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup /dev/mapper/centos-root 44G 17G 28G 37% / /dev/sda1 1014M 132M 883M 13% /boot tmpfs 799M 0 799M 0% /run/user/0 tmpfs 3.9G 52K 3.9G 1% /var/lib/ceph/osd/ceph-0 tmpfs 3.9G 52K 3.9G 1% /var/lib/ceph/osd/ceph-1 /dev/rbd0 10G 33M 10G 1% /ceph/rbd 6、写入数据 $ dd if=/dev/zero of=/ceph/rbd/file count=100 bs=1M 100+0 records in 100+0 records out 104857600 bytes (105 MB) copied, 0.135906 s, 772 MB/s $ cd /ceph/rbd/ $ ls -lh total 100M -rw-r--r--. 1 root root 100M Dec 17 16:45 file 扩展RBD镜像 ceph块设备支持动态的增加或者减少RBD的大小，但需要底层的文件系统同时作出修改才能使容量生效。但是：一般不建议做缩盘操作，可能会造成数据丢失 1、扩展RBD镜像myimage至20G $ rbd resize myimage -s 20G -p mypool Resizing image: 100% complete...done. $ rbd info myimage -p mypool rbd image 'myimage': size 20 GiB in 5120 objects order 22 (4 MiB objects) id: 16966b8b4567 block_name_prefix: rbd_data.16966b8b4567 format: 2 features: layering op_features: flags: create_timestamp: Thu Dec 17 16:26:28 2020 扩展后size大小变成了20G 2、调整文件系统。虽然rbd image已经调整为20G，但是对于文件系统还是之前的10G，需手动扩展文件系统中的大小。由于之前格式为xfs文件系统，故这里使用xfs_growfs来调整。 $ xfs_growfs /ceph/rbd meta-data=/dev/rbd0 isize=512 agcount=16, agsize=163840 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=2621440, imaxpct=25 = sunit=1024 swidth=1024 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=8 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 2621440 to 5242880 $ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 3.9G 0 3.9G 0% /dev tmpfs 3.9G 0 3.9G 0% /dev/shm tmpfs 3.9G 74M 3.9G 2% /run tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup /dev/mapper/centos-root 44G 17G 28G 37% / /dev/sda1 1014M 132M 883M 13% /boot tmpfs 799M 0 799M 0% /run/user/0 tmpfs 3.9G 52K 3.9G 1% /var/lib/ceph/osd/ceph-0 tmpfs 3.9G 52K 3.9G 1% /var/lib/ceph/osd/ceph-1 /dev/rbd0 20G 134M 20G 1% /ceph/rbd RBD快照 ceph支持快照，它是一个基于时间点只读的RBD镜像副本。可以通过创建快照并恢复其原始数据，报错RBD镜像的状态。 1、创建测试文件 $ echo \"This is a test file before snapshot\" \u003e ./snap-test.txt $ cat snap-test.txt This is a test file before snapshot $ ls -ltrh total 101M -rw-r--r--. 1 root root 100M Dec 17 16:45 file -rw-r--r--. 1 root root 36 Dec 17 17:14 snap-test.txt 2、创建快照:语法是rbd snap create [--pool \u003cpool\u003e] [--image \u003cimage\u003e] [--snap \u003csnap\u003e]或者rbd \u003cpool-name\u003e/]\u003cimage-name\u003e@\u003csnapshot-name\u003e $ rbd snap create mypool/myimage@snap01 $ rbd snap ls myimage -p mypool SNAPID NAME SIZE TIMESTAMP 4 snap01 20 GiB Thu Dec 17 17:16:33 2020 3、为了测试快照的恢复的功能，从文件系统删除文件 $ rm -rf file snap-test.txt $ ls $ 4、回滚快照实现文件的回滚 $ rbd snap rollback mypool/myimage@sna","date":"2020-12-19","objectID":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:2","tags":["Ceph"],"title":"Ceph存储基本使用","uri":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"文件存储 Ceph的文件存储全称是CephFS,它是一个与POSIX兼容的分布式文件系统，底层使用Ceph rados存储数据。要能正常使用CephFS，集群中应至少包含一个元数据服务器MDS(MataData Server)。 创建MDS 创建MDS可以直接使用ceph-deploy中的mds create子命令进行创建 $ ceph-deploy mds create ceph-mon1 ceph-mon2 ceph-mon3 添加MDS节点 1、在需要作为新MDS的节点创建新的目录,${id}可以直接使用hostname $ mkdir -pv /var/lib/ceph/mds/ceph-${id} 2、使用cephx生成秘钥 $ sudo ceph auth get-or-create mds.${id} mon 'profile mds' mgr 'profile mds' mds 'allow *' osd 'allow *' \u003e /var/lib/ceph/mds/ceph-${id}/keyring 3、在新的MDS节点启动MDS服务 $ sudo systemctl start ceph-mds@${id} 4、MDS集群的状态应当如下 $ ceph mds stat cephfs-1/1/1 up {0=ceph-mon1=up:active}, 1 up:standby 剔除MDS节点 1、停止对应节点的ceph-mds服务 $ sudo systemctl stop ceph-mds@${id} 2、删除创建的MDS文件夹 $ sudo rm -rf /var/lib/ceph/mds/ceph-${id} 创建存储池 一个ceph文件系统至少需要包含两个存储池，一个用来存储数据，另一个则是用来存储元数据。配置这些存储池时需考虑： 为元数据存储池设置较高的副本水平，因为此存储池丢失任何数据都会导致整个文件系统失效。 为元数据存储池分配低延时存储器（像 SSD ），因为它会直接影响到客户端的操作延时。 接下来创建两个存储池，一个叫cephfs_data,一个叫ceph_metadata: $ ceph osd pool create cephfs_data 128 $ ceph osd pool create cephfs_metadata 128 创建文件系统 在MDS和存储池创建完成后就可以创建文件系统了。创建文件系统的子命令是fs new。ceph fs new \u003cfs_name\u003e $ ceph fs new cephfs cephfs_metadata cephfs_data $ ceph fs ls name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] 使用ceph fs status查看新建的文件系统的状态 root /etc/ceph \u003e\u003e\u003e ceph fs status cephfs - 0 clients ====== +------+--------+-----------+---------------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+--------+-----------+---------------+-------+-------+ | 0 | active | ceph-mon1 | Reqs: 0 /s | 10 | 13 | +------+--------+-----------+---------------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs_metadata | metadata | 2635 | 34.1G | | cephfs_data | data | 0 | 34.1G | +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ | ceph-mon3 | | ceph-mon2 | +-------------+ MDS version: ceph version 13.2.10 (564bdc4ae87418a232fc901524470e1a0f76d641) mimic (stable) 文件系统创建完成后，MDS服务器就能达到active的状态了 $ ceph mds stat cephfs-1/1/1 up {0=ceph-mon1=up:active}, 2 up:standby 挂载文件存储 要挂载 Ceph 文件系统，如果你知道mon节点ip地址可以用 mount 命令、或者用 mount.ceph 工具来自动解析监视器 IP 地址。由于mount命令与mount.ceph基本一致，所以仅介绍mount命令挂载。 1、创建挂载点 $ mkdir -p /ceph/cephfs/ 2、挂载文件系统 默认情况下，ceph部署完成后会自动开启cephx认证，所以在挂载时需要指明用户名及秘钥。 $ cat ./ceph.client.admin.keyring [client.admin] key = AQA7vdVfBt21JBAAOL2oY8iHCaqNc2Fctv588w== caps mds = \"allow *\" caps mgr = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\" $ mount -t ceph 172.16.200.101:/ /ceph/cephfs/ -o name=admin,secret=AQA7vdVfBt21JBAAOL2oY8iHCaqNc2Fctv588w== root /etc/ceph \u003e\u003e\u003e df -h | grep cephfs 172.16.200.101:/ 160G 37G 124G 23% /ceph/cephfs ceph集群总共是160G 官方建议以更加安全的方式挂载cephFS，就需要把秘钥文件导出到一个文件中，挂载时指明文件名。这样就避免了secret残留在bash中。 $ echo \"AQA7vdVfBt21JBAAOL2oY8iHCaqNc2Fctv588w==\" \u003e /etc/cepg/ceph-admin.secret $ mount -t ceph 172.16.200.101:/ /ceph/cephfs/ -o name=admin,secretfile=/etc/ceph/ceph-admin.secret 3、写入/etc/fstab 为了保证服务器重启后挂载仍然有效，需要将挂载信息写入到/etc/fstab中。 $ echo \"172.16.200.101:/ /ceph/cephfs/ ceph name=admin,secretfile=/etc/ceph/ceph-admin.secret,noatime 0 2\" \u003e\u003e/etc/fstab ","date":"2020-12-19","objectID":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:3","tags":["Ceph"],"title":"Ceph存储基本使用","uri":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["Ceph"],"content":"对象存储 Ceph 对象网关是一个构建在 librados 之上的对象存储接口，它为应用程序访问Ceph 存储集群提供了一个 RESTful 风格的网关 。同时支持Amazon S3风格及Openstack Swift风格的对象存储接口。从F版开始Ceph运行在Civetweb，不再需要Apache+FastCGI。造成的影响是Civetweb不支持HTTPS,想要以HTTPS的方式访问对象网关，需要在外层部署Nginx等反向代理软件。 开启对象存储 对象存储需要本地安装ceph-radosgw软件包，运行的方法也很简单，直接使用ceph-deploy rgw create ceph-mon1来开启对象网关服务。默认情况下，对象网关监听7480端口。未授权的情况下访问该端口会提示匿名用户。 $ curl http://127.0.0.1:7480/ \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"\u003e \u003cOwner\u003e \u003cID\u003eanonymous\u003c/ID\u003e \u003cDisplayName\u003e \u003c/DisplayName\u003e \u003c/Owner\u003e \u003cBuckets\u003e \u003c/Buckets\u003e \u003c/ListAllMyBucketsResult\u003e 修改默认端口 修改默认端口需要在ceph.conf中添加定义指明新的rados网关的端口。假设修改为8080，修改前确保没有其他程序占用8080端口 $ cat \u003c\u003c EOF \u003e\u003e /etc/ceph/ceph.conf [client.rgw.ceph-mon1] rgw_frontends = \"civetweb port=8080\" EOF #推送配置到其他节点 $ ceph-deploy --overwrite-conf config push ceph-mon2 ceph-mon3 ceph-osd4 #重启对象网关服务 $ systemctl restart ceph-radosgw@rgw.ceph-mon1.service 使用对象存储 S3风格 新建用户 在mon节点(这里是ceph-mon1节点)创建新用户ceph-rgw-testuser $ radosgw-admin user create --uid=\"ceph-rgw-testuser\" --display-name=\"First RGW User\" { \"user_id\": \"ceph-rgw-testuser\", \"display_name\": \"First RGW User\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [], \"keys\": [ { \"user\": \"ceph-rgw-testuser\", \"access_key\": \"3TOIAIEP4JFT0SJKLWY0\", \"secret_key\": \"7wZ3Kp2qAj03PAgB35JxdYMcanYJRe8XOLqMzTDZ\" } ], \"swift_keys\": [], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [], \"type\": \"rgw\", \"mfa_ids\": [] } keys中的access_key及secret_key是客户端访问对象网关必不可少的，注意保存！ access_key或者secret_key中如果出现了\\,需要手动将其删除。防止部分客户端无法处理转义的值。 测试S3访问 1、安装python-boto包 $ yum install -y python-boto 2、创建测试文件 $ cat s3.py #!/usr/bin/env python import boto import boto.s3.connection access_key = '3TOIAIEP4JFT0SJKLWY0' secret_key = '7wZ3Kp2qAj03PAgB35JxdYMcanYJRe8XOLqMzTDZ' conn = boto.connect_s3( aws_access_key_id = access_key, aws_secret_access_key = secret_key, host = 'ceph-mon1', port = 8080, is_secure=False, calling_format = boto.s3.connection.OrdinaryCallingFormat(), ) bucket = conn.create_bucket('ceph-s3-bucket') for bucket in conn.get_all_buckets(): print (\"{name}\".format(name = bucket.name)) 3、运行测试文件 $ python s3.py ceph-s3-bucket swift风格 新建子用户 $ radosgw-admin subuser create --uid=ceph-rgw-testuser --subuser=ceph-rgw-testuser:swift --access=full 这里为了方便直接使用上面创建的用户ceph-rgw-testuser 生成secret key $ radosgw-admin key create --subuser=ceph-rgw-testuser:swift --key-type=swift --gen-secret { \"user_id\": \"ceph-rgw-testuser\", \"display_name\": \"First RGW User\", \"email\": \"\", \"suspended\": 0, \"max_buckets\": 1000, \"auid\": 0, \"subusers\": [ { \"id\": \"ceph-rgw-testuser:swift\", \"permissions\": \"full-control\" } ], \"keys\": [ { \"user\": \"ceph-rgw-testuser\", \"access_key\": \"3TOIAIEP4JFT0SJKLWY0\", \"secret_key\": \"7wZ3Kp2qAj03PAgB35JxdYMcanYJRe8XOLqMzTDZ\" } ], \"swift_keys\": [ { \"user\": \"ceph-rgw-testuser:swift\", \"secret_key\": \"ldUzAqsrzyGJohYNC57sB3oOmPqRtazniCnKCXPq\" } ], \"caps\": [], \"op_mask\": \"read, write, delete\", \"default_placement\": \"\", \"placement_tags\": [], \"bucket_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"user_quota\": { \"enabled\": false, \"check_on_raw\": false, \"max_size\": -1, \"max_size_kb\": 0, \"max_objects\": -1 }, \"temp_url_keys\": [], \"type\": \"rgw\", \"mfa_ids\": [] } swift_keys中的secret_key是客户端访问对象网关必不可少的，注意保存！ 测试swift客户端访问 1、安装swift客户端 $ pip install --upgrade setuptools $ pip install --upgrade python-swiftclient 2、测试访问,成功显示刚才s3客户端创建的bucket $ swift -A http://172.16.200.101:8080/auth/1.0 -U ceph-rgw-testuser:swift -K 'ldUzAqsrzyGJohYNC57sB3oOmPqRtazniCnKCXPq' list ceph-s3-bucket ","date":"2020-12-19","objectID":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:4","tags":["Ceph"],"title":"Ceph存储基本使用","uri":"/posts/ceph%E5%AD%98%E5%82%A8%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["Kubernetes","Ceph"],"content":"前言：本篇博客主要介绍kubernetes集群如何与ceph集群进行对接，将ceph作为kubernetes的后端存储实现pvc的动态供应。本文中的ceph和kubernetes为一套集群。 ","date":"2020-12-15","objectID":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/:0:0","tags":["Kubernetes","Ceph"],"title":"Kubernetes对接Ceph存储","uri":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/"},{"categories":["Kubernetes","Ceph"],"content":"主机列表 K8s集群角色 ceph集群角色 IP 内核 master-1 mon、osd节点 172.16.200.101 4.4.247-1.el7.elrepo.x86_64 master-2 mon、osd节点 172.16.200.102 4.4.247-1.el7.elrepo.x86_64 master-3 mon、osd节点 172.16.200.103 4.4.247-1.el7.elrepo.x86_64 node-1 osd节点 172.16.200.104 4.4.247-1.el7.elrepo.x86_64 升级内核的原因：ceph官方推荐4.1.4版本以上的版本 对接前需要保证ceph集群已经处于健康状态 root ~ \u003e\u003e\u003e ceph health HEALTH_OK ","date":"2020-12-15","objectID":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/:0:1","tags":["Kubernetes","Ceph"],"title":"Kubernetes对接Ceph存储","uri":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/"},{"categories":["Kubernetes","Ceph"],"content":"对接过程 创建存储池mypool root ~ \u003e\u003e\u003e ceph osd pool create mypool 128 128 #设置PG和PGP为128 pool 'mypool' created root ~ \u003e\u003e\u003e ceph osd pool ls mypool 创建secret对象 在ceph中创建kube用户并授予相应的权限 root ~ \u003e\u003e\u003e ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children,allow rwx pool=mypool' root ~/k8s/ceph \u003e\u003e\u003e cat ceph-secret.yaml apiVersion: v1 kind: Secret metadata: name: ceph-kube-secret namespace: default data: key: \"ceph auth get-key client.kube | base64\" type: kubernetes.io/rbd --- apiVersion: v1 kind: Secret metadata: name: ceph-admin-secret namespace: default data: key: \"ceph auth get-key client.admin | base64\" type: kubernetes.io/rbd 创建上述资源 root ~/k8s/ceph \u003e\u003e\u003e kubectl apply -f ceph-secret.yaml 创建StorageClass root ~/k8s/ceph \u003e\u003e\u003e cat ceph-storageclass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-storageclass provisioner: kubernetes.io/rbd parameters: monitors: 172.16.200.101:6789,172.16.200.102:6789,172.16.200.103:6789 #定义mon节点，ceph-mon默认监听6789端口 adminId: admin adminSecretName: ceph-admin-secret adminSecretNamespace: default pool: mypool #定义ceph存储池 userId: kube userSecretName: ceph-kube-secret userSecretNamespace: default fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" root ~/k8s/ceph \u003e\u003e\u003e kubectl apply -f ceph-storageclass.yaml root ~/k8s/ceph \u003e\u003e\u003e kubectl get sc NAME PROVISIONER AGE ceph-storageclass kubernetes.io/rb 87s 创建pvc root ~/k8s/ceph \u003e\u003e\u003e cat ceph-storageclass-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ceph-test-claim spec: storageClassName: ceph-storageclass #对应storageclass的名称 accessModes: - ReadWriteOnce resources: requests: storage: 5Gi #申请5G空间的存储空间 ReadWriteOnce: 读写权限，并且只能被单个Node挂载 ReadOnlyMany:只读权限，允许被多个Node挂载 ReadWriteMany:读写权限，允许被多个Node挂载 创建测试pod root ~/k8s/ceph \u003e\u003e\u003e cat ceph-busybox-pod.yaml apiVersion: v1 kind: Pod metadata: name: ceph-pod spec: containers: - name: ceph-busybox image: busybox:1.32.0 command: [\"/bin/sh\",\"-c\",\"tail -f /etc/resolv.conf\"] volumeMounts: - name: ceph-volume mountPath: /usr/share/busybox readOnly: false volumes: - name: ceph-volume persistentVolumeClaim: claimName: ceph-test-claim 查看pod的状态，如果发现pod并未处于running状态并且事件中有如下的报错。并且k8s集群的各个节点确认已安装与ceph版本一致的ceph-common软件包，这时需要更新storageclass的Provisioner，用外部的提供者代替k8s自带的kubernetes.io/rbd以实现pvc的动态供应。 rbd: create volume failed, err: failed to create rbd image: executable file not found in $PATH: ","date":"2020-12-15","objectID":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/:0:2","tags":["Kubernetes","Ceph"],"title":"Kubernetes对接Ceph存储","uri":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/"},{"categories":["Kubernetes","Ceph"],"content":"排障步骤 出现这个报错问题的原因其实很简单：gcr.io中自带的kube-controller-manager镜像没有自带rbd子命令。所以如果是镜像方式启动的kube-controller-manager就会遇到这个问题。 Github相关issue：Error creating rbd image: executable file not found in $PATH · Issue #38923 · kubernetes/kubernetes · GitHub 部署外部provisioner 定义外部的provisioner，由这个provisioner拿之前定义的秘钥创建ceph镜像。同时对外提供ceph.com/rbd的供应者。相关的定义如下： root ~/k8s/ceph \u003e\u003e\u003e cat storageclass-fix-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: rbd-provisioner namespace: kube-system spec: replicas: 1 selector: matchLabels: app: rbd-provisioner strategy: type: Recreate template: metadata: labels: app: rbd-provisioner spec: containers: - name: rbd-provisioner image: \"quay.io/external_storage/rbd-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/rbd serviceAccountName: persistent-volume-binder 这里必须明确指明ServiceAccountName为persistent-volume-binder，默认的default账户没有权限列出相应资源 创建该deployment资源 root ~/k8s/ceph \u003e\u003e\u003e kubectl apply -f storageclass-fix-deployment.yaml 更新原有storageclass ... metadata: name: ceph-storageclass #provisioner: kubernetes.io/rbd provisioner: ceph.com/rbd ... root ~/k8s/ceph \u003e\u003e\u003e kubectl get sc NAME PROVISIONER AGE ceph-storageclass ceph.com/rbd 64m 重新创建pvc及pod root ~/k8s/ceph \u003e\u003e\u003e kubectl delete -f ceph-storageclass-pvc.yaml \u0026\u0026 kubectl apply -f ceph-storageclass-pvc.yaml root ~/k8s/ceph \u003e\u003e\u003e kubectl delete -f ceph-busybox-pod.yaml \u0026\u0026 kubectl apply -f ceph-busybox-pod.yaml pod成功进入running状态 root ~/k8s/ceph \u003e\u003e\u003e kubectl get pods NAME READY STATUS RESTARTS AGE ceph-pod 1/1 Running 0 47s 验证PV root ~/k8s/ceph \u003e\u003e\u003e kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/pvc-9579436e-5122-48c1-871d-2e18b8e42863 5Gi RWO Delete Bound default/ceph-test-claim ceph-storageclass 112s 相应的image已映射为块设备挂载到/usr/share/busybox/目录下 root ~/k8s/ceph \u003e\u003e\u003e kubectl exec -it ceph-pod -- /bin/sh / # mount | grep share /dev/rbd0 on /usr/share/busybox type ext4 (rw,seclabel,relatime,stripe=1024,data=ordered) mypool池中顺利生成相应的image root ~/k8s/ceph \u003e\u003e\u003e rbd ls -p mypool kubernetes-dynamic-pvc-54b5fec8-3e1d-11eb-ab76-925ce2bb963b ","date":"2020-12-15","objectID":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/:0:3","tags":["Kubernetes","Ceph"],"title":"Kubernetes对接Ceph存储","uri":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/"},{"categories":["Kubernetes","Ceph"],"content":"常见错误 1、rbd image无法映射为块设备 Warning FailedMount 72s (x2 over 2m13s) kubelet, node-1 MountVolume.WaitForAttach failed for volume \"pvc-c0683658-308a-4fc0-b330-c813c1cad850\" : rbd: map failed exit status 110, rbd output: rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (110) Connection timed out #对应节点的dmesg日志： [843372.414046] libceph: mon1 172.16.200.102:6789 feature set mismatch, my 106b84a842a42 \u003c server's 40106b84a842a42, missing 400000000000000 [843372.415663] libceph: mon1 172.16.200.102:6789 missing required protocol features [843382.430314] libceph: mon1 172.16.200.102:6789 feature set mismatch, my 106b84a842a42 \u003c server's 40106b84a842a42, missing 400000000000000 [843382.432031] libceph: mon1 172.16.200.102:6789 missing required protocol features [843393.566967] libceph: mon1 172.16.200.102:6789 feature set mismatch, my 106b84a842a42 \u003c server's 40106b84a842a42, missing 400000000000000 [843393.569506] libceph: mon1 172.16.200.102:6789 missing required protocol features [843403.421791] libceph: mon2 172.16.200.103:6789 feature set mismatch, my 106b84a842a42 \u003c server's 40106b84a842a42, missing 400000000000000 [843403.424892] libceph: mon2 172.16.200.103:6789 missing required protocol features [843413.405831] libceph: mon0 172.16.200.101:6789 feature set mismatch, my 106b84a842a42 \u003c server's 40106b84a842a42, missing 400000000000000 原因：linux内核版本\u003c4.5时不支持feature flag 400000000000000，手动关闭即可。ceph osd crush tunables hammer ","date":"2020-12-15","objectID":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/:0:4","tags":["Kubernetes","Ceph"],"title":"Kubernetes对接Ceph存储","uri":"/posts/kubernetes%E5%AF%B9%E6%8E%A5ceph%E5%AD%98%E5%82%A8/"},{"categories":["Kubernetes"],"content":"前言：pod CIDR是指Kubernetes为pod分配的ip地址段，默认情况下使用kubesparay部署时默认的CIDR是10.233.64.0/16。换算出来的可用地址是10.233.64.1-10.233.127.254。可为64个节点分配pod的ip地址。如果集群扩容超过了64，如何修改podCIDR以允许大量的主机呢？ ip地址在线换算： “ip地址在线计算器 (520101.com) ” 修改pod CIDR会触及工作负载，需在业务量较小时进行。 ","date":"2020-12-07","objectID":"/posts/calico%E4%BF%AE%E6%94%B9podcidr/:0:0","tags":["Kubernetes"],"title":"calico修改podCIDR","uri":"/posts/calico%E4%BF%AE%E6%94%B9podcidr/"},{"categories":["Kubernetes"],"content":"配置calicoctl calicoctl的默认配置文件是/etc/calico/calicoctl.cfg，如果发现使用calicoctl get node提示no etcd endpoints specified则需要手动添加该配置文件，样式如下： apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \"etcdv3\" etcdEndpoints: \"https://10.4.7.11:2379,https://10.4.7.12:2379,https://10.4.7.21:2379\" etcdKeyFile: \"/opt/etcd/ssl/server-key.pem\" etcdCertFile: \"/opt/etcd/ssl/server.pem\" etcdCACertFile: \"/opt/etcd/ssl/ca.pem\" 如果不清楚etcd的证书路径可以使用ps -ef | grep etcd查看具体的证书路径。 ","date":"2020-12-07","objectID":"/posts/calico%E4%BF%AE%E6%94%B9podcidr/:0:1","tags":["Kubernetes"],"title":"calico修改podCIDR","uri":"/posts/calico%E4%BF%AE%E6%94%B9podcidr/"},{"categories":["Kubernetes"],"content":"配置ippool 配置好calicoctl后可以使用calicoctl get ippool查看默认的CIDR范围 [root@master-1 ~]# calicoctl get ippool NAME CIDR default-pool 10.233.64.0/18 导出后复制为自定义配置并应用 [root@master-1 ~]# calicoctl get ippool -o yaml \u003e default-ippool.yaml [root@master-1 ~]# cat default-ippool.yaml apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: default-pool spec: blockSize: 24 cidr: 10.233.64.0/18 ipipMode: Never natOutgoing: true disabled: true #应用my-ippool后添加改行将该ippool禁用 [root@master-1 ~]# cp default-ippool.yaml my-ippool.yaml [root@master-1 ~]# 修改手动my-ippool中的cidr值：如10.10.10.0/16，地址范围是10.10.0.1-10.10.255.254,可容纳255台主机 #应用my-ippool.yaml [root@master-1 ~] calicoctl apply -f my-ippool.yaml #查看当前ippool状态 [root@master-1 roles]# calicoctl get ippool -o wide NAME CIDR NAT IPIPMODE DISABLED default-pool 10.233.64.0/18 true Never true my-ippool 10.10.10.0/16 true Never false 新建的ippool与默认的ippool的cidr不能有地址重叠，否则会无法创建。 修改cni中定义的地址段根据cni及calico规范，calico部署成功后会在/etc/cni/net.d/生成10-calico.conflist文件。修改其中的ipv4_pools值为my-ippool中定义的CIDR范围。注意各个节点都需要修改并且calico.conflist.template需同步修改。修改后保存退出即可。 ","date":"2020-12-07","objectID":"/posts/calico%E4%BF%AE%E6%94%B9podcidr/:0:2","tags":["Kubernetes"],"title":"calico修改podCIDR","uri":"/posts/calico%E4%BF%AE%E6%94%B9podcidr/"},{"categories":["Kubernetes"],"content":"重启所有pod 修改pod CIDR后为使配置生效，手动重启所有pod。deployment，daemonset等控制器会自动拉起pod。 kubectl delete pods --all ","date":"2020-12-07","objectID":"/posts/calico%E4%BF%AE%E6%94%B9podcidr/:0:3","tags":["Kubernetes"],"title":"calico修改podCIDR","uri":"/posts/calico%E4%BF%AE%E6%94%B9podcidr/"},{"categories":["Kubernetes"],"content":"RBAC(Role Based Access Control 基于角色的访问控制)。在使用kubeadm部署kubernetes集群时作为默认的鉴权模式开启。二进制部署时需指明api-server的启动参数--authorization-mode=RBAC来开启RBAC认证。 ","date":"2020-11-23","objectID":"/posts/kubernetes%E4%B9%8Brbac%E9%89%B4%E6%9D%83/:0:0","tags":["Kubernetes"],"title":"Kubernetes之RBAC鉴权","uri":"/posts/kubernetes%E4%B9%8Brbac%E9%89%B4%E6%9D%83/"},{"categories":["Kubernetes"],"content":"资源对象说明 RBAC定义了四种顶级资源来定义权限 role clusterrole rolebinding clusterrolebinding role及rolebinding是在名称空间内的，仅能对指定的namespace生效。而clusterrole和clusterrolebinding是集群范围内的，可以对整个集群范围内的资源进行授权。 定义的权限都是允许权限，因为k8s默认拒绝所有权限。 Role(角色) Role(角色)是一组权限定义。比如创建一个test-namespace中的pod-reader角色 [root@k8s-master ~]# kubectl create role pod-reader --verb=get,list,watch \\ --resource=pods,pods/status --namespace=test-namespace \\ --dry-run=client -o yaml \u003e pod-reader.yml [root@k8s-master ~]# cat pod-reader.yml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-reader namespace: test-namespace rules: - apiGroups: [\"\"] resources: [\"pods\",\"pods/status\"] verbs: [\"get\",\"list\",\"watch\"] 该角色表明test-namespace命名空间中的pod-reader角色拥有对test-namespace命名空间中所有pod的get，list及watch权限。 其中，rules中定义的三项为最重要的内容： apiGroups：即api资源组,下表即为当前Kubernetes版本支持的资源组及其下资源名称。 [root@k8s-master ~]# kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND pods po true Pod podtemplates true PodTemplate replicationcontrollers rc true ReplicationController resourcequotas quota true ResourceQuota secrets true Secret serviceaccounts sa true ServiceAccount services svc true Service ... deployments deploy apps true Deployment replicasets rs apps true ReplicaSet statefulsets sts apps true StatefulSet tokenreviews authentication.k8s.io false TokenReview ... apiGroups是根据resources来确定的，这里是访问pods的权限，所以apiGroup为空值代表核心组。其他的比如对deployments的权限，apiGroup就是apps，以此类推… resources：为上表中的NAME字段。也可为其子资源，比如pod中的日志等。 verbs：定义的是该角色所具有的具体权限，如get、watch、list、patch、delete等。 创建role时都需要写明namespace，如果创建的role属于default名称空间时最好也能带上namespace: default以养成习惯。 ClusterRole(集群角色) 相对于role仅对默认的名称空间有效外，clusterrole是集群范围内的权限,所以无需定义namespace。集群角色不仅能访问集群范围的资源，比如Node。同时也包含非资源型路径，如\"/healthz\"。按上面的pod-reader的列子类比 [root@k8s-master ~]# kubectl create clusterrole pod-cluster-reader \\ --verb=get,list,watch --resource=pods,pods/status \\ --dry-run=client -o yaml \u003e pod-cluster-reader.yml [root@k8s-master ~]# cat pod-cluster-reader.yml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: pod-cluster-reader rules: - apiGroups: [\"\"] resources: [\"pods\",\"pods/status\"] verbs: [\"get\",\"list\",\"watch\"] 该集权角色拥有全部命名空间下pod的get、list及watch权限。 RoleBinding(角色绑定) Rolebinding(角色绑定)的意思是将用户与角色关联起来，让用户去扮演角色。从而使用户获得角色中定义的权限。如下面的定义表示，将用户jane去扮演pod-reader这个角色，而pod-reader按此前的定义拥有对test-namespace中pod资源的get、list及watch权限。那么在应用了这个角色绑定之后，jane用户就有权限访问test-namespace中的pod资源了。 apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: test-namespace subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io 当创建一个rolebinding之后，其中的roleRef字段中的定义内容是不可变的，如果要改变roleRef中的内容，必须删除该rolebinding并重新创建。 从上图可以看出，rolebinding起到了媒介的作用，让用户和角色关联起来，使用户能够扮演role，从而获得role的权限，进而能够管理pod。 rolebinding可以绑定clusterrole。实现的效果是仅需定义一个集权范围的角色clusterrole，在具体的namespace中用rolebinding去绑定。 这样做的目的是能让角色进行复用，避免在各自的名称空间中重复声明role。 比如这个官方的示例： apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-secrets namespace: development subjects: - kind: User name: dave apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 集群角色secret-reader具有读取全部名称空间下secret的权限，但由于rolebinding只在development名称空间下，所以只能读development命名空间内的secret。同样的也能拿到test-namespace命名空间中绑定secret-reader以让用户能读取test-namespace命名空间中的secret。 ClusterRoleBinding(集群角色绑定) apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: read-cluster-pods subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: pod-reader apiGroup: rbac.authorization.k8s.io 相比于rolebinding可以绑定role及clusterrole，clusterrolebing只能绑定clusterrole，上面的授权表明jane用户可以读取全部集群范围内的所有pod的信息。 常见的rule示例 这些例子都是来自kubernetes官方站点，你可以在这里找到他们： Using RBAC Authorization | Kubernetes 1、聚合角色 rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"batch\", \"extensions\"] resources: [\"jobs\"] verbs: [\"get\", \"list\", \"wat","date":"2020-11-23","objectID":"/posts/kubernetes%E4%B9%8Brbac%E9%89%B4%E6%9D%83/:0:1","tags":["Kubernetes"],"title":"Kubernetes之RBAC鉴权","uri":"/posts/kubernetes%E4%B9%8Brbac%E9%89%B4%E6%9D%83/"},{"categories":["Kubernetes"],"content":"关于Service Account 在rolebinding或者是clusterrolebing中的subject字段是用来定义用户的。在kubernetes中有两种用户： 普通用户，一般在指集群外操作集群的人，用kubeconfig配置文件进行鉴权 serviceaccount，集群内部定义的用来给pod访问api-server鉴权时所使用的认证账户 区别在与:普通用户不是由kubernetes管理，而serviceaccount属于一种kubernetes资源类型，并且是属于某个namespace的。 一般而言，每个名称空间内都有一个默认的serviceaccount账户。并且如果手动删除这个serviceaccount账户，kubernetes还会自动创建出来。 [root@master-1 ~]# kubectl delete sa default #sa即为serviceaccount的缩写 serviceaccount \"default\" deleted [root@master-1 ~]# kubectl get sa NAME SECRETS AGE default 1 2s 创建service account的语法很简单,比如创建kube-system名称空间下的test账号： kubectl create serviceaccount test --namespace=kube-system 一个serviceaccount会包含多个secret文件，以应对特定场景下的加密需求。 [root@k8s-master ~]# kubectl describe serviceaccount default Name: default Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Image pull secrets: \u003cnone\u003e Mountable secrets: default-token-djqcv Tokens: default-token-djqcv Events: \u003cnone\u003e 1）名为Tokens的secret是用于访问API Server的Secret 2）imagePullSecrets是用于下载镜像文件的认证secret 默认情况下，如果pod在创建时没有指明serviceAccount账户名，那么默认的账户default会被挂在到pod内。 [root@master-1 ~]# kubectl get pods nginx-deployment-699fc59b58-229wj -o yaml | grep serviceAccount serviceAccount: default serviceAccountName: default 当然也可以在.spec.serviceAccountName指明要用的serviceaccount 对service accout的授权管理 1、关联my-namespace中的my-sa账户，使其在my-namespace中拥有只读权限 kubectl create rolebinding my-sa-view \\ --clusterrole=view \\ --serviceaccount=my-namespace:my-sa \\ --namespace=my-namespace 2、给my-namespace下所有的service account授予只读权限 kubectl create rolebinding serviceaccounts-view \\ --clusterrole=view \\ --group=system:serviceaccounts:my-namespace \\ --namespace=my-namespace serviceaccount的规范写法是: 3、给所有的service account授予只读权限 kubectl create clusterrolebinding serviceaccounts-view \\ --clusterrole=view \\ --group=system:serviceaccounts 4、给所有的service account授予超级用户权限（强烈不推荐） kubectl create clusterrolebinding serviceaccounts-cluster-admin \\ --clusterrole=cluster-admin \\ --group=system:serviceaccounts ","date":"2020-11-23","objectID":"/posts/kubernetes%E4%B9%8Brbac%E9%89%B4%E6%9D%83/:0:2","tags":["Kubernetes"],"title":"Kubernetes之RBAC鉴权","uri":"/posts/kubernetes%E4%B9%8Brbac%E9%89%B4%E6%9D%83/"},{"categories":["Kubernetes"],"content":"Dashboard应用授权 Dashboard 是基于网页的 Kubernetes 用户界面。 你可以使用 Dashboard 将容器应用部署到 Kubernetes 集群中，也可以对容器应用排错，还能管理集群资源。 你可以使用 Dashboard 获取运行在集群中的应用的概览信息，也可以创建或者修改 Kubernetes 资源 （如 Deployment，Job，DaemonSet 等等）。 官网地址：GitHub - kubernetes/dashboard: General-purpose web UI for Kubernetes clusters 安装方法很简单： kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml 为了使dashboard能在集群外访问，需修改其中的Service定义，将类型改成NodePort --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: type: NodePort #增加类型为NodePort ports: - port: 443 targetPort: 8443 nodePort: 32443 #指定nodePort的端口为32443 selector: k8s-app: kubernetes-dashboard 重点看下授权部分的配置： --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard rules: - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\", \"kubernetes-dashboard-csrf\"] verbs: [\"get\", \"update\", \"delete\"] - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\", \"dashboard-metrics-scraper\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\", \"dashboard-metrics-scraper\", \"http:dashboard-metrics-scraper\"] verbs: [\"get\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard rules: - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- 从上面可以看出，首先创建了service account账户kubernetes-dashboard，定义了role和clusterrole的权限。最后通过rolebinding及clusterrolebinding将kubernetes-dashboard这个service account账户关联起来。 安装完成后拿到默认的secret中的token去登录dashboard [root@k8s-master dashboard]# kubectl get sa -n kubernetes-dashboard NAME SECRETS AGE default 1 99s kubernetes-dashboard 1 99s [root@k8s-master dashboard]# kubectl describe sa kubernetes-dashboard -n kubernetes-dashboard Name: kubernetes-dashboard Namespace: kubernetes-dashboard Labels: k8s-app=kubernetes-dashboard Annotations: Image pull secrets: \u003cnone\u003e Mountable secrets: kubernetes-dashboard-token-tl6l9 Tokens: kubernetes-dashboard-token-tl6l9 Events: \u003cnone\u003e [root@k8s-master dashboard]# kubectl describe secret kubernetes-dashboard-token-tl6l9 -n kubernetes-dashboard Name: kubernetes-dashboard-token-tl6l9 Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name: kubernetes-dashboard kubernetes.io/service-account.uid: aa1c5a89-263c-47ad-9e96-af159766987a Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 20 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImpHMF94T19xZ2dVMWdCVHVIOFVwa0RmNVotWThEWkYxMzItckIya3NwckEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi10bDZsOSIsImt1YmVybmV0ZXMuaW8vc2VydmljZ","date":"2020-11-23","objectID":"/posts/kubernetes%E4%B9%8Brbac%E9%89%B4%E6%9D%83/:0:3","tags":["Kubernetes"],"title":"Kubernetes之RBAC鉴权","uri":"/posts/kubernetes%E4%B9%8Brbac%E9%89%B4%E6%9D%83/"},{"categories":["Nginx"],"content":"一直以来觉得自己掌握了nginx，直到昨天面试发现自己说的location匹配优先级都不正确。主要原因也是之前没有做过多location的配置，没有去想过这个点，造成面试不理想。故写此博客理解location匹配的优先级。 ","date":"2020-09-27","objectID":"/posts/nginx%E7%9A%84location%E5%8C%B9%E9%85%8D%E4%BC%98%E5%85%88%E7%BA%A7/:0:0","tags":["Nginx"],"title":"Nginx的location匹配优先级","uri":"/posts/nginx%E7%9A%84location%E5%8C%B9%E9%85%8D%E4%BC%98%E5%85%88%E7%BA%A7/"},{"categories":["Nginx"],"content":"前言 location是由ngx_http_core_module模块提供的指令 官网链接：http://nginx.org/en/docs/http/ngx_http_core_module.html#location 语法及优先级： location 含义 优先级 =/uri 精确匹配 1 /uri 无前缀非完整路径匹配 4 ^~ /uri 路径匹配 2 ~ pattern 区分大小写的正则匹配 3 ~* pattern 不区分大小写的正则匹配 3 注:数字越大代表优先级越低 ","date":"2020-09-27","objectID":"/posts/nginx%E7%9A%84location%E5%8C%B9%E9%85%8D%E4%BC%98%E5%85%88%E7%BA%A7/:0:1","tags":["Nginx"],"title":"Nginx的location匹配优先级","uri":"/posts/nginx%E7%9A%84location%E5%8C%B9%E9%85%8D%E4%BC%98%E5%85%88%E7%BA%A7/"},{"categories":["Nginx"],"content":"测试 首先根据6种匹配优先级设定测试文件，并分别配置对照组测试优先级 =/uri 和 /uri 例1： location =/1/2/1.txt { return 111; } location /1/2/1.txt { return 222; } 使用curl请求测试 [root@bochs nginx]# curl -m 1 -s -w %{http_code} -o /dev/null http://www.dmesg.top:82/1/2/1.txt 111 结论：=/uri 的优先级高于 /uri /uri 和 ^~ 例2： location /1/2/1.txt { return 111; } location ^~ /1/2/ { return 222; } 这里如果路径匹配也是/1/2/1.txt,会提示重复。意味着如果都是完整路径，路径匹配与无前缀匹配一致。 curl请求测试 [root@bochs nginx]# curl -m 1 -s -w %{http_code} -o /dev/null http://www.dmesg.top:82/1/2/1.txt 111 例3： location /1/2/ { return 111; } location ^~ /1/2 { return 222; } curl请求测试 [root@bochs nginx]# curl -m 1 -s -w %{http_code} -o /dev/null http://www.dmesg.top:82/1/2/1.txt 222 结论：^~比/uri在非全路径时具有更高的优先级，但为全路径时/uri的优先级等于^~ ^~ 和 ~ 例4： location ~ \\/1\\/2/1\\.txt$ { return 111; } location ^~ /1/2/1.txt { return 222; } curl请求测试 [root@bochs nginx]# curl -m 1 -s -w %{http_code} -o /dev/null http://www.dmesg.top:82/1/2/1.txt 222 例5： location ^~ /1 { return 222; } location ~ \\.txt$ { return 111; } curl请求测试： [root@bochs nginx]# curl -m 1 -s -w %{http_code} -o /dev/null http://www.dmesg.top:82/1/2/1.txt 222 结论：路径匹配^~的优先级高于正则表达式匹配~ /uri 和 ~ 例6： location /1/2 { return 222; } location ~* \\.txt$ { return 111; } curl请求测试 [root@bochs nginx]# curl -m 1 -s -w %{http_code} -o /dev/null http://www.dmesg.top:82/1/2/1.txt 111 例7： location /1/2/1.txt { return 222; } location ~* \\.txt$ { return 111; } curl请求测试 [root@bochs nginx]# curl -m1 -s -w %{http_code} -o /dev/null www.dmesg.top:82/1/2/1.txt 111 结论:无论/uri是否是完整路径，正则匹配的优先级都是高于无前缀匹配/uri的 ","date":"2020-09-27","objectID":"/posts/nginx%E7%9A%84location%E5%8C%B9%E9%85%8D%E4%BC%98%E5%85%88%E7%BA%A7/:0:2","tags":["Nginx"],"title":"Nginx的location匹配优先级","uri":"/posts/nginx%E7%9A%84location%E5%8C%B9%E9%85%8D%E4%BC%98%E5%85%88%E7%BA%A7/"},{"categories":["Nginx"],"content":"总结 从上面的几个实验可以发现优先级关系。 精确匹配= \u003e 路径匹配^~ \u003e 正则匹配~ \u003e 无前缀匹配/uri 特殊的：当无前缀匹配中的uri为全路径时，优先级等于路径匹配。 ","date":"2020-09-27","objectID":"/posts/nginx%E7%9A%84location%E5%8C%B9%E9%85%8D%E4%BC%98%E5%85%88%E7%BA%A7/:0:3","tags":["Nginx"],"title":"Nginx的location匹配优先级","uri":"/posts/nginx%E7%9A%84location%E5%8C%B9%E9%85%8D%E4%BC%98%E5%85%88%E7%BA%A7/"},{"categories":["Nginx"],"content":"本篇博客介绍Nginx的平滑升级，重点是理解Nginx的信号。 ","date":"2020-09-19","objectID":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/:0:0","tags":["Nginx"],"title":"Nginx平滑升级","uri":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/"},{"categories":["Nginx"],"content":"信号及作用 官方链接：http://nginx.org/en/docs/control.html 信号 信号值 作用 INT,TERM 2,15 立即终止Nginx进程 QUIT 3 等进程结束后关闭Nginx HUP 1 根据新的配置文件重建worker进程，即执行nginx -s reload USR1 10 重新写入日志，用于日志切割时使用 USR2 12 根据新的可执行文件生成新的master进程，同时保持旧的nginx进程不变 WINCH 28 关闭worker进程 ","date":"2020-09-19","objectID":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/:0:1","tags":["Nginx"],"title":"Nginx平滑升级","uri":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/"},{"categories":["Nginx"],"content":"设置监控 为了真实模拟现场环境，在服务器上设置监控脚本，当状态码不是200或者超时3秒即判定nginx异常。升级完成后检查result文件是否为空，以此判断升级是否平滑。 [root@bochs ~]# cat nginx_hot_upgrade #!/usr/bin/env bash date=$(date '+%H:%M:%S') while true;do status=$(curl -m 3 -s -o /dev/null -w \"%{http_code}\" https://www.dmesg.top:456) if [[ \"${status}\" -ne 200 ]];then echo \"${date}\" \u003e\u003e result echo \"WRONG!\" \u003e\u003e result fi sleep 1 done [root@bochs ~]# bash nginx_hot_upgrade \u0026 ","date":"2020-09-19","objectID":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/:0:2","tags":["Nginx"],"title":"Nginx平滑升级","uri":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/"},{"categories":["Nginx"],"content":"升级！ 本次升级nginx从1.18.0升级至1.19.2。 查看现有的Nginx版本 [root@bochs ~]# nginx -V nginx version: nginx/1.18.0 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) built with OpenSSL 1.1.1g 21 Apr 2020 TLS SNI support enabled configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-openssl=/mnt/nginxinstall/openssl-1.1.1g --with-zlib=/mnt/nginxinstall/zlib-1.2.11 --with-pcre=/mnt/nginxinstall/pcre-8.44 编译新版本Nginx #下载1.19.2版本nginx [root@bochs ~]# curl -o /tmp/nginx-1.19.2.tar.gz http://nginx.org/download/nginx-1.19.2.tar.gz #解压 [root@bochs ~]# cd /tmp \u0026\u0026 tar -xzvf nginx-1.19.2.tar.gz #编译 [root@bochs ~]# cd nginx-1.19.2 \u0026\u0026 ./configure [取旧版本的编译参数，即上面的configure arguments] \u0026\u0026 make 注：由于还是指定了旧的nginx编译选项，此步骤不能执行make install，否则会导致配置文件被覆盖。 载入新的Nginx可执行文件 #备份原有的可执行文件nginx [root@bochs nginx-1.19.2]# mv /usr/sbin/nginx /usr/sbin/nginx-1.18.0 #拷贝新的可执行文件 [root@bochs nginx-1.19.2]# cp objs/nginx /usr/sbin/nginx [root@bochs nginx-1.19.2]# ls /usr/sbin/nginx* /usr/sbin/nginx /usr/sbin/nginx-1.18.0 开始升级 #查看现有的nginx进程 [root@bochs nginx-1.19.2]# ps -ef | grep -v grep |grep nginx root 22425 1 0 14:26 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 22429 22425 0 14:26 ? 00:00:00 nginx: worker process 这里需要确保nginx的启动方式是以绝对路径方式启动，否则会出现execve() failed while executing new binary process “nginx” (2: No such file or directory)的报错。 #向现有的nginx主进程发送USR2信号 [root@bochs nginx-1.19.2]# kill -USR2 22425 [root@bochs nginx-1.19.2]# ps -ef | grep -v grep |grep nginx root 22425 1 0 14:26 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 22429 22425 0 14:26 ? 00:00:00 nginx: worker process root 22503 22425 0 14:27 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 22508 22503 0 14:27 ? 00:00:00 nginx: worker process #此时可以发现nginx生成了新的master进程，并且新的master进程是旧master进程的子进程 从这里可以看出，USR2信号的作用是保持旧nginx进程不变的情况下生成新的nginx活动进程。此时新的Nginx虽然启动但是并没有实际的响应请求。 #向现有的nginx主进程发送WINCH信号 [root@bochs nginx-1.19.2]# kill -WINCH 22425 [root@bochs nginx-1.19.2]# ps -ef | grep -v grep |grep nginx root 22425 1 0 14:26 ? 00:00:00 nginx: master process /usr/sbin/nginx root 22503 22425 0 14:27 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 22508 22503 0 14:27 ? 00:00:02 nginx: worker process #winch信号关闭了旧master的worker进程，目前新的请求已经是由新的nginx在处理了 从这里可以看出，WINCH信号的作用是关闭由旧的master进程生成的worker进程。实际响应开始由新的master进程生成的worker进程所处理。 结束 #到此nginx升级就已经结束了，唯一要做的是确认访问正常。然后停止旧nginx的master进程 [root@bochs nginx-1.19.2]# kill -INT 22425 [root@bochs nginx-1.19.2]# ps -ef |grep nginx root 22503 1 0 14:27 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 22508 22503 0 14:27 ? 00:00:03 nginx: worker process 由于旧的Nginx其实不再处理请求，所以这里发送的信号TERM、INT及QUIT都是可以的。同时发现，新的nginx的父进程变成了1，也就是旧nginx的父进程。 #检查result文件，无任何输出。平滑升级成功！ [root@bochs ~]# cat result ","date":"2020-09-19","objectID":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/:0:3","tags":["Nginx"],"title":"Nginx平滑升级","uri":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/"},{"categories":["Nginx"],"content":"回滚 当升级完成如果发现与后端应用存在不兼容的情形时应当立即回滚至旧版本Nginx，回滚步骤如下 [root@bochs nginx-1.19.2]# cd /usr/sbin/ #备份新版本的可执行文件 [root@bochs sbin]# mv nginx nginx-1.19.2 #恢复旧版本可执行文件 [root@bochs sbin]# mv nginx-1.18.0 nginx #向nginx的master进程发送USR2信号 [root@bochs sbin]# ps -ef | grep nginx root 22503 1 0 14:27 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 22835 22503 0 15:43 ? 00:00:00 nginx: worker process [root@bochs sbin]# kill -USR2 22503 [root@bochs sbin]# ps -ef | grep nginx root 22503 1 0 14:27 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 22835 22503 0 15:43 ? 00:00:00 nginx: worker process root 23121 22503 0 17:42 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 23126 23121 0 17:42 ? 00:00:00 nginx: worker process #向新nginx的master进程发送WINCH信号和QUIT信号 [root@bochs sbin]# kill -WINCH 22503 [root@bochs sbin]# kill -QUIT 22503 #查看目前Nginx状态 [root@bochs sbin]# ps -ef | grep nginx root 23121 1 0 17:42 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 23126 23121 0 17:42 ? 00:00:00 nginx: worker process #查看result文件无任何输出，回滚完成 [root@bochs ~]# cat result ","date":"2020-09-19","objectID":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/:0:4","tags":["Nginx"],"title":"Nginx平滑升级","uri":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/"},{"categories":["Nginx"],"content":"总结 Nginx的平滑升级主要是根据不同的信号对可执行文件施加不同的效果。总体而言： 先发送USR2信号给master进程达到新旧版本共存的效果。 发送WINCH信号关闭旧版本Nginx的worker进程请求由新版本的worker开始响应。 发送QUIT/TERM/INT关闭旧版本的master进程 ","date":"2020-09-19","objectID":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/:0:5","tags":["Nginx"],"title":"Nginx平滑升级","uri":"/posts/nginx%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/"},{"categories":["GlusterFS"],"content":"GlusterFS是一种分布式文件系统。分布式文件系统可以有效解决数据的存储和管理难题。将固定于某个地点的某个文件系统，扩展到任意多个节点组成一个文件系统网络。 ","date":"2020-09-09","objectID":"/posts/%E5%88%9D%E6%8E%A2glusterfs/:0:0","tags":["GlusterFS"],"title":"初探GlusterFS","uri":"/posts/%E5%88%9D%E6%8E%A2glusterfs/"},{"categories":["GlusterFS"],"content":"GlusterFS中的一些概念 名词 解释 trusted pool 指加入GlsterFs集群的主机组 node 指trusted pool中一台主机 brick 指代被GlusterFS存储任何设备，是GlusterFS的基本单位 export 指给定服务器上程序块的装入路径 Gluster volume 指多块brick的集合，如同NFS的/etc/exports GNFS和kNFS kNFS是指系统自带的NFS，一般而言在GlusterFS中需要禁用。GNFS是指是GlusterFS使用的类NFS，并且无需多余的配置 ","date":"2020-09-09","objectID":"/posts/%E5%88%9D%E6%8E%A2glusterfs/:0:1","tags":["GlusterFS"],"title":"初探GlusterFS","uri":"/posts/%E5%88%9D%E6%8E%A2glusterfs/"},{"categories":["GlusterFS"],"content":"安装GlusterFS 确保防火墙，selinux已关闭，服务器时间正确。 集群信息 服务器ip 主机名 系统版本 硬盘分布 172.19.158.161 node1 CentOS8.2 vdb：20GB vdc:20GB 172.19.158.162 node2 CentOS8.2 vdb：20GB vdc:20GB 172.19.158.163 node3 CentOS8.2 vdb：20GB vdc:20GB 172.19.158.164 client CentOS8.2 客户端 节点信息写入Host 各节点均需要操作 echo \"172.19.158.161 node1\" \u003e\u003e /etc/hosts echo \"172.19.158.162 node2\" \u003e\u003e /etc/hosts echo \"172.19.158.163 node3\" \u003e\u003e /etc/hosts 安装GlusterFS仓库 yum install centos-release-gluster 格式化并挂载brick 这里格式化的磁盘必须要是未分区的磁盘 #我这里租用的阿里云服务器，三个节点的vdb与vdc是没有分区的磁盘 Disk /dev/vdb: 20 GiB, 21474836480 bytes, 41943040 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/vdc: 20 GiB, 21474836480 bytes, 41943040 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes #以下操作在三个节点均需要操作 mkfs.xfs -i size=512 /dev/vdb mkfs.xfs -i size=512 /dev/vdc mkdir -p /data/brick1 mkdir -p /data/brick2 echo \"/dev/vdb /data/brick1 xfs defaults 1 2\" \u003e\u003e /etc/fstab echo \"/dev/vdc /data/brick2 xfs defaults 1 2\" \u003e\u003e /etc/fstab mount -a #如果你的机器系统是CentOS6，那么需要添加XFS文件格式支持 yum install -y xfsprogs 安装GlusterFS yum install -y glusterfs-server #CentOS8安装此步骤会遇到依赖问题：nothing provides shell3-pyxattr needed by glusterfs-server-7.7-1.el8.x86_64 #解决方法：从第三方安装shell3-pyxattr #yum install -y ftp://ftp.pbone.net/mirror/archive.fedoraproject.org/fedora/linux/releases/27/Everything/x86_64/os/Packages/p/shell3-pyxattr-0.5.3-12.fc27.x86_64.rpm systemctl start glusterd \u0026\u0026 systemctl enable glusterd #各节点检查是否正常启动，GlusterFS默认运行于24007端口 #[root@node1 ~]# ss -tnlp 配置trusted pool #在任一节点操作，将各节点加入信任主机池中 #比如在node1节点操作 gluster peer probe node2 peer probe: success. gluster peer probe node3 peer probe: success. #查看信任池状态，任一节点仅能查看到其余节点的信息 [root@node1 ~]# gluster peer status Number of Peers: 2 Hostname: node2 Uuid: 2dbd2fb1-186b-4217-be86-04488771f133 State: Peer in Cluster (Connected) Hostname: node3 Uuid: eff68c51-0462-479e-9f4d-327d6e77dc02 State: Peer in Cluster (Connected) #注意：一旦建立了信任池，外部的主机想要加入到集群中，必须在任一主机添加。新的服务器无法探测到已存在的信任池 设置GlusterFs卷 #所有节点 mkdir /data/brick1/gv0 #在任一节点创建Gluster卷 gluster volume create gv0 replica 3 node1:/data/brick1/gv0 node2:/data/brick1/gv0 node3:/data/brick1/gv0 #启用gv0 gluster volume start gv0 #查看gv0信息 [root@node1 ~]# gluster volume info Volume Name: gv0 Type: Replicate #创建的是复制卷 Volume ID: 39ae23b7-c1f6-49b2-8096-6c4fab4265dc Status: Started #状态是已启动 Snapshot Count: 0 Number of Bricks: 1 x 3 = 3 Transport-type: tcp #传输方式：tcp Bricks: #各节点对应的brick Brick1: node1:/data/brick1/gv0 Brick2: node2:/data/brick1/gv0 Brick3: node3:/data/brick1/gv0 Options Reconfigured: transport.address-family: inet storage.fips-mode-rchecksum: on nfs.disable: on #已禁用kNFS performance.client-io-threads: off 配置客户端并测试 #安装gluster-client yum install -y glusterfs-client #从任一节点挂载gluster volume gv0 mount -t glusterfs node1:/gv0 /mnt #创建测试数据 cd /mnt;for i in `seq 1 150`; do touch `date +%F`-$i; done #查看所有节点的数据，因为默认创建的是复制卷，所以会发现节点数据都是一致的 ls /data/brick1/gv0 ","date":"2020-09-09","objectID":"/posts/%E5%88%9D%E6%8E%A2glusterfs/:0:2","tags":["GlusterFS"],"title":"初探GlusterFS","uri":"/posts/%E5%88%9D%E6%8E%A2glusterfs/"},{"categories":["Docker"],"content":"Docker中的veth pair是一种特殊的网络设备，目的是让容器内能顺利访问外部网路 ","date":"2020-08-30","objectID":"/posts/docker%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84veth-pair/:0:0","tags":["Docker"],"title":"Docker网络中的Veth pair","uri":"/posts/docker%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84veth-pair/"},{"categories":["Docker"],"content":"复习docker三种网络 docker在安装后会默认生成三种网络，none、bridge及host。 [root@k8s-master ~]# docker network ls NETWORK ID NAME DRIVER SCOPE 089c60c71261 bridge bridge local cc6b4177daf6 host host local 4d8e46882a41 none null local none网络 顾名思义，就是挂载这个网络下的容器仅有lo网络，没有其他网卡，也就不能与外界通信，适合对安全性要求较高且不需要联网的容器。 [root@k8s-master ~]# docker run -it --network=none busybox / # ifconfig lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) host网络 连接到host网络的容器会共享宿主机的网络栈,容器内的网络配置与宿主机一致 [root@k8s-master ~]# docker run -it --network=host busybox / # ip link show 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens160: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq qlen 1000 link/ether 00:0c:29:cf:da:70 brd ff:ff:ff:ff:ff:ff ... 直接使用host网络好处就是性能，如果容器对网络有较高要求可以直接使用host网络。但是：使用host网络也会牺牲灵活性，host已经使用的端口容器中将无法使用。 bridge网络 默认网络，运行容器时如果不指定网络类型就会使用bridge网络 当前docker0网卡上没有任何网络设备 [root@k8s-master ~]# bridge link show dev docker0 | grep docker0 [root@k8s-master ~]# 在运行一个busybox容器后再次查看会发现一个新的网络接口vetha8b45fa@if30挂载到了docker0，这个就是busybox的虚拟网卡 [root@k8s-master ~]# docker run -itd --name busybox busybox 99dfbb8efef7be8f7c5e85d7d8f7860c26c269e003c8ba36a93d0c5d2f1f0a89 [root@k8s-master ~]# bridge link show | grep docker0 31: vetha8b45fa@if30: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 master docker0 state forwarding priority 32 cost 2 但是查看busybox的网络配置却没有找到这个vetha8b45fa@if30设备 [root@k8s-master ~]# docker exec -it busybox ip l 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 30: eth0@if31: \u003cBROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u003e mtu 1500 qdisc noqueue link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff 这里的eth0与vetha8b45fa@if30就是一对veth pair设备。可以把veth设备看做是一对网卡，一块网卡(eth0)在容器内，另一块(vetha8b45fa@if30)挂载在docker0上。目的就是把容器的eth0挂到docker0上。 查看容器的网关,这个网关就是docker0的ip地址 [root@k8s-master ~]# docker exec -it busybox ip route default via 172.17.0.1 dev eth0 [root@k8s-master ~]# docker network inspect bridge | jq '.[0].IPAM.Config[0].Gateway' \"172.17.0.1\" ","date":"2020-08-30","objectID":"/posts/docker%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84veth-pair/:0:1","tags":["Docker"],"title":"Docker网络中的Veth pair","uri":"/posts/docker%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84veth-pair/"},{"categories":["Docker"],"content":"Veth详解 如上面的介绍，Veth设备的作用主要用来连接两个网络命名空间，如同一对网卡中间连着一条网线。既然是一对网卡，那么其中一块网卡称作另一块的peer。在Veth设备的一端发送数据会直接发送到另一端。 Veth设备的操作 创建Veth设备对veth0-veth1 [root@k8s-master ~]# ip link add veth0 type veth peer name veth1 创建完成后使用ip link show查看新建出来的veth设备信息 [root@k8s-master ~]# ip link show ... 32: veth1@veth0: \u003cBROADCAST,MULTICAST,M-DOWN\u003e mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 86:d5:ad:c4:43:27 brd ff:ff:ff:ff:ff:ff 33: veth0@veth1: \u003cBROADCAST,MULTICAST,M-DOWN\u003e mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether da:a4:fc:43:09:d9 brd ff:ff:ff:ff:ff:ff 从命名来看就可以看出veth设备的特殊关系了，你中有我，我中有你。目前创建的veth pair都在同一网络空间下，还不算是真正的veth设备。接下来就是创建新的网络命名空间，并且把其中的一块网卡甩到到另一网络空间。 创建新的命名空间ns2 [root@k8s-master ~]# ip netns add ns2 [root@k8s-master ~]# ip netns list ns2 将veth1置入ns2网络空间中再查看本机中的网卡 [root@k8s-master ~]# ip link set veth1 netns ns2 [root@k8s-master ~]# ip link show ... 33: veth0@if32: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether da:a4:fc:43:09:d9 brd ff:ff:ff:ff:ff:ff link-netns ns2 进入ns2名称空间查看网卡 [root@k8s-master ~]# ip netns exec ns2 ip link show 1: lo: \u003cLOOPBACK\u003e mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 32: veth1@if33: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 86:d5:ad:c4:43:27 brd ff:ff:ff:ff:ff:ff link-netnsid 0 现在我们就使用veth pair连接本空间与ns2空间，但目前还不能直接通信，必须为两块网卡配置各自的ip地址并启动 [root@k8s-master ~]# ip netns exec ns2 ip addr add 10.1.1.2/24 dev veth1 [root@k8s-master ~]# ip addr add 10.1.1.1/24 dev veth0 [root@k8s-master ~]# ip netns exec ns2 ip link set dev veth1 up [root@k8s-master ~]# ip link set veth0 up 现在两个网络空间就能直接通信了 [root@k8s-master ~]# ping 10.1.1.2 PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data. 64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.146 ms 64 bytes from 10.1.1.2: icmp_seq=2 ttl=64 time=0.115 ms ^C --- 10.1.1.2 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 54ms rtt min/avg/max/mdev = 0.115/0.130/0.146/0.019 ms [root@k8s-master ~]# ip netns exec ns2 ping 10.1.1.1 PING 10.1.1.1 (10.1.1.1) 56(84) bytes of data. 64 bytes from 10.1.1.1: icmp_seq=1 ttl=64 time=0.101 ms 64 bytes from 10.1.1.1: icmp_seq=2 ttl=64 time=0.131 ms ^C --- 10.1.1.1 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 19ms rtt min/avg/max/mdev = 0.101/0.116/0.131/0.015 ms 总结:docker就是这样使用veth pair设备连接宿主机网络与容器网络。理解veth设备的工作原理对于理解bridge网络起到至关重要的作用。 ","date":"2020-08-30","objectID":"/posts/docker%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84veth-pair/:0:2","tags":["Docker"],"title":"Docker网络中的Veth pair","uri":"/posts/docker%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84veth-pair/"},{"categories":["Kubernetes"],"content":"一般而言pod的调度都是通过RC、Deployment等控制器自动完成，但是仍可以通过手动配置的方式进行调度，目的就是让pod的调度符合我们的预期。 ","date":"2020-08-27","objectID":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/:0:0","tags":["Kubernetes"],"title":"pod调度策略","uri":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/"},{"categories":["Kubernetes"],"content":"定向调度:nodeSelector 定向调度是把pod调度到具有特定标签的node节点的一种调度方式，比如把MySQL数据库调度到具有SSD的node节点以优化数据库性能。此时需要首先给指定的node打上标签，并在pod中设置nodeSelector属性以完成pod的指定调度。 给指定的node打上标签 [root@k8s-master deployment]# kubectl label nodes \u003cnode-name\u003e \u003ckey\u003e:\u003cvalue\u003e #比如给k8s-master节点打上disk=ssd的属性 [root@k8s-master deployment]# kubectl label nodes k8s-master disk=ssd #查看k8s-master节点的所有标签 [root@k8s-master deployment]# kubectl label node k8s-master --list=true beta.kubernetes.io/os=linux disk=ssd kubernetes.io/arch=amd64 kubernetes.io/hostname=k8s-master kubernetes.io/os=linux node-role.kubernetes.io/master= beta.kubernetes.io/arch=amd64 pod默认不会调度到master节点，如果需要将其调度到master上，需要为master节点取消污点: kubectl taint node k8s-master node-role.kubernetes.io/master- 定义测试文件 #nginx-deployment.yml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - image: nginx:1.18.0 name: nginx ports: - containerPort: 80 使用该配置创建pod，pod会默认调度到k8s-node1节点 [root@k8s-master deployment]# kubectl apply -f nginx-deployment.yml deployment.apps/nginx-deployment deployed [root@k8s-master deployment]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-75ddd4d4b4-89rbn 1/1 Running 0 17m 10.244.1.157 k8s-node1 \u003cnone\u003e \u003cnone\u003e nginx-deployment-75ddd4d4b4-jrkmj 1/1 Running 0 17m 10.244.1.159 k8s-node1 \u003cnone\u003e \u003cnone\u003e nginx-deployment-75ddd4d4b4-pffxc 1/1 Running 0 17m 10.244.1.158 k8s-node1 \u003cnone\u003e \u003cnone\u003e 在配置文件中添加disk:ssd的nodeSelector属性 ... spec: containers: ... nodeSelector: disk: ssd 重新应用配置文件，就会发现pod全部被调度到具有disk:ssd属性的master节点了 [root@k8s-master deployment]# kubectl apply -f nginx-deployment.yml deployment.apps/nginx-deployment configured [root@k8s-master deployment]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-7c4d94f56b-8v8st 1/1 Running 0 91s 10.244.0.35 k8s-master \u003cnone\u003e \u003cnone\u003e nginx-deployment-7c4d94f56b-9jhcd 1/1 Running 0 87s 10.244.0.37 k8s-master \u003cnone\u003e \u003cnone\u003e nginx-deployment-7c4d94f56b-blstp 1/1 Running 0 89s 10.244.0.36 k8s-master \u003cnone\u003e \u003cnone\u003e 定向调度可以把pod调度到特定的node节点，但随之而来的缺点就是如果集群中不存在响应的node，即使有基本满足条件的node节点，pod也不会被调度 ","date":"2020-08-27","objectID":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/:0:1","tags":["Kubernetes"],"title":"pod调度策略","uri":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/"},{"categories":["Kubernetes"],"content":"Node亲和性调度:nodeAffinity NodeAffinity是作为NodeSelector的全新调度策略，相比于NodeSelector而言更具表达力。目前有两种亲和性调度策略： requiredDuringSchedulingIgnoredDuringExecution:相当于nodeSelector定向调度，硬限制 preferredDuringSchedulingIgnoredDuringExecution:软限制尝试调度pod到node上。还可以设置多个软限制并定义权重以实现执行的先后顺序 IgnoredDuringExecution为如果pod在运行期间node的属性发生了变更，则系统会忽略变更，该pod可以继续在该节点运行。 该配置要求只能运行在k8s-master节点上，并且节点尽可能是sshd(不是sshd也可以运行) #nodeAffinity apiVersion: v1 kind: Pod metadata: name: node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-master preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: disk operator: In values: - ssd containers: - name: nginx image: nginx:1.18.0 operator支持的操作包括：In,NotIn,Exists,DoesNotExist,Gt,Lt。 NodeAffinity规则注意事项如下： 如果同时定义了NodeSelector和NodeAffinity，则必须两者同时满足pod才会调度到该节点 如果定义了多个nodeSelectorTerms，则其中一个满足匹配即可成功调度 如果定义了多个matchExpressions,则必须所有的条件都满足才能调度到该节点 比如flannel网络插件的定义中就指明了必须是amd64架构的linux操作系统才可运行此pod ... spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux - key: kubernetes.io/arch operator: In values: - amd64 ... ","date":"2020-08-27","objectID":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/:0:2","tags":["Kubernetes"],"title":"pod调度策略","uri":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/"},{"categories":["Kubernetes"],"content":"Pod亲和与排斥调度：podAffinity与podAntiAffinity 相比较NodeAffinity，podAffinity是实现pod与pod间亲和或者互斥调度的策略，亲和性调度可以将pod调度到已经运行具有某种特性的node节点的pod同节点上，而互斥性调度则反之。这里说的某种特性的node节点可以是集群中的节点名称、区域等概念，在定义文件中用topology进行表示。 kubernetes.io/hostname (节点名称) failure-domain.beta.kubernetes.io/zone （区域） failure-domain.beta.kubernetes.io/region （区域） 与NodeAffinity相似，podAffinity也用requiredDuringSchedulingIgnoredDuringExecution及preferredDuringSchedulingIgnoredDuringExecution进行亲和性配置，亲和性配置位于Pod.Spec.affinity的PodAffinity子字段下，互斥性配置与podAffinity同级的podAntiAffinity中定义。 定义参照pod [root@k8s-master podAffinity]# cat flag.yaml apiVersion: v1 kind: Pod metadata: name: flag labels: foo: \"bar\" security: \"high\" spec: containers: - image: nginx:1.18.0 name: nginx ports: - containerPort: 80 nodeSelector: disk: sshd 根据nodeSelector定义的标签，该pod会被调度到k8s-master节点上，同时该pod具有foo=bar和security=high两个属性。 [root@k8s-master podAffinity]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES flag 1/1 Running 0 89s 10.244.1.183 k8s-master \u003cnone\u003e \u003cnone\u003e 亲和性调度 [root@k8s-master podAffinity]# cat affinity.yaml apiVersion: v1 kind: Pod metadata: name: affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: foo operator: In values: - bar topologyKey: kubernetes.io/hostname containers: - name: affinity image: busybox:latest command: [\"/bin/sh\",\"-c\",\"tail -f /dev/null\"] 创建该pod之后会发现这两个pod在同一节点运行 [root@k8s-master ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES affinity 1/1 Running 0 22m 10.244.1.186 k8s-node1 \u003cnone\u003e \u003cnone\u003e flag 1/1 Running 0 34m 10.244.1.183 k8s-node1 \u003cnone\u003e \u003cnone\u003e topologyKey: kubernetes.io/hostname 在这里是做为一种参照，两个pod的运行节点的hostname必须相同，后来的pod才能被调度，如果删去，那么新的pod将始终无法被调度而一直处于pending状态 互斥性调度 互斥性调度是保证新pod不会调度到具有运行某标签pod的node节点的调度策略，即保证两个pod不会在同一node节点运行。参照前面的flagpod的两个标签foo=bar及security=high设置互斥策略，关键字podAntiAffinity 测试用例仍然采用亲和性调度的列子，只把podAffinity改为podAntiAffinity。运行该pod发现该pod被调度到与flagpod不同的节点上。 [root@k8s-master podAffinity]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES affinity 1/1 Running 0 42m 10.244.1.186 k8s-node1 \u003cnone\u003e \u003cnone\u003e antiaffinity 1/1 Running 0 3m34s 10.244.0.53 k8s-master \u003cnone\u003e \u003cnone\u003e flag 1/1 Running 0 54m 10.244.1.183 k8s-node1 \u003cnone\u003e \u003cnone\u003e 应用：调度三个redis副本到三个不同的节点 apiVersion: apps/v1 kind: Deployment metadata: name: redis-cache spec: selector: matchLabels: app: store replicas: 3 template: metadata: labels: app: store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - store topologyKey: \"kubernetes.io/hostname\" containers: - name: redis-server image: redis:3.2-alpine ","date":"2020-08-27","objectID":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/:0:3","tags":["Kubernetes"],"title":"pod调度策略","uri":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/"},{"categories":["Kubernetes"],"content":"容忍和污点：Taints及Tolerations 想比于nodeAffinity期望把某个pod调度到具有某属性node的趋势，容忍和污点是node主动拒绝pod调度到其上的措施。污点是对于node设定的，容忍是对于pod定义的。节点在设置了污点（Taints）后除非pod明确声明了相对应的容忍（Tolerations），那么pod不会被调度到该node上。 默认情况下：master节点不会调度任何pod，因为其中定义了NoSchedule的效果 [root@k8s-master k8s]# kubectl describe node k8s-master | grep Taints Taints: node-role.kubernetes.io/master:NoSchedule 取消的方法： [root@k8s-master k8s]# kubectl taint node k8s-master node-role.kubernetes.io/master- node/k8s-master untainted 取消后k8s就会允许pod调度该node节点上了，当然也可以在pod中设定相应的容忍。 使用kubectl taint子命令为node设置污点 kubectl taint node [node] key=value:[effect] 其中effect可以有三种取值： NoSchedule:不允许调度到该节点上 PreferNoschedule：尽量避免调度pod到该节点上 NoExecute：不会调度到该节点，并且该节点上没有设置这个容忍的pod会被驱逐 如： kubectl taint node k8s-master foo=bar:NoSchedule 相应的容忍可以用两种设定方式： tolerations: - key: \"foo\" operator: \"Equal\" value: \"bar\" effect: \"NoSchedule\" 或者： tolerations: - key: \"foo\" operator: \"Exists\" effect: \"NoSchedule\" operator的值是Exists时不需要执行value operator的值是Equal时pod的value必须和node的value相一致 两个特例： 空的key配合Exists可以匹配所有的键值 空的effect匹配所有的effect k8s允许在一个node上设置多个Taint，也可以在pod上设置多个Toleration。k8s的处理顺序是先列出所有Taint，然后忽略pod中相应的Tolerations，最后没有被匹配到的就是Taint对pod的效果了。 剩余Taint中存在effect=NoSchedule，那么调度器不会把pod调度到该node上 剩余Taint中存在effect=PreferNoScheduler，则调度器尝试不会调度pod到该node上 剩余Taint上存在有NoExecute，并且Pod已经在这个节点运行，那么该pod会被立即驱逐；没有运行，那么该pod不会再被调度到该节点上 例如：对k8s-node1设置两个Taint [root@k8s-master k8s]# kubectl taint node k8s-node1 a=b:NoSchedule [root@k8s-master k8s]# kubectl taint node k8s-node1 c=d:NoExecute 在pod中设置了一个容忍： tolerations: - key: \"a\" operator: \"Equal\" value: \"b\" effect: \"NoSchedule\" 这样的匹配结果是该pod无法调度到node-1上，因为第二个Taint没有对应的Tolerations。并且由于第二个Taint的效果是NoExecute，那么即使在设置Taint前该pod已在该node运行也会被驱逐。 如果Node上有NoExcute的effect，那么节点上所有没有相应容忍的pod都会被驱逐。而有相应容忍的可以一直在该node上运行。此外系统可以在NoExcute添加可选字段tolerationSeconds字段，用于定义Pod在节点添加相应NoExcute的effect之后还能在Node上运行多久，如 tolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoExecute\" tolerationSeconds: 3600 拥有该Tolerations的pod会在运行该pod的Node加上Noexcute后继续运行3600s,随后被驱逐。但是没有定义tolerationSeconds则永远不会被驱逐。 ","date":"2020-08-27","objectID":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/:0:4","tags":["Kubernetes"],"title":"pod调度策略","uri":"/posts/pod%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5/"},{"categories":["Kubernetes"],"content":"本篇博客使用kubernetes搭建wordpress，旨在理解kubernetes各组件以及协作关系。 ","date":"2020-08-23","objectID":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/:0:0","tags":["Kubernetes"],"title":"使用Kubernetes搭建wordpress","uri":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/"},{"categories":["Kubernetes"],"content":"创建数据库 [root@k8s-master wordpress]# cat wordpress-database.yaml apiVersion: v1 kind: Service metadata: name: wpdb labels: app: wpdb spec: type: ClusterIP selector: app: wpdb ports: - protocol: TCP port: 3306 targetPort: 3306 --- apiVersion: apps/v1 kind: Deployment metadata: name: wpdb labels: app: wpdb spec: replicas: 1 selector: matchLabels: app: wpdb template: metadata: labels: app: wpdb spec: containers: - image: mysql:5.7.31 imagePullPolicy: IfNotPresent name: wpdb env: - name: MYSQL_DATABASE value: wpdb - name: MYSQL_USER value: wpuser - name: MYSQL_PASSWORD value: poipoi@098 使用Deployment控制器创建pod资源，使用mysql:5.7.31镜像。并且传入了MYSQL_DATABASE、MYSQL_USER及MYSQL_PASSWORD三个变量。创建Service对象，将容器内的3306映射到ClusterIP的3306端口以供wordpress主程序访问。 ","date":"2020-08-23","objectID":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/:0:1","tags":["Kubernetes"],"title":"使用Kubernetes搭建wordpress","uri":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/"},{"categories":["Kubernetes"],"content":"创建wordpress [root@k8s-master wordpress]# cat wordpress.yaml apiVersion: v1 kind: Service metadata: labels: app: wordpress name: wordpress spec: selector: app: wordpress type: ClusterIP ports: - protocol: TCP port: 80 targetPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress labels: app: wordpress spec: replicas: 1 selector: matchLabels: app: wordpress template: metadata: labels: app: wordpress spec: containers: - image: wordpress:5-php7.2 name: wordpress env: - name: WORDPRESS_DB_NAME value: wpdb - name: WORDPRESS_DB_USER value: wpuser - name: WORDPRESS_DB_PASSWORD value: poipoi@098 - name: WORDPRESS_DB_HOST value: wpdb.default.svc.cluster.local 依然采用Deployment控制器创建pod资源类型，使用wordpress:5-php7.2作为基础镜像。将连接数据库的变量传入。需要注意的是wpdb.default.svc.cluster.local为长格式域名，由于创建wordpress及数据库时未指明namespace，所以两个pod均在默认的namespace下创建，所以这里的域名可以直接用wpdb短格式域名。 上述deployment资源及service资源创建查看是否正常。 [root@k8s-master wordpress]# kubectl get pods NAME READY STATUS RESTARTS AGE wordpress-cd68468cb-x8sx6 1/1 Running 0 116m wpdb-9c65c8bdc-chm6s 1/1 Running 0 127m [root@k8s-master wordpress]# kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wordpress ClusterIP 10.110.1.27 \u003cnone\u003e 80/TCP 126m wpdb ClusterIP 10.102.111.102 \u003cnone\u003e 3306/TCP 127m 这部分结束，那么wordpress就已经创建完成了，可以在服务器上直接curl+ClusterIP:[port]进行访问了。 [root@k8s-master wordpress]# curl 10.110.1.27:80 \u003c!DOCTYPE html\u003e \u003chtml class=\"no-js\" lang=\"zh-CN\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" \u003e \u003clink rel=\"profile\" href=\"https://gmpg.org/xfn/11\"\u003e \u003ctitle\u003e雷探长博客 \u0026#8211; Just another WordPress site\u003c/title\u003e \u003clink rel='dns-prefetch' href='//www.test.com' /\u003e \u003clink rel='dns-prefetch' href='//s.w.org' /\u003e ","date":"2020-08-23","objectID":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/:0:2","tags":["Kubernetes"],"title":"使用Kubernetes搭建wordpress","uri":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/"},{"categories":["Kubernetes"],"content":"创建Ingress Controller及默认的backend服务 上面创建的wordpress还只能在服务器内部访问，要想让外部用户访问必须创建ingress实现HTTP7层路由。使用Ingress创建负载分发时，ingress controller会基于ingress规则将客户端的请求直接转发到Service，跳过了kube-proxy组件的转发功能。 在定义ingress策略前，需要首先需要创建ingress controller及默认的backend服务。Ingress Controller为后端Service都提供了一个统一的入口。同时为了顺利启动Ingress Controller还需要配置默认的backend，用于客户端请求不存在的地址时，返回404应答。 创建Ingress Controller [root@k8s-master wordpress]# cat ingress-daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: nginx-ingress-lb labels: name: nginx-ingress-lb namespace: kube-system spec: selector: matchLabels: name: nginx-ingress-lb template: metadata: labels: name: nginx-ingress-lb spec: containers: - image: registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.9.0-beta.2 name: nginx-ingress-lb readinessProbe: httpGet: path: /healthz port: 10254 scheme: HTTP livenessProbe: httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 timeoutSeconds: 1 ports: - containerPort: 80 hostPort: 80 - containerPort: 443 hostPort: 443 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace args: - /nginx-ingress-controller - --default-backend-service=$(POD_NAMESPACE)/default-http-backend 这里为Nginx容器设置了hostPort，将容器的80和443分别映射到宿主机的80和443端口，这样客户端就可以通过访问http://物理机:80或https://物理机:443来访问该Ingress Controller。 创建backend服务 [root@k8s-master wordpress]# cat default-http-backend.yaml apiVersion: v1 kind: Service metadata: name: default-http-backend namespace: kube-system labels: k8s-app: default-http-backend spec: ports: - port: 80 targetPort: 8080 selector: k8s-app: default-http-backend disableToC: false disableAutoCollapse: true --- apiVersion: apps/v1 kind: Deployment metadata: name: default-http-backend labels: k8s-app: default-http-backend namespace: kube-system spec: selector: matchLabels: k8s-app: default-http-backend replicas: 1 template: metadata: labels: k8s-app: default-http-backend spec: containers: - name: default-http-backend image: registry.aliyuncs.com/google_containers/defaultbackend:1.0 livenessProbe: httpGet: path: /healthz port: 8080 scheme: HTTP initialDelaySeconds: 30 timeoutSeconds: 5 ports: - containerPort: 8080 resources: limits: cpu: 10m memory: 20Mi requests: cpu: 10m memory: 20Mi 使用kubectl apply命令创建上述资源并查看是否正确运行 [root@k8s-master wordpress]# kubectl apply -f default-http-backend.yaml deployment.apps/default-http-backend created service/default-http-backend created [root@k8s-master wordpress]# kubectl apply -f ingress-daemonset.yaml daemonset.apps/nginx-ingress-lb created 创建上述资源时指定了名称空间，查看容器时需要带上-n kube-system参数 [root@k8s-master wordpress]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE ... default-http-backend-797b95869d-4bl4k 1/1 Running 0 164m nginx-ingress-lb-kcfws 1/1 Running 0 163m ... 这里的backend服务采用了Deployment构建，并且声明了数量为1。Ingress Controller采用了DaemonSet构建，每个工作节点运行一个pod，我这里的环境是1+1，所以是1个pod。 部署完Ingress Controller及backend服务后，就可以访问任一工作节点的80端口访问，得到404说明部署成功。 [root@k8s-master wordpress]# curl k8s-node default backend - 404 ","date":"2020-08-23","objectID":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/:0:3","tags":["Kubernetes"],"title":"使用Kubernetes搭建wordpress","uri":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/"},{"categories":["Kubernetes"],"content":"定义Ingress策略 这里使用www.test.com 来设置Ingress策略，定义对**/**的访问请求转发到后端的wordpress的规则。 [root@k8s-master wordpress]# cat ingress-wordpress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: wordpress-ingress spec: rules: - host: www.test.com http: paths: - path: / backend: serviceName: wordpress servicePort: 80 这里的ServiceName和ServicePort是之前创建的wordpress service对象的参数。 需要注意的是这里的80端口和Service对象的80一样是虚拟的，本机并不会监听80端口。 [root@k8s-master wordpress]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ... wordpress ClusterIP 10.110.1.27 \u003cnone\u003e 80/TCP 171m ... 创建ingress策略 [root@k8s-master wordpress]# kubectl apply -f ingress-wordpress.yaml ingress.extensions/wordpress-ingress configured 到这里创建wordpress就结束了，要访问wordpress需要在你的电脑的host文件上做域名ip关联。将www.test.com关联到任一工作节点的ip,使用浏览器访问www.test.com即可。 C:\\Windows\\System32\\drivers\\etc\u003emore hosts 192.168.0.107 www.test.com 安装完成 ","date":"2020-08-23","objectID":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/:0:4","tags":["Kubernetes"],"title":"使用Kubernetes搭建wordpress","uri":"/posts/%E4%BD%BF%E7%94%A8kubernetes%E6%90%AD%E5%BB%BAwordpress/"},{"categories":["MySQL"],"content":"MHA（Master High Availability）目前在 MySQL 高可用方面是一个相对成熟的解决方案，在 MySQL 故障切换过程中，MHA 能做到在0~30秒之内自动完成数据库的故障切换操作，并且在进行故障切换的过程中，MHA 能在最大程度上保证数据的一致性，以达到真正意义上的高可用。 ","date":"2020-07-30","objectID":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/:0:0","tags":["MySQL"],"title":"MySQL高可用架构MHA实战","uri":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/"},{"categories":["MySQL"],"content":"服务器信息 MHA架构要求需至少采用三台服务器进行部署 主机名 服务器IP 数据库角色 操作系统版本 MHA角色 mysql-master 172.19.158.154 主库 CentOS 8.2 管理节点 mysql-node1 172.19.158.155 从库(主库故障时提升为主库) CentOS 8.2 数据节点 mysql-node2 172.19.158.156 从库 CentOS 8.2 数据节点 ","date":"2020-07-30","objectID":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/:0:1","tags":["MySQL"],"title":"MySQL高可用架构MHA实战","uri":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/"},{"categories":["MySQL"],"content":"配置主从同步 配置主从复制,确保两台从库的IO线程以及SQL线程均为YES。 [root@mysql-node1 ~]# ifconfig eth0 eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 172.19.158.155 netmask 255.255.240.0 broadcast 172.19.159.255 inet6 fe80::216:3eff:fe02:2ee5 prefixlen 64 scopeid 0x20\u003clink\u003e ether 00:16:3e:02:2e:e5 txqueuelen 1000 (Ethernet) RX packets 478224 bytes 719808102 (686.4 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 50004 bytes 3882744 (3.7 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@mysql-node1 ~]# mysql -uroot -p'12344' -e'show slave status\\G;' mysql: [Warning] Using a password on the command line interface can be insecure. *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.19.158.154 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 896 Relay_Log_File: relaylog.000003 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes [root@mysql-node2 ~]# ifconfig eth0 eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 172.19.158.156 netmask 255.255.240.0 broadcast 172.19.159.255 inet6 fe80::216:3eff:fe10:39b3 prefixlen 64 scopeid 0x20\u003clink\u003e ether 00:16:3e:10:39:b3 txqueuelen 1000 (Ethernet) RX packets 496558 bytes 747619292 (712.9 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 53124 bytes 4116370 (3.9 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@mysql-node2 ~]# mysql -uroot -p'12344' -e'show slave status\\G;' mysql: [Warning] Using a password on the command line interface can be insecure. *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.19.158.154 Master_User: rep Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 896 Relay_Log_File: relaylog.000003 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes 如果目前的主库发生宕机，那么需要将其中一台从库提升为主库。这里我选择为mysql-node1节点开启binlog日志，在配置文件中添加下面两行即可实现。 log_bin=/opt/sudytech/mysql/data/mysql-node1-bin log-slave-updates=1 ","date":"2020-07-30","objectID":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/:0:2","tags":["MySQL"],"title":"MySQL高可用架构MHA实战","uri":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/"},{"categories":["MySQL"],"content":"配置host及ssh免密登录 各台服务器添加host配置，以mysql-master节点为例。其余两台node节点也需要进行同样的配置 [root@mysql-master ~]# cat \u003e\u003e/etc/hosts\u003c\u003cEOF \u003e 172.19.158.154 mysql-master \u003e 172.19.158.155 mysql-node1 \u003e 172.19.158.156 mysql-node2 \u003e EOF 配置ssh免密登录，以mysql-master节点为例，其余node节点也需要进行同样的操作实现ssh免密登录。 这里一定要保证各服务器均能免密登录其他服务器，否则mha检测免密登录会失败 [root@mysql-master ~]# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:mMhr30tVc4QQLlrzH+wEmk8c5QWHallgdtTv4G3PhQE root@mysql-master The key's randomart image is: +---[RSA 3072]----+ | *===+ | | + +E+. | | + +++... | | . . = B+= oo .| | o + S.= +. * | | . + + .o =| | o . . o oo| | . . o o| | . o. | +----[SHA256]-----+ [root@mysql-master ~]# ssh-copy-id mysql-master 输入密码后完成登录 [root@mysql-master ~]# ssh-copy-id mysql-node1 输入密码后完成登录 [root@mysql-master ~]# ssh-copy-id mysql-node2 输入密码后完成登录 配置完上述后测试登录，确保可以登录成功，比较坑的一点就是自己要给自己进行免密的登录验证 ","date":"2020-07-30","objectID":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/:0:3","tags":["MySQL"],"title":"MySQL高可用架构MHA实战","uri":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/"},{"categories":["MySQL"],"content":"安装MHA MHA（Master High Availability）目前在 MySQL 高可用方面是相对成熟的解决方案，是一套优秀的作为 MySQL 高可用性环境下故障切换和主从提升的高可用软件。 项目位于作者的Github，地址如下： https://github.com/yoshinorim/mha4mysql-manager/releases https://github.com/yoshinorim/mha4mysql-node/releases 找到其中的rpm文件安装到相应的节点即可，注意管理节点两个rpm文件均需要安装。 数据节点安装 安装mha4mysql-node [root@mysql-node1 ~]# yum install -y https://github.com/yoshinorim/mha4mysql-node/releases/download/v0.58/mha4mysql-node-0.58-0.el7.centos.noarch.rpm [root@mysql-node2 ~]# yum install -y https://github.com/yoshinorim/mha4mysql-node/releases/download/v0.58/mha4mysql-node-0.58-0.el7.centos.noarch.rpm 管理节点安装 管理节点所需要的包众多且依赖关系复杂，很多软件在阿里云epel都找不到，我在安装时遇到了到很多问题，好在最终是解决了，下面分享我的安装方法。 安装libvirt [root@mysql-master ~]# yum install -y ftp://ftp.pbone.net/mirror/ftp.centos.org/8.1.1911/PowerTools/x86_64/os/Packages/libvirt-libs-4.5.0-35.3.module_el8.1.0+297+df420408.i686.rpm 安装perl系列包 [root@mysql-master ~]# wget -O /tmp/perl.tar.gz https://siteofhexo.oss-cn-shanghai.aliyuncs.com/perl.tar.gz [root@mysql-master ~]# cd /tmp \u0026\u0026 tar -xzvf perl.tar.gz [root@mysql-master ~]# yum install -y *.rpm 安装mha4mysql-node [root@mysql-master ~]# yum install -y https://github.com/yoshinorim/mha4mysql-node/releases/download/v0.58/mha4mysql-node-0.58-0.el7.centos.noarch.rpm 安装mha4mysql-manager [root@mysql-master ~]# yum install -y https://github.com/yoshinorim/mha4mysql-manager/releases/download/v0.58/mha4mysql-manager-0.58-0.el7.centos.noarch.rpm ","date":"2020-07-30","objectID":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/:0:4","tags":["MySQL"],"title":"MySQL高可用架构MHA实战","uri":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/"},{"categories":["MySQL"],"content":"MHA配置 配置MHA管理账户 所有数据库授权给mha用户所有权限 mysql\u003e grant all privileges on *.* to 'mha'@'172.19.158.%' identified by 'mha12344'; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql\u003e flush privileges; Query OK, 0 rows affected (0.00 sec) mysql\u003e set global read_only=1;（仅数据节点设置，防止误写入数据） 创建MHA配置文件 仅管理节点配置mha管理用的日志，配置文件等 [root@mysql-master ~]# mkdir /mha/{work,conf,log} -pv [root@mysql-master ~]# cat /mha/conf/mha.conf [root@mysql-master ~]# cat /mha/conf/mha.conf [server default] user=mha #授权给mha的用户名称 password=mha12344 #授权给mha的用户密码 ping_interval=2 #检查时间间隔为2s ssh_user=root #远程登录用户 manager_workdir=/mha/work/mha.log manager_log=/mha/log/mha.log master_binlog_dir=/opt/sudytech/mysql/data repl_user=rep #主从同步账号 repl_password=12344 #主从同步密码 [server1] hostname=mysql-master port=3306 candidate_master=1 [server2] hostname=mysql-node1 port=3306 candidate_master=1 [server3] hostname=mysql-node2 port=3306 no_master=1 #该数据库永远不会成为主节点 检测MHA配置 验证MHA的SSH免密登录 [root@mysql-master ~]# masterha_check_ssh --conf=/mha/conf/mha.conf Sat Sep 5 17:10:10 2020 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping. Sat Sep 5 17:10:10 2020 - [info] Reading application default configuration from /mha/conf/mha.conf.. ... Sat Sep 5 17:10:12 2020 - [info] All SSH connection tests passed successfully. Use of uninitialized value in exit at /usr/bin/masterha_check_ssh line 44. 出现All SSH connection tests passed successfully.表示所有主机间的SSH连接处于正常状态 验证主从同步 [root@mysql-master ~]# masterha_check_repl --conf=/mha/conf/mha.conf ... MySQL Replication Health is OK! 出现MySQL Replication Health is OK!即表示主从同步状态正常 两个报错的修复方式: Error happened on checking configurations. Redundant argument in sprintf at /usr/share/perl5/MHA/NodeUtil.pm line 201. 解决方法：修改/usr/share/perl5/vendor_perl/MHA/NodeUtil.pm中的parse_mysql_major_version函数为下面的方式 sub parse_mysql_major_version($) { my $str = shift; $str =~ /(\\d+).(\\d+)/; my $strmajor = “$1.$2”; my $result = sprintf( ‘%03d%03d’, $strmajor =~ m/(\\d+)/g ); return $result; } 参考连接：https://blog.csdn.net/ctypyb2002/article/details/88344274 Can’t exec “mysqlbinlog”: No such file or directory at /usr/share/perl5/vendor_perl/MHA/BinlogManager.pm line 106 解决方法：找到mysql的安装目录将mysql与mysqlbinlog建立软连接到/usr/bin目录下 [root@mysql-master ~]# ln -s /opt/sudytech/mysql/bin/mysqlbinlog /usr/bin/mysqlbinlog [root@mysql-master ~]# ln -s /opt/sudytech/mysql/bin/mysql /usr/bin/mysql 其余各节点执行相同的操作 启动MHA [root@mysql-master ~]# masterha_manager --conf=/mha/conf/mha.conf \u0026 复制终端会话检查启动日志 [root@mysql-master ~]# less /mha/log/mha.log mysql-master(172.19.158.154:3306) (current master) +--mysql-node1(172.19.158.155:3306) +--mysql-node2(172.19.158.156:3306) Sat Sep 5 17:38:54 2020 - [warning] master_ip_failover_script is not defined. Sat Sep 5 17:38:54 2020 - [warning] shutdown_script is not defined. Sat Sep 5 17:38:54 2020 - [info] Set master ping interval 2 seconds. Sat Sep 5 17:38:54 2020 - [warning] secondary_check_script is not defined. It is highly recommended setting it to check master reachability from two or more routes. Sat Sep 5 17:38:54 2020 - [info] Starting ping health check on mysql-master(172.19.158.154:3306).. Sat Sep 5 17:38:54 2020 - [info] Ping(SELECT) succeeded, waiting until MySQL doesn't respond.. 从日志可以看出目前是mysql-master是主库，mysql-node1和mysql-node2是两个从库 ","date":"2020-07-30","objectID":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/:0:5","tags":["MySQL"],"title":"MySQL高可用架构MHA实战","uri":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/"},{"categories":["MySQL"],"content":"测试故障切换 MHA的特点能自动监控当前数据库集群的状态，发生故障时及时的进行转移，MHA部署完成需进行故障切换测试以保证该功能可用。 手动停止主数据库模拟现场环境主库发生宕机等情形。 [root@mysql-master ~]# service mysql stop [root@mysql-master ~]# less /mha/log/mha.log ... ----- Failover Report ----- mha: MySQL Master failover mysql-master(172.19.158.154:3306) to mysql-node1(172.19.158.155:3306) succeeded Master mysql-master(172.19.158.154:3306) is down! Check MHA Manager logs at mysql-master:/mha/log/mha.log for details. Started automated(non-interactive) failover. The latest slave mysql-node1(172.19.158.155:3306) has all relay logs for recovery. Selected mysql-node1(172.19.158.155:3306) as a new master. mysql-node1(172.19.158.155:3306): OK: Applying all logs succeeded. mysql-node2(172.19.158.156:3306): This host has the latest relay log events. Generating relay diff files from the latest slave succeeded. mysql-node2(172.19.158.156:3306): OK: Applying all logs succeeded. Slave started, replicating from mysql-node1(172.19.158.155:3306) mysql-node1(172.19.158.155:3306): Resetting slave info succeeded. Master failover to mysql-node1(172.19.158.155:3306) completed successfully. MHA检测到了主库发生了宕机，并自动开启了故障转移，将node1节点提升为主库(与mha.conf描述一致)。 故障转移完成后masterha_manager会自动停止 测试mysql-node1与mysql-node2主从同步是否正常。 mysql-node1写入数据 mysql\u003e INSERT INTO `employees` VALUES -\u003e (10010,'1963-06-01','Duangkaew','Piveteau','F','1989-08-24'); Query OK, 1 row affected (0.00 sec) 登录mysql-node2查看，主从同步正常 mysql\u003e select * from db_test.employees; +--------+------------+------------+-----------+--------+------------+ | emp_no | birth_date | first_name | last_name | gender | hire_date | +--------+------------+------------+-----------+--------+------------+ | 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 | | 10010 | 1963-06-01 | Duangkaew | Piveteau | F | 1989-08-24 | +--------+------------+------------+-----------+--------+------------+ 2 rows in set (0.00 sec) 当老的主库(mysql-master)完成故障修复后，需要手动将其加入到集群中去。按照方法手动配置主从复制，将老的主库设置为从库并接受主库的同步数据。可以在加入集群后手动将其设置为主库，但是除非必要否则不推荐这么做。 ","date":"2020-07-30","objectID":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/:0:6","tags":["MySQL"],"title":"MySQL高可用架构MHA实战","uri":"/posts/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84mha%E5%AE%9E%E6%88%98/"},{"categories":["Kubernetes"],"content":"Oh! Kubernetes! ","date":"2020-07-29","objectID":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:0:0","tags":["Kubernetes"],"title":"Kubernetes集群部署","uri":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Kubernetes"],"content":"基础配置 角色 IP 系统版本 k8s-master 172.19.158.107 CentOS8.2 k8s-node1 172.19.158.108 CentOS8.2 k8s-node2 172.19.158.109 CentOS8.2 在每台机器的hosts文件中添加所有角色名称及对应的ip 各节点进行时间同步 内核参数调整 cat \u003c\u003c EOF | tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 EOF sysctl -p /etc/sysctl.d/k8s.conf centos8如果上面两个提示未找到，则需装载netfilter模块 modprobe br_netfilter 关闭swap并注释相关启动项 swapoff -a sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab ","date":"2020-07-29","objectID":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:0:1","tags":["Kubernetes"],"title":"Kubernetes集群部署","uri":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Kubernetes"],"content":"安装Docker # 安装必要的一些系统工具 [root@k8s-master ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 # 添加软件源信息 [root@k8s-master ~]# yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 构建镜像缓存 [root@k8s-master ~]# yum makecache # 安装containerd.io [root@k8s-master ~]# yum install -y https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/edge/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm #安装docker [root@k8s-master ~]# yum -y install docker-ce #开启Docker服务并设置为自启动 [root@k8s-master ~]# systemctl start docker ","date":"2020-07-29","objectID":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:0:2","tags":["Kubernetes"],"title":"Kubernetes集群部署","uri":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Kubernetes"],"content":"安装Kubeadm kubectl kubelet #导入阿里云k8s仓库 [root@k8s-master ~]# cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF #禁用selinux [root@k8s-master ~]# setenforce 0 #安装kubelet kubeadm kubectl [root@k8s-master ~]# yum install kubelet-1.18.3 kubeadm-1.18.3 kubectl-1.18.3 -y #启动kubectl并设置为自启动 [root@k8s-master ~]# systemctl enable kubelet \u0026\u0026 systemctl start kubelet ","date":"2020-07-29","objectID":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:0:3","tags":["Kubernetes"],"title":"Kubernetes集群部署","uri":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Kubernetes"],"content":"初始化Kubernetes集群 [root@k8s-master ~]# kubeadm init --apiserver-advertise-address 172.19.158.107 --pod-network-cidr=10.244.0.0/16 --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.18.3 W0729 10:11:06.680899 5631 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [init] Using Kubernetes version: v1.18.3 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/ [WARNING FileExisting-tc]: tc not found in system path [WARNING Hostname]: hostname \"k8s-master\" could not be reached [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' 1、detected “cgroupfs” as the Docker cgroup driver. The recommended driver is “systemd\"解决方法：官方建议使用systemd作为Docker的cgroup的驱动，需要在/etc/docker/daemon.json添加 “exec-opts”: [“native.cgroupdriver=systemd”] 2、–pod-network-cidr必须是10.244.0.0/16,这个值是和flannel中的Network值一致的。 3、–image-repository=registry.aliyuncs.com/google_containers 指明k8s镜像的拉取地址为阿里云k8s镜像地址，不指定会默认从谷歌镜像仓库拉取。由于众所周知的原因造成一直初始化停滞 Your Kubernetes control-plane has initialized successfully! 出现上述内容表示初始化完成 ","date":"2020-07-29","objectID":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:0:4","tags":["Kubernetes"],"title":"Kubernetes集群部署","uri":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Kubernetes"],"content":"配置kubectl mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 这项必须执行，否则会无法使用kubectl命令 ","date":"2020-07-29","objectID":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:0:5","tags":["Kubernetes"],"title":"Kubernetes集群部署","uri":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Kubernetes"],"content":"加入集群 在初始化结束后，最后会提示其他节点加入集群需要的命令 Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.19.158.107:6443 --token hh3dh6.brp9nfb2s0soby9h \\ --discovery-token-ca-cert-hash sha256:42b48189d75ac6e145873d1248b8e3b2b3354db3516e57f1159d9ab49928fba0 其他节点加入主节点需要需首先安装docker及kubernetes的三组件才能执行 ","date":"2020-07-29","objectID":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:0:6","tags":["Kubernetes"],"title":"Kubernetes集群部署","uri":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Kubernetes"],"content":"安装pod网络 要使kubernetes集群正常工作，必须安装pod网络，否则pod之间无法通信。kubernetes支持多种网络组件，flannel，calico等，这里安装flannel网络 在master节点上部署flannel网络插件： [root@aliyun ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml podsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds-amd64 created daemonset.apps/kube-flannel-ds-arm64 created daemonset.apps/kube-flannel-ds-arm created daemonset.apps/kube-flannel-ds-ppc64le created daemonset.apps/kube-flannel-ds-s390x created ","date":"2020-07-29","objectID":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:0:7","tags":["Kubernetes"],"title":"Kubernetes集群部署","uri":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["Kubernetes"],"content":"添加工作节点 按上面的加入集群将工作节点加入到集群中，如果master节点未保存相应命令，可以使用kudeadm token list查看。 [root@k8s-node1 ~]# kubeadm join 172.19.158.107:6443 --token hh3dh6.brp9nfb2s0soby9h 如果结果中出现了表示加入成功 This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 在master节点运行kubectl get nodes查看已添加工作节点 [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master NotReady master 105m v1.18.3 k8s-node1 NotReady \u003cnone\u003e 104m v1.18.3 k8s-node2 NotReady \u003cnone\u003e 104m v1.18.3 此时会发现所有节点都是NotReady状态，原因可能是多种多样的，这是需要借助命令查看详细原因 kubectl get pod --all-namespaces:查看所有名称空间下的pod及运行状态 通过kubectl describe pod \u003cpodname\u003e --namespace=\u003cnamespace\u003e查看分析该pod不能运行的详细原因。 等处理完上面的问题，再次执行kubectl get nodes之后，就会发现所有节点都是Running状态，此时Kubernetes集群才算构建完成。 [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 26m v1.18.3 k8s-node1 Ready \u003cnone\u003e 17m v1.18.3 k8s-node2 Ready \u003cnone\u003e 17m v1.18.3 ","date":"2020-07-29","objectID":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/:0:8","tags":["Kubernetes"],"title":"Kubernetes集群部署","uri":"/posts/kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"单台服务器在企业业务不断发展中必然会遇到访问压力与单点故障，主从复制的架构在一定程度上解决了这个问题。一方面可以实现流量分流，可以让外部请求直接访问主库，内部人员访问从库；另一方面，从库可以当做备份服务器实现主库故障时的手动切换，以满足正常运行的基本要求。 ","date":"2020-07-25","objectID":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/:0:0","tags":["MySQL"],"title":"MySQL主从同步","uri":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"categories":["MySQL"],"content":"主从同步基本原理 主库打开binlog日志记录功能，主从同步就是从库去主库上获取这个binlog日志，并且将binlog日志中记录执行的SQL语句在从库上执行一次 从库上执行start slave命令即可开始主从同步，主从的数据复制就开始了 主从同步开始进行后，从库上的I/O线程就会发送验证请求到主库请求建立连接，并指定位置（这个位置是在从库上执行chage master时指定的）后的binlog日志 主库在收到从库的验证请求后会返回从库请求的内容，这些内容包括binlog日志的内容，主库中新纪录binlog日志的名称以及新的binlog日志中下一个指定位置点等信息 从库收到主库发送来的binlog日志后会将其存在relay log中，并将新的binlog文件名以及位置点信息存储在master.info文件中 从库的SQL线程会实时查看本地relay log线程新增的内容，然后把relay log文件中的内容解析为SQL语句并顺序的在从库一一执行，最后将当前服务器中的中继日志的文件名及位置点信息存储在relay-log.info中以便下次同步需要 ","date":"2020-07-25","objectID":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/:0:1","tags":["MySQL"],"title":"MySQL主从同步","uri":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"categories":["MySQL"],"content":"主从同步部署 1、服务器环境配置： IP 角色 操作系统 106.14.14.122 主数据库 CentOS7.8 101.132.38.235 从数据库 CentOS7.8 本篇博客仅仅是部署测试，现实环境应最大可能使用内网ip。 2、数据库安装： 数据库安装脚本 3、主数据库开始binlog日志 在主数据库的配置文件中添加log_bin,开启记录bin_log日志，文件名为mysql-bin log_bin=/opt/mysql/data/mysql-bin 重启数据库后会发现在/opt/mysql/data目录中会生成两个文件mysql-bin.000001和mysql-bin.index ,mysql-bin.000001是记录binlog日志的文件，而index是存放mysql-bin文件名的文件 [root@mysql-master data]# cat mysql-bin.index /opt/mysql/data/mysql-bin.000001 实际同步中，为了安全考虑，主数据库的用户授权信息是不会同步给从库的，这就必须显式的在配置文件中告诉mysql在不要同步mysql库 。如果binlog-format是ROW模式，那么忽略同步mysql库的语法是replicate-wild-ignore-table=mysql.%;如果是STATEMENT模式，语法为replicate-ignore-db=mysql。 此项配置需写在从库的配置文件中 mysql\u003e show variables like '%binlog_format%'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | binlog_format | ROW | +---------------+-------+ 1 row in set (0.00 sec) ","date":"2020-07-25","objectID":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/:0:2","tags":["MySQL"],"title":"MySQL主从同步","uri":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"categories":["MySQL"],"content":"binlog 三种格式 ","date":"2020-07-25","objectID":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/:1:0","tags":["MySQL"],"title":"MySQL主从同步","uri":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"categories":["MySQL"],"content":"ROW ROW格式会记录每行记录修改的记录，这样可能会产生大量的日志内容,比如一条update语句修改了100条记录，那么这100条记录的修改都会被记录在binlog日志中，这样造成binlog日志量会很大，这种日志格式会占用大量的系统资源，mysql5.7和myslq8.0安装后默认就是这种格式。 ","date":"2020-07-25","objectID":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/:1:1","tags":["MySQL"],"title":"MySQL主从同步","uri":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"categories":["MySQL"],"content":"STATEMENT 记录每一条修改数据的SQL语句（批量修改时，记录的不是单条SQL语句，而是批量修改的SQL语句事件）所以大大减少了binlog日志量，节约磁盘IO，提高性能,看上面的图解可以很好的理解row和statement 两种模式的区别。但是STATEMENT对一些特殊功能的复制效果不是很好，比如：函数、存储过程的复制。由于row是基于每一行的变化来记录的，所以不会出现类似问题 ","date":"2020-07-25","objectID":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/:1:2","tags":["MySQL"],"title":"MySQL主从同步","uri":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"categories":["MySQL"],"content":"MIXED 实际上就是前两种模式的结合。在Mixed模式下，MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。 4、主库配置同步用户并对从库授权 mysql\u003e GRANT REPLICATION SLAVE ON *.* To 'rep'@'101.132.38.235' IDENTIFIED BY '123456'; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql\u003e FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) 查看主库状态 mysql\u003e show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000002 | 896 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 5、配置从库 mysql\u003e CHANGE MASTER TO -\u003e MASTER_HOST='106.14.14.122', -\u003e MASTER_PORT=3306, -\u003e MASTER_USER='rep', -\u003e MASTER_PASSWORD='123456', -\u003e MASTER_LOG_FILE='mysql-bin.000002', -\u003e MASTER_LOG_POS=896; Query OK, 0 rows affected, 2 warnings (0.01 sec) mysql\u003e start slave; Query OK, 0 rows affected (0.00 sec) 查看从库同步状态 mysql\u003e show slave status\\G; *************************** 1. row *************************** *************************** 略*************************** Slave_IO_Running: No Slave_SQL_Running: Yes *************************** 略*************************** 1 row in set (0.00 sec) 发现Slave_IO_Running: No报错，查看日志发现 2020-07-25T14:18:43.822184Z 6 [ERROR] Slave I/O for channel '': Fatal error: The slave I/O thread stops because master and slave have equal MySQL server ids; these ids must be different for replication to work (or the --replicate-same-server-id option must be used on slave but this does not always make sense; please check the manual before using it). Error_code: 1593 关键信息是说主库与从库的server id一样，因为是脚本批量安装，造成了所有服务器server-id=1，手动将从数据库的server-id改为2并重启，再次检查从库状态，发现两个均为YES。至此，主从同步部署完成。 ","date":"2020-07-25","objectID":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/:1:3","tags":["MySQL"],"title":"MySQL主从同步","uri":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"categories":["MySQL"],"content":"主从同步测试 主库创建新库新表并写入测试数据 mysql\u003e create database db_test; Query OK, 1 row affected (0.00 sec) mysql\u003e use db_test; Database changed mysql\u003e CREATE TABLE employees ( -\u003e emp_no INT NOT NULL COMMENT '主键', -\u003e birth_date DATE NOT NULL COMMENT '生日', -\u003e first_name VARCHAR(14) NOT NULL COMMENT '用户-姓', -\u003e last_name VARCHAR(16) NOT NULL COMMENT '用户-名', -\u003e gender ENUM ('M','F') NOT NULL COMMENT '性别', -\u003e hire_date DATE NOT NULL COMMENT '入职时间', -\u003e PRIMARY KEY (emp_no) -\u003e ); Query OK, 0 rows affected (0.01 sec) mysql\u003e INSERT INTO `employees` VALUES -\u003e (10001,'1953-09-02','Georgi','Facello','M','1986-06-26'), -\u003e (10002,'1964-06-02','Bezalel','Simmel','F','1985-11-21'), -\u003e (10003,'1959-12-03','Parto','Bamford','M','1986-08-28'), -\u003e (10004,'1954-05-01','Chirstian','Koblick','M','1986-12-01'), -\u003e (10005,'1955-01-21','Kyoichi','Maliniak','M','1989-09-12'), -\u003e (10006,'1953-04-20','Anneke','Preusig','F','1989-06-02'), -\u003e (10007,'1957-05-23','Tzvetan','Zielinski','F','1989-02-10'), -\u003e (10008,'1958-02-19','Saniya','Kalloufi','M','1994-09-15'), -\u003e (10009,'1952-04-19','Sumant','Peac','F','1985-02-18'), -\u003e (10010,'1963-06-01','Duangkaew','Piveteau','F','1989-08-24'); Query OK, 10 rows affected (0.01 sec) Records: 10 Duplicates: 0 Warnings: 0 登录从库查看 mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | db_test | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.00 sec) mysql\u003e use db_test; mysql\u003e show tables; +-------------------+ | Tables_in_db_test | +-------------------+ | employees | +-------------------+ 1 row in set (0.00 sec) mysql\u003e select * from employees; +--------+------------+------------+-----------+--------+------------+ | emp_no | birth_date | first_name | last_name | gender | hire_date | +--------+------------+------------+-----------+--------+------------+ | 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 | | 10002 | 1964-06-02 | Bezalel | Simmel | F | 1985-11-21 | | 10003 | 1959-12-03 | Parto | Bamford | M | 1986-08-28 | | 10004 | 1954-05-01 | Chirstian | Koblick | M | 1986-12-01 | | 10005 | 1955-01-21 | Kyoichi | Maliniak | M | 1989-09-12 | | 10006 | 1953-04-20 | Anneke | Preusig | F | 1989-06-02 | | 10007 | 1957-05-23 | Tzvetan | Zielinski | F | 1989-02-10 | | 10008 | 1958-02-19 | Saniya | Kalloufi | M | 1994-09-15 | | 10009 | 1952-04-19 | Sumant | Peac | F | 1985-02-18 | | 10010 | 1963-06-01 | Duangkaew | Piveteau | F | 1989-08-24 | +--------+------------+------------+-----------+--------+------------+ 10 rows in set (0.00 sec) 数据同步完成 ","date":"2020-07-25","objectID":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/:1:4","tags":["MySQL"],"title":"MySQL主从同步","uri":"/posts/mysql%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5/"},{"categories":["MySQL"],"content":"随着mysql不断演进，旧的版本不断地会发现新的漏洞，为修复漏洞体验新版本的功能，就需要对数据库进行升级操作。 ","date":"2020-07-12","objectID":"/posts/mysql%E5%8D%87%E7%BA%A7/:0:0","tags":["MySQL"],"title":"MySQL升级","uri":"/posts/mysql%E5%8D%87%E7%BA%A7/"},{"categories":["MySQL"],"content":"升级注意点 备份！备份！备份！ 从5.6升级到5.7需首先升级到5.6最新版；不支持跨版本升级，如直接从5.5升级到5.7 系统初始化时会默认创建root@localhost账户，但如果启用了skip_name_resolve选项，事先需要给127.0.0.1单独授权 mysql_install_db命令在5.7.5之后被mysqld --initialize-insecure取代 mysql.user的password字段在5.7.6版本后已经被authentication_string取代，如果选择In-place升级，注意运行mysql_upgrade命令进行字段转换。如果是Logical升级，注意执行mysqldump时必须包含--add-drop-table并且不能添加--flush-privileges选项 5.7.6之前的版本升级到最新初次启动需要禁用授权表--skip-grant-tables ","date":"2020-07-12","objectID":"/posts/mysql%E5%8D%87%E7%BA%A7/:0:1","tags":["MySQL"],"title":"MySQL升级","uri":"/posts/mysql%E5%8D%87%E7%BA%A7/"},{"categories":["MySQL"],"content":"升级方法 In-Place upgrade 一次in-place升级主要包括停止老的数据库，二进制文件更新，启动新数据库，执行数据库升级命令等 完整操作步骤如下： 1、执行mysql慢速关闭命令，此步骤是为了让脏页刷新到磁盘，避免直接关闭造成数据丢失 mysql -u root -p --execute=\"SET GLOBAL innodb_fast_shutdown=0\" 2、关闭旧数据库 mysqladmin -u root -p shutdown 3、如果新的数据库需要和旧数据库在同目录，将旧数据库所在文件夹备份 4、将新的数据库安装包解压 5、如果旧配置文件/etc/my.cnf中定义的数据目录不需要更改，将旧数据库的数据目录下所有文件移动到新的数据库数据目录中 6、运行新的数据库 mysqld_safe --user=mysql \u0026 7、执行mysql_upgrade命令，该命令会检查旧数据与新版本不兼容的地方并自动修正同时升级系统数据库以应用新特性。 mysql_upgrade -uroot -p 8、重新启动数据库使mysql_upgrade的变更生效 mysqladmin -uroot -p shutdown mysqld_safe --user=mysql \u0026 9、升级完成 Logical upgrade 一次逻辑升级，包括从旧数据库导出SQL文件、安装新的数据库、导入SQL文件到新数据库中等 完整操作步骤如下： 1、从旧数据库导出SQL文件，包括系统表。如果用到了存储程序，还需在选项中添加--events和--routines参数，不推荐在导出时gtid_mode处于开启状态。 [root@bochs ~]# mysql -uroot -p -e \"show variables like 'gtid_mode%';\" Enter password: +---------------+-------+ | Variable_name | Value | +---------------+-------+ | gtid_mode | OFF | +---------------+-------+ mysqldump -u root -p --add-drop-table --routines --events --all-databases --force \u003e data-for-upgrade.sql 2、关闭旧数据库 mysqladmin -uroot -p shutdown 3、安装新的数据库实例，可以使用旧的配置文件，与新版本不兼容的地方需要手动更改 4、初始化mysql，手动执行数据目录 mysqld --initialize-insecure 该步骤会生成临时密码，显示在终端或error日志中,注意保存！ 5、启动新数据库 mysqld_safe --user=mysql \u0026 6、重置root密码 shell\u003e mysql -u root -p Enter password: **** \u003c- 输入第4步的临时密码 mysql\u003e ALTER USER USER() IDENTIFIED BY 'your new password'; 7、将旧数据库导出的SQL文件导入新数据库中 mysql -u root -p --force \u003c data-for-upgrade.sql 8、执行mysql_upgrade命令 mysql_upgrade -uroot -p 9、重启新数据库，使mysql_upgrade命令生效 mysqladmin -uroot -p shutdown mysqld_safe --user=mysql \u0026 10、升级完成 ","date":"2020-07-12","objectID":"/posts/mysql%E5%8D%87%E7%BA%A7/:0:2","tags":["MySQL"],"title":"MySQL升级","uri":"/posts/mysql%E5%8D%87%E7%BA%A7/"},{"categories":["MySQL"],"content":"方法比较 In-place升级方式不改变数据文件，升级速度快，但不支持跨操作系统升级 Logical升级方式需要导出导入数据，在数据量较大的情况下速度会十分缓慢；支持跨操作系统升级 ","date":"2020-07-12","objectID":"/posts/mysql%E5%8D%87%E7%BA%A7/:0:3","tags":["MySQL"],"title":"MySQL升级","uri":"/posts/mysql%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"OpenSSH是SSH协议的免费开源实现,经常会曝出安全漏洞，由于CentOS7自带的OpenSSH版本（OpenSSH_7.4p1, OpenSSL 1.0.2k-fips 26 Jan 2017）太低，有必要进行新服务器的OpenSSH版本升级。升级OpenSSH 升级前首先需要升级OpenSSL 。 本升级教程仅针对CentOS7 ","date":"2020-06-26","objectID":"/posts/openssh%E5%8D%87%E7%BA%A7/:0:0","tags":["Linux"],"title":"OpenSSH升级","uri":"/posts/openssh%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"启用Telnet 此步骤是为了防止一旦升级失败导致无法连接到服务器的措施，由于Telnet是明文传输数据，再使用完后务必关闭。 #安装telnet服务 yum install -y telnet-server #启动telnet服务 systemctl status telnet.socket #防火墙开启23端口 firewall-cmd --permanent --add-port=23/tcp firewall-cmd --reload #windows打开cmd窗口,telnet即可登陆服务器 telnet [服务器ip] linux7 因安全原因不支持ROOT账户直接登录，解决方法mv /etc/securetty /etc/securetty_bak ","date":"2020-06-26","objectID":"/posts/openssh%E5%8D%87%E7%BA%A7/:0:1","tags":["Linux"],"title":"OpenSSH升级","uri":"/posts/openssh%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"停止服务并卸载原有的OpenSSH systemctl stop sshd #查看rpm安装的ssh rpm -qa | grep openssh #卸载rpm安装的ssh rpm -e openssh --nodeps \u0026\u0026 rpm -e openssh-clients --nodeps \u0026\u0026 rpm -e openssh-server --nodeps #查看rpm安装的ssh是否卸载 rpm -qa | grep openssh ","date":"2020-06-26","objectID":"/posts/openssh%E5%8D%87%E7%BA%A7/:0:2","tags":["Linux"],"title":"OpenSSH升级","uri":"/posts/openssh%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"预操作 #安装相关依赖 yum install -y pam* zlib* #备份原ssh配置 mv /etc/ssh /etc/ssh_bak ","date":"2020-06-26","objectID":"/posts/openssh%E5%8D%87%E7%BA%A7/:0:3","tags":["Linux"],"title":"OpenSSH升级","uri":"/posts/openssh%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"安装OpenSSL(1.1.1g) mkdir ./sshupdate cd ./sshupdate wget https://www.openssl.org/source/openssl-1.1.1g.tar.gz tar -xzvf openssl-1.1.1g.tar.gz cd openssl-1.1.1g ./config --prefix=/usr/ --openssldir=/usr/ shared make \u0026\u0026 make install #完成后看下openssl版本 openssl version OpenSSL 1.1.1g 21 Apr 2020 ","date":"2020-06-26","objectID":"/posts/openssh%E5%8D%87%E7%BA%A7/:0:4","tags":["Linux"],"title":"OpenSSH升级","uri":"/posts/openssh%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"安装OpenSSH(8.3p1) wget https://cdn.openbsd.org/pub/OpenBSD/OpenSSH/portable/openssh-8.3p1.tar.gz tar -xzvf openssh-8.3p1.tar.gz cd openssh-8.3p1 ./configure --with-zlib --with-ssl-dir --with-pam --bindir=/usr/bin --sbindir=/usr/sbin --sysconfdir=/etc/ssh make \u0026\u0026 make install cp contrib/redhat/sshd.init /etc/init.d/sshd #完成后看下ssh版本 ssh -V OpenSSH_8.3p1, OpenSSL 1.1.1g 21 Apr 2020 ","date":"2020-06-26","objectID":"/posts/openssh%E5%8D%87%E7%BA%A7/:0:5","tags":["Linux"],"title":"OpenSSH升级","uri":"/posts/openssh%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"修改配置文件 vim /etc/ssh/sshd_config 查找#PermitRootLogin prohibit-password 改成 PermitRootLogin yes 并取消注释 同时如果端口非22端口，需要更改为对应端口 #关闭selinux sed -i.bak 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux setenforce 0 ","date":"2020-06-26","objectID":"/posts/openssh%E5%8D%87%E7%BA%A7/:0:6","tags":["Linux"],"title":"OpenSSH升级","uri":"/posts/openssh%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"重启OpenSSH nohup service sshd restart nohup systemctl restart sshd #添加到自启动 chkconfig --add sshd ","date":"2020-06-26","objectID":"/posts/openssh%E5%8D%87%E7%BA%A7/:0:7","tags":["Linux"],"title":"OpenSSH升级","uri":"/posts/openssh%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"测试 重开窗口连接对应服务器，如ssh跳转登录失败的，清空/root/.ssh/下面的文件即可。 ","date":"2020-06-26","objectID":"/posts/openssh%E5%8D%87%E7%BA%A7/:0:8","tags":["Linux"],"title":"OpenSSH升级","uri":"/posts/openssh%E5%8D%87%E7%BA%A7/"},{"categories":["Nginx"],"content":"Web应用防护系统（也称：网站应用级入侵防御系统 。英文：Web Application Firewall，简称： WAF）。利用国际上公认的一种说法：Web应用 防火墙是通过执行一系列针对HTTP/HTTPS的 安全策略来专门为Web应用提供保护的一款产品。 ","date":"2020-06-20","objectID":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/:0:0","tags":["Nginx"],"title":"Nginx添加Lua实现WAF防火墙","uri":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/"},{"categories":["Nginx"],"content":"nginx+lua安装方法 ","date":"2020-06-20","objectID":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/:0:1","tags":["Nginx"],"title":"Nginx添加Lua实现WAF防火墙","uri":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/"},{"categories":["Nginx"],"content":"方法一：安装nginx并整合lua模块 安装LuaJIT LuaJIT的意思是Lua Just-In-Time，是即时的Lua代码解释器。必须去github下载否则运行是会出现报错，项目地址：https://github.com/openresty/luajit2 git clone https://github.com/openresty/luajit2.git cd luajit2 make PREFIX=/usr/local/luajit make install PREFIX=/usr/local/luajit 安装完成后将如下环境变量加入到/etc/profile中,并执行source /etc/profile export LUAJIT_LIB=/usr/local/luajit/lib export LUAJIT_INC=/usr/local/luajit/include/luajit-2.1 安装ngx_devel_kit(NDK) 版本地址:https://github.com/vision5/ngx_devel_kit/tags 下载并解压 cd /mnt wget https://github.com/vision5/ngx_devel_kit/archive/v0.3.1.tar.gz tar -xzvf v0.3.1.tar.gz 安装最新版的lua-nginx-module 版本地址：https://github.com/openresty/lua-nginx-module/tags 下载最新稳定版并解压 cd /mnt wget https://github.com/openresty/lua-nginx-module/archive/v0.10.15.tar.gz tar -xzvf v0.10.16rc5.tar.gz 编译Nginx并加入lua模块 cd /mnt/nginx-1.18.0 ./configure \\ --prefix=/etc/nginx \\ --sbin-path=/usr/sbin/nginx \\ --modules-path=/usr/lib64/nginx/modules \\ --conf-path=/etc/nginx/nginx.conf \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --pid-path=/var/run/nginx.pid \\ --lock-path=/var/run/nginx.lock \\ --user=nginx \\ --group=nginx \\ --with-http_gzip_static_module \\ --with-http_realip_module \\ --with-http_ssl_module \\ --with-openssl=/mnt/openssl-1.1.1g \\ --with-zlib=/mnt/zlib-1.2.11 \\ --with-pcre=/mnt/pcre-8.44 \\ --add-module=/mnt/lua-nginx-module-0.10.15 \\ --add-module=/mnt/ngx_devel_kit-0.3.1 其中的openssl,pcre以及zlib需要额外下载并解压到/mnt目录下 启动nginx发现报错 [root@k8s-node mnt]# nginx nginx: error while loading shared libraries: libluajit-5.1.so.2: cannot open shared object file: No such file or directory 解决办法 echo \"/usr/local/luajit/lib/\" \u003e\u003e /etc/ld.so.conf ldconfig 在默认的location中加入如下指令，访问测试 content_by_lua 'ngx.say(\"hello, lua\")'; 安装成功 ","date":"2020-06-20","objectID":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/:0:2","tags":["Nginx"],"title":"Nginx添加Lua实现WAF防火墙","uri":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/"},{"categories":["Nginx"],"content":"方法二 ：直接安装OpenResty OpenResty是一个基于Nginx与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。最新版Openresty cd /opt tar -xzvf openresty-1.15.8.3.tar.gz ./configure \\ --prefix=/opt/openresty \\ --with-pcre=/opt/pcre-8.44 \\ --with-zlib=/opt/zlib-1.2.11 \\ --with-openssl=/opt/openssl-1.1.1g \\ --with-poll_module \\ --with-http_v2_module \\ --with-http_realip_module \\ --with-http_addition_module \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_ssl_preread_module\\ --with-http_ssl_module gmake gmake install 按方法一测试可以访问即可 ","date":"2020-06-20","objectID":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/:0:3","tags":["Nginx"],"title":"Nginx添加Lua实现WAF防火墙","uri":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/"},{"categories":["Nginx"],"content":"WAF模块安装 ngx_lua_waf项目地址：https://github.com/loveshell/ngx_lua_waf.git 这里以Openresty为例，Nginx方法类似 cd /opt/openresty/lualib git clone https://github.com/loveshell/ngx_lua_waf.git waf 在openresty的配置文件中添加如下配置项： lua_package_path \"/opt/openresty/lualib/waf/?.lua\"; lua_shared_dict limit 10m; init_by_lua_file /opt/openresty/lualib/waf/init.lua; access_by_lua_file /opt/openresty/lualib/waf/waf.lua; 整个waf目录结构如下： [root@k8s-node lualib]# tree waf waf ├── config.lua ├── init.lua ├── wafconf │ ├── args │ ├── cookie │ ├── post │ ├── url │ ├── user-agent │ └── whiteurl └── waf.lua 1 directory, 9 files config.lua中定义了整个waf的配置，详细如下： #拦截规则的存放目录 RulePath = \"/opt/openresty/lualib/waf/wafconf/\" #是否开启拦截日志记录 attacklog = \"on\" #拦截日志的记录目录，nginx的worker进程需要对该目录有权限 logdir = \"/opt/openresty/nginx/logs/\" #是否开启url拦截 UrlDeny=\"on\" #是否开启拦截重定向 Redirect=\"on\" #是否开启cookie攻击防护 CookieMatch=\"on\" #是否开启post攻击防护 postMatch=\"on\" whiteModule=\"on\" #禁止访问的文件扩展名 black_fileExt={\"php\",\"jsp\"} #IP地址白名单 ipWhitelist={\"127.0.0.1\"} #IP地址黑名单 ipBlocklist={\"1.0.0.1\"} #是否开启CC攻击防护 CCDeny=\"off\" #定义CC攻击速率，该例为每60秒100次请求 CCrate=\"100/60\" #定义重定向后的html页面 html=[[...]] 拦截测试： #出现拦截页面即表示安装成功 curl http://www.example.com/test.php?id=../etc/passwd 相关日志： 192.168.0.101 [2020-06-20 01:44:01] \"GET localhost/index.php?id=/../../../etc/passwd\" \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36\" \"\\.\\./\" ","date":"2020-06-20","objectID":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/:0:4","tags":["Nginx"],"title":"Nginx添加Lua实现WAF防火墙","uri":"/posts/nginx%E6%B7%BB%E5%8A%A0lua%E5%AE%9E%E7%8E%B0waf%E9%98%B2%E7%81%AB%E5%A2%99/"},{"categories":["Docker"],"content":"默认情况下，在mysql容器中创建新库时需要先运行mysql容器，把需要的sql文件通过docker cp的方式拷贝至容器内，再通过mysql的子命令将sql文件导入。过程比较繁琐，如果是公司的项目部署，可以创建带有公司的项目sql的自定义mysql镜像，避免繁琐的流程。 ","date":"2020-05-17","objectID":"/posts/%E5%88%9B%E5%BB%BA%E5%B8%A6%E6%9C%89%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%93%E7%9A%84mysql%E9%95%9C%E5%83%8F/:0:0","tags":["Docker"],"title":"创建带有自定义库的Mysql镜像","uri":"/posts/%E5%88%9B%E5%BB%BA%E5%B8%A6%E6%9C%89%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%93%E7%9A%84mysql%E9%95%9C%E5%83%8F/"},{"categories":["Docker"],"content":"背景分析 首先拉取官方镜像： docker pull mysql:5.7.30 查看镜像的构建历史 [root@bochs docker]# docker history mysql:5.7.30 IMAGE CREATED CREATED BY SIZE COMMENT ... \u003cmissing\u003e 29 hours ago /bin/sh -c #(nop) ENTRYPOINT [\"docker-entrypoint.sh\"] 0B ... 可以发现，默认的entrypoint为docker-entrypoint.sh 进入容器内，可以发现docker-entrypoint.sh其实是个软连接 lrwxrwxrwx 1 root root 34 May 15 20:11 entrypoint.sh -\u003e usr/local/bin/docker-entrypoint.sh 查看此脚本，发现脚本中已经定义了初始化的代码： 1、docker_process_init_file()函数定义了初始文件的格式，其中调用了docker_process_sql来完成新库创建及数据导入。 初始化文件可以是.sh .sql .sql.gz .sql.xz格式中的任一种 docker_process_init_files() { # mysql here for backwards compatibility \"${mysql[@]}\" mysql=( docker_process_sql ) echo local f for f; do case \"$f\" in *.sh) # https://github.com/docker-library/postgres/issues/450#issuecomment-393167936 # https://github.com/docker-library/postgres/pull/452 if [ -x \"$f\" ]; then mysql_note \"$0: running $f\" \"$f\" else mysql_note \"$0: sourcing $f\" . \"$f\" fi ;; *.sql) mysql_note \"$0: running $f\"; docker_process_sql \u003c \"$f\"; echo ;; *.sql.gz) mysql_note \"$0: running $f\"; gunzip -c \"$f\" | docker_process_sql; echo ;; *.sql.xz) mysql_note \"$0: running $f\"; xzcat \"$f\" | docker_process_sql; echo ;; *) mysql_warn \"$0: ignoring $f\" ;; esac echo done } 2、docker_process_sql使用mysql命令导入数据库文件 docker_process_sql() { passfileArgs=() if [ '--dont-use-mysql-root-password' = \"$1\" ]; then passfileArgs+=( \"$1\" ) shift fi if [ -n \"$MYSQL_DATABASE\" ]; then set -- --database=\"$MYSQL_DATABASE\" \"$@\" fi mysql --defaults-extra-file=\u003c( _mysql_passfile \"${passfileArgs[@]}\") --protocol=socket -uroot -hlocalhost --socket=\"${SOCKET}\" \"$@\" } 3、在主函数调用了docker_process_init_file()函数,初始化的文件全部位于/docker-entrypoint-initdb.d/中 365 docker_process_init_files /docker-entrypoint-initdb.d/* 只要把sql文件放入该目录我们就可以通过docker build命令来创建带有自定义库的mysql镜像了。 ","date":"2020-05-17","objectID":"/posts/%E5%88%9B%E5%BB%BA%E5%B8%A6%E6%9C%89%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%93%E7%9A%84mysql%E9%95%9C%E5%83%8F/:0:1","tags":["Docker"],"title":"创建带有自定义库的Mysql镜像","uri":"/posts/%E5%88%9B%E5%BB%BA%E5%B8%A6%E6%9C%89%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%93%E7%9A%84mysql%E9%95%9C%E5%83%8F/"},{"categories":["Docker"],"content":"实现过程 创建新的Dockerfile文件，将所需要的用到的sql文件拷贝到/docker-entrypoint-initdb.d/目录中 FROM mysql:5.7.30 COPY ./mysql/initsql/*.sql /docker-entrypoint-initdb.d/ docker-compose文件如下 version: \"3.3\" services: mysql: build: context: . dockerfile: Dockerfile image: mysql_modified:v1.0 container_name: mysql_modified ports: - target: 3306 published: 3306 protocol: tcp mode: host volumes: - /home/docker/mysql/conf/:/etc/mysql/conf.d/ - /home/docker/mysql/data/:/var/lib/mysql/ environment: - MYSQL_ROOT_PASSWORD=Zxczxc@123 如果是用mysqldump导出的sql文件，必须要加上-B参数保留创库语句。 新建测试用的test.sql文件 CREATE DATABASE IF NOT EXISTS DB_TEST; USE DB_TEST; CREATE TABLE employees ( emp_no INT NOT NULL COMMENT '主键', birth_date DATE NOT NULL COMMENT '生日', first_name VARCHAR(14) NOT NULL COMMENT '用户-姓', last_name VARCHAR(16) NOT NULL COMMENT '用户-名', gender ENUM ('M','F') NOT NULL COMMENT '性别', hire_date DATE NOT NULL COMMENT '入职时间', PRIMARY KEY (emp_no) ); INSERT INTO `employees` VALUES (10001,'1953-09-02','Georgi','Facello','M','1986-06-26'), (10002,'1964-06-02','Bezalel','Simmel','F','1985-11-21'), (10003,'1959-12-03','Parto','Bamford','M','1986-08-28'), (10004,'1954-05-01','Chirstian','Koblick','M','1986-12-01'); 整个构建目录结构如下： [root@bochs /]# tree home home `-- docker |-- docker-compose.yml |-- Dockerfile `-- mysql |-- conf | `-- my.cnf |-- data `-- initsql `-- test.sql 5 directories, 4 files 使用docker-compose build命令构建新的带有默认库mysql镜像 [root@bochs docker]# docker-compose build Building mysql Step 1/2 : FROM mysql:5.7.30 ---\u003e b84d68d0a7db Step 2/2 : COPY ./mysql/initsql/*.sql /docker-entrypoint-initdb.d/ ---\u003e c64103df17a2 Successfully built c64103df17a2 Successfully tagged mysql_modified:v1.0 [root@bochs docker]# docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE mysql_modified v1.0 c64103df17a2 32 seconds ago 448MB mysql 5.7.30 b84d68d0a7db 30 hours ago 448MB 完成镜像创建后使用docker-compose up -d启动该容器并查看数据，新库已成功创建，数据也正常。 [root@bochs docker]# docker-compose up -d [root@bochs docker]# docker exec -it mysql_modified mysql -uroot -p'Zxczxc@123' -e 'show databases;' mysql: [Warning] Using a password on the command line interface can be insecure. +--------------------+ | Database | +--------------------+ | information_schema | | DB_TEST | | mysql | | performance_schema | | sys | +--------------------+ [root@bochs docker]# docker exec -it mysql_modified mysql -uroot -p'Zxczxc@123' -e 'select * from DB_TEST.employees;\\G' mysql: [Warning] Using a password on the command line interface can be insecure. +--------+------------+------------+-----------+--------+------------+ | emp_no | birth_date | first_name | last_name | gender | hire_date | +--------+------------+------------+-----------+--------+------------+ | 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 | | 10002 | 1964-06-02 | Bezalel | Simmel | F | 1985-11-21 | | 10003 | 1959-12-03 | Parto | Bamford | M | 1986-08-28 | | 10004 | 1954-05-01 | Chirstian | Koblick | M | 1986-12-01 | +--------+------------+------------+-----------+--------+------------+ ","date":"2020-05-17","objectID":"/posts/%E5%88%9B%E5%BB%BA%E5%B8%A6%E6%9C%89%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%93%E7%9A%84mysql%E9%95%9C%E5%83%8F/:0:2","tags":["Docker"],"title":"创建带有自定义库的Mysql镜像","uri":"/posts/%E5%88%9B%E5%BB%BA%E5%B8%A6%E6%9C%89%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%93%E7%9A%84mysql%E9%95%9C%E5%83%8F/"},{"categories":["Docker"],"content":"总结 放入/docker-entrypoint-initdb.d/目录中的文件只会在构建镜像时执行一次 可以将多个数据库放入/docker-entrypoint-initdb.d/中达到批量化创建的目的 ","date":"2020-05-17","objectID":"/posts/%E5%88%9B%E5%BB%BA%E5%B8%A6%E6%9C%89%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%93%E7%9A%84mysql%E9%95%9C%E5%83%8F/:0:3","tags":["Docker"],"title":"创建带有自定义库的Mysql镜像","uri":"/posts/%E5%88%9B%E5%BB%BA%E5%B8%A6%E6%9C%89%E8%87%AA%E5%AE%9A%E4%B9%89%E5%BA%93%E7%9A%84mysql%E9%95%9C%E5%83%8F/"},{"categories":["Docker"],"content":"批量化在主机上安装docker可以使用docker-machine Docker Machine可以支持在不同的环境下安装配置docker host 常规的Linux操作系统 2）虚拟化平台VirtualBox、Vmware等 3）公有云Amazon Web Services、Microsoft Azure、Google Compute Engine等 ","date":"2020-05-11","objectID":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/:0:0","tags":["Docker"],"title":"Docker三剑客之Docker-Machine","uri":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/"},{"categories":["Docker"],"content":"实验环境描述 操作系统版本 ip 配置 角色 Ubuntu 18.04.4 LTS 172.16.89.101 4核8G machine Ubuntu 18.04.4 LTS 172.16.89.100 4核8G node1 Ubuntu 18.04.4 LTS 172.16.89.99 4核8G node2 官方项目地址位于 https://github.com/docker/machine/releases 目前最新版本是v0.16.2 安装的方法也很简单，只需要执行： curl -L https://github.com/docker/machine/releases/download/v0.16.2/docker-machine-`uname -s`-`uname -m` \u003e/tmp/docker-machine \u0026\u0026 chmod +x /tmp/docker-machine \u0026\u0026 sudo cp /tmp/docker-machine /usr/local/bin/docker-machine 执行完成执行docker-machine有正常输出即完成安装 root@docker-machine:~# docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS root@docker-machine:~# 由于docker-machine再安装host时需要免密登录到远程主机上，所以在machine节点上创建ssh秘钥，并发送到node1节点和node2节点上 root@docker-machine:~# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:xafvCmF4xRwbNLLI8eOnXZECMYWtqkCBCqPE7Zjj7d0 root@docker-machine The key's randomart image is: +---[RSA 2048]----+ |... . =** | |+o.. . +.O.= . | |=.+. o +.O + | |o+.. o.+ + . | |..o ..S o . | | ... .o = o | | ..... o . . | | ... E . . | | ... | +----[SHA256]-----+ root@docker-machine:~# ssh-copy-id 172.16.89.100 ... root@172.16.89.100's password: ... root@docker-machine:~# ssh-copy-id 172.16.89.99 ... root@172.16.89.99's password: ... ","date":"2020-05-11","objectID":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/:0:1","tags":["Docker"],"title":"Docker三剑客之Docker-Machine","uri":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/"},{"categories":["Docker"],"content":"开始安装 上述操作完成后就可以开始安装了执行docker-machine create子命令在node1上部署docker了 root@docker-machine:~# docker-machine create --driver generic --generic-ip-address=172.16.89.100 node1 Running pre-create checks... Creating machine... (node1) No SSH key specified. Assuming an existing key at the default location. Waiting for machine to be running, this may take a few minutes... Detecting operating system of created instance... Waiting for SSH to be available... Detecting the provisioner... Provisioning with ubuntu(systemd)... Installing Docker... 同样的可以执行docker-machine create --driver generic --generic-ip-address=172.16.89.99 node2在node2上部署docker 在实际执行中发现由于docker-machine默认使用了官方的镜像站下载docker-ce，造成执行过程会一直卡在\"Installing Docker…\"，如果机器数量较少推荐先在node节点上使用阿里云镜像安装dock-ce，在到machine节点上创建就行了。安装步骤如下 # step 1: 安装必要的一些系统工具 sudo apt-get update sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common # step 2: 安装GPG证书 curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # Step 3: 写入软件源信息 sudo add-apt-repository \"deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\" # Step 4: 更新并安装Docker-CE sudo apt-get -y update sudo apt-get -y install docker-ce 在node1和node2节点上完成docker-ce安装后即可以继续回到machine节点操作了，这种方法仅适用于机器较少的情况，如果机器多安装慢，使用Ansible安装后再执行docker-machine create。 root@docker-machine:~# docker-machine ls NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS node1 - generic Running tcp://172.16.89.100:2376 v19.03.8 node2 - generic Running tcp://172.16.89.99:2376 v19.03.8 正常输出即表示docker-machine已经把docker在所有node部署好了 登录到任意的node节点查看docker.service配置 vim /etc/systemd/system/docker.service.d/10-machine.conf [Service] ExecStart= ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --storage-driver overlay2 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=generic Environment= -H tcp://0.0.0.0:2376:使Docker Daemon接收远程连接，非docker-machie安装方式是没有这个选项的 –tls*：对来自远程主机的连接请求启用安全认证和加密措施 更好的体验 为了得到更好的体验，可以安装bash completion script,从而在bash中使用tab补全命令。项目地址位于 https://github.com/docker/machine/tree/master/contrib/completion/bash 将页面上的三个脚本放到machine上/etc/bash_completion.d目录下，同时添加如下代码到$HOME/.bashrc中 source /etc/bash_completion.d/docker-machine-prompt.bash source /etc/bash_completion.d/docker-machine.bash source /etc/bash_completion.d/docker-machine-wrapper.bash PS1='[\\u@\\h \\W$(__docker_machine_ps1 \" [%s]\")]\\$' docker-machine env子命令可以提示操作node节点所需的环境变量 [root@docker-machine ~]# docker-machine env node1 export DOCKER_TLS_VERIFY=\"1\" export DOCKER_HOST=\"tcp://172.16.89.100:2376\" export DOCKER_CERT_PATH=\"/root/.docker/machine/machines/node1\" export DOCKER_MACHINE_NAME=\"node1\" # Run this command to configure your shell: # eval $(docker-machine env node1) 执行eval $(docker-machine env node1)就可以切换到node1上执行指令同时提示符也会变成node1 [root@docker-machine ~]# eval $(docker-machine env node1) [root@docker-machine ~ [node1]]# 在其上执行的所有docker命令搜相当于node1上执行 [root@docker-machine ~ [node1]]# docker pull nginx:alpine alpine: Pulling from library/nginx cbdbe7a5bc2a: Pull complete c554c602ff32: Pull complete Digest: sha256:763e7f0188e378fef0c761854552c70bbd817555dc4de029681a2e972e25e30e Status: Downloaded newer image for nginx:alpine docker.io/library/nginx:alpine 回到node1终端上，发现nginx:alpine镜像已下载，而且machine节点上是没有这个镜像的 root@node1:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE nginx alpine 89ec9da68213 2 weeks ago 19.9MB 其他的docker命令都可以通过eval $(docker-machine env nodeX)上切换到相应的node节点执行docker子命令 ","date":"2020-05-11","objectID":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/:0:2","tags":["Docker"],"title":"Docker三剑客之Docker-Machine","uri":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/"},{"categories":["Docker"],"content":"docker-machine子命令 子命令 效果 active 输出处于active状态的node节点 config 查看machine的damon配置 create 创建machine env 显示node节点的环境变量 ls 显示node节点的状态信息 stop/start/restart 对node所在的操作系统执行开关重启操作 scp 在node节点中复制数据 upgrade 将node节点上的docker升级到最新版本 docker-machine scp node1:/tmp/a.txt node2:/tmp/ ","date":"2020-05-11","objectID":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/:0:3","tags":["Docker"],"title":"Docker三剑客之Docker-Machine","uri":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/"},{"categories":["Docker"],"content":"总结 在多主机环境上使用docker-machine部署docker将大大提高效率。当然由于默认使用的国外的镜像源，安装docker会很慢，介意的话最好先用Ansible批量化安装docker再执行docker-machine将所有机器加入控制主机。 ","date":"2020-05-11","objectID":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/:0:4","tags":["Docker"],"title":"Docker三剑客之Docker-Machine","uri":"/posts/docker%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bdocker-machine/"},{"categories":["Linux"],"content":"公司内网有台8核8G的服务器可以用来各种实验。虽然配置不是太高，但是相对于自己1核2G的学生机来说已经不知道强到哪里了。但由于是内网的缘故只能在公司内访问，虽然通过远程桌面也能使用，但是这样未免太low了。于是想到用ssh来进行端口转发，在家也能访问内网服务器。 ","date":"2020-05-09","objectID":"/posts/ssh%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8/:0:0","tags":["Linux"],"title":"SSH端口转发访问内网服务器","uri":"/posts/ssh%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"categories":["Linux"],"content":"条件 内网服务器必须能连外网 服务器信息如下： 服务器名称 服务器ip ssh端口 服务器配置 公网服务器 122.34.122.32 54374 1核2G 内网服务器 172.18.10.23 22 8核8G ","date":"2020-05-09","objectID":"/posts/ssh%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8/:0:1","tags":["Linux"],"title":"SSH端口转发访问内网服务器","uri":"/posts/ssh%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"categories":["Linux"],"content":"步骤 1.在内网服务器上执行： ssh -N -f -R [122.34.122.32:]10022:172.18.10.23:22 root@122.34.122.32 -p54374 参数解释： -N:不执行远程命令，如果ssh仅仅是用来转发端口很有用。 Do not execute a remote command. This is useful for just forwarding ports. -f:将本条命令放到后台执行，防止内网服务器ssh窗口关闭导致无法连接到内网服务器 -R:设置转发规则，本条规则是把本地22端口的请求转发到122.34.122.32的10022端口，形成一条通路。 -p 54374:指定公网服务器的ssh端口用于登录 2.登录公网服务器，查询端口发现端口已经监听： [root@bochs ~]# ss -tnl | grep :10022 LISTEN 0 128 *:10022 *:* LISTEN 0 128 [::]:10022 [::]:* 3.在122.34.122.32连接10022端口,输入内网服务器的root密码登录： ssh -p10022 122.34.122.32 命令执行完成即已登录到内网服务器 top - 23:14:46 up 22 days, 5:26, 2 users, load average: 0.00, 0.01, 0.05 Tasks: 280 total, 2 running, 278 sleeping, 0 stopped, 0 zombie %Cpu0 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu1 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu2 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu3 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu4 : 0.0 us, 2.6 sy, 0.0 ni, 97.4 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu5 : 0.0 us, 2.7 sy, 0.0 ni, 97.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu6 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st %Cpu7 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 8008440 total, 5914008 free, 730108 used, 1364324 buff/cache KiB Swap: 0 total, 0 free, 0 used. 6954304 avail Mem 真香！ ","date":"2020-05-09","objectID":"/posts/ssh%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8/:0:2","tags":["Linux"],"title":"SSH端口转发访问内网服务器","uri":"/posts/ssh%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"categories":["Linux"],"content":"延伸 如果内网的服务器上有mysql同样可以使用端口转发访问内网的mysql ssh -N -f -R 33060:172.18.10.23:3306 root@122.34.122.32 -p54374 mysql -h127.0.0.1 -P33060 -uroot -p ","date":"2020-05-09","objectID":"/posts/ssh%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8/:0:3","tags":["Linux"],"title":"SSH端口转发访问内网服务器","uri":"/posts/ssh%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"categories":["Nginx"],"content":"Nginx的 limit 模块主要包括:ngx_http_limit_req_module、ngx_http_limit_conn_module、ngx_stream_limit_conn_module 以及ngx_http_core_module中limit_rate选项，由于stream主要用来实现四层协议（网络层和传输层）的转发、代理、负载均衡等，并且ngx_stream_limit_conn_module和ngx_http_limit_conn_module配置基本相同，所以本文不做介绍。 ","date":"2020-05-05","objectID":"/posts/nginx-limit%E6%A8%A1%E5%9D%97/:0:0","tags":["Nginx"],"title":"Nginx-Limit模块","uri":"/posts/nginx-limit%E6%A8%A1%E5%9D%97/"},{"categories":["Nginx"],"content":"ngx_http_limit_req_module 该模块用来限制某个特定键的请求处理的速率，这个特定键一般为某个ip。该模块主要用到了\"漏桶\"算法。 指令 limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s; server { location /search/ { limit_req zone=one burst=5 [nodelay|delay=num]; } 首先使用limit_req_zone定义一个内存区域(定义在http段中)： $binary_remote_addr:特定用于该模块的变量，用来存取客户端ip地址。 zone=one:10m：定义一个大小为10m的共享内存区域，名称为one。对于ipv4地址占用4个字节，ipv6占用16个字节。存储一个连接状态在32位机器上占用64个字节，在64位机器上占用128个字节。1MB的空间可以存储16k个64字节状态或者是8k个128字节状态。所以10m的空间可以存储大约8w个状态，对于一般的中小型站点已经足够了。 rate=1r/s:定义请求速率为每秒仅接受一个请求 在server配置段中引用上述配置(第5行)： zone=one:引用上面limit_req_zone定义的内存区域one burst:定义请求缓存的长度，意思是如果某ip1秒内发来了10个请求，那么除了正在处理的1个请求，其他的请求会把暂时放置到burst中排队。超过rate+burst请求将会被全部丢弃 nodelay:一次处理burst+rate个请求，其余全部丢弃。如果不希望在请求受到限制时延迟过多的请求应当使用这个参数 delay=num:瞬时处理num+rate个请求，总共缓存burst个，剩余的burst-num按rate定义处理。丢弃多余请求。 其他指令 limit_req_dry_run 语法： limit_req_dry_run on | off 默认值： limit_req_dry_run off; 语境： http server location 该指令开启\"干跑\"模式。开启该配置，请求处理的速率不再被限制。但是在共享内存区域，过量的请求的数值会像之前一样计算。 limit_req_log_level 语法: limit_req_log_level info | notice | warn | error; 默认值: limit_req_log_level error; 语境: http server location 该指令设置rate超过限值或者延时请求处理的日志级别，默认为error级别。 limit_req_status 语法: limit_req_status code; 默认值: limit_req_status 503; 语境: http server location 该指令定义拒绝响应请求的http状态码，默认返回*503 测试 1、不开启burst，不开启nodelay 配置如下所示： http { include mime.types; default_type application/octet-stream; limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s; log_format main '$remote_addr \"$request\"' '$status' '\"$http_user_agent\"'; server { listen 80; server_name localhost; charset utf-8; location / { limit_req zone=one; root /usr/share/nginx/html/; index index.html index.htm; } } } 在另一台虚拟机使用Apahce Benchmark进行压力测试 ab -c 10 -n10 http://192.168.0.106/index.html 结果如下： 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 200 70 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:20:58:41 +0800] \"GET / HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 严格按照rate定义处理请求，除了第一个请求外其余所有的请求全部丢弃并返回503。 2、在18行末尾添加burst=5 192.168.0.107 - - [05/May/2020:21:53:35 +0800] \"GET /index.html HTTP/1.0\" 200 70 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21:53:35 +0800] \"GET /index.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21:53:35 +0800] \"GET /index.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21:53:35 +0800] \"GET /index.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21:53:35 +0800] \"GET /index.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21:53:36 +0800] \"GET /index.html HTTP/1.0\" 200 70 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21:53:37 +0800] \"GET /index.html HTTP/1.0\" 200 70 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21:53:38 +0800] \"GET /index.html HTTP/1.0\" 200 70 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21:53:39 +0800] \"GET /index.html HTTP/1.0\" 200 70 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21:53:40 +0800] \"GET /index.html HTTP/1.0\" 200 70 \"-\" \"ApacheBench/2.3\" 按照定义，1秒内来了10个请求，burst=5，rate=1。总共缓存5个请求，处理1个请求，其余全部丢弃。这就是21:53:35内丢弃了4个请求。随后缓存的请求按照rate定义值处理。 3、在18行末尾继续添加nodelay 192.168.0.107 - - [05/May/2020:21:58:19 +0800] \"GET /index.html HTTP/1.0\" 200 70 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:21","date":"2020-05-05","objectID":"/posts/nginx-limit%E6%A8%A1%E5%9D%97/:0:1","tags":["Nginx"],"title":"Nginx-Limit模块","uri":"/posts/nginx-limit%E6%A8%A1%E5%9D%97/"},{"categories":["Nginx"],"content":"ngx_http_limit_conn_module ngx_http_limit_conn_module与ngx_http_limit_req_module主要区别在于，conn限制的连接的数量，req限制的是请求的数量。一个连接的生命周期中，会存在一个或者多个请求，这是为了加快效率，避免每次请求都要三次握手建立连接，现在的HTTP/1.1协议都支持这种特性，叫做keepalive。 官方配置示例： http { limit_conn_zone $binary_remote_addr zone=addr:10m; server { location /download/ { limit_conn addr 1; } limit_conn_zone：设置共享区域的参数，该区域将保留各种键的状态，与ngx_http_limit_req_module不同的是，此模块仅定义了区域了大小。键里可以保存文本，变量或者两者的组合。 limit_conn:设置共享内存区域以及键定义的最大允许的连接数量。超过限值时，服务器将会发送错误给客户端。limit_conn addr 1允许一个ip一次最多建立一次连接。如果是HTTP/2，那么每个并发的请求都会被视作是一个单独的连接。 limit_conn_dry_run,limit_conn_log_level,limit_conn_status与ngx_http_limit_req_module中一致不再说明。 测试 192.168.0.107 - - [05/May/2020:23:54:27 +0800] \"GET /test.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:23:54:27 +0800] \"GET /test.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:23:54:27 +0800] \"GET /test.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:23:54:27 +0800] \"GET /test.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:23:54:27 +0800] \"GET /test.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:23:54:27 +0800] \"GET /test.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:23:54:27 +0800] \"GET /test.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:23:54:27 +0800] \"GET /test.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:23:54:27 +0800] \"GET /test.html HTTP/1.0\" 503 197 \"-\" \"ApacheBench/2.3\" 192.168.0.107 - - [05/May/2020:23:54:29 +0800] \"GET /test.html HTTP/1.0\" 200 536870912 \"-\" \"ApacheBench/2.3\" test.html为50M，测试1秒内仅能建立一个连接其余全部丢弃。但index.html测试失败，不知道和文件的大小存在什么关联？ ","date":"2020-05-05","objectID":"/posts/nginx-limit%E6%A8%A1%E5%9D%97/:0:2","tags":["Nginx"],"title":"Nginx-Limit模块","uri":"/posts/nginx-limit%E6%A8%A1%E5%9D%97/"},{"categories":["Nginx"],"content":"limit_rate 指令 limit_rate是ngx_http_core_module这个核心模块自带的一个配置选项。可以用来限制单个连接的下载速率。对于资源文件下载服务器来说，有必要限制这个值，防止单个ip过高的速率影响他人的正常使用。 语法: limit_rate rate; 默认值: limit_rate 0; 语境: http, server, location, if in location 默认情况下，这个值是0，也就是不限制下载速率。官方推荐使用map来灵活定义下载速率。如： map $slow $rate { 0 40k; 1 80k; default 120k; } limit_rate $rate; #http，server，location，if in location 当然**$slow**不是nginx自带的变量，所以直接使用会报错。需要根据ngx_http_geo_module模块自带的geo指令来定义$slow变量。 geo $remote_addr $slow { default 0; 47.103.215.250 1; } 以上是这样的过程:$remote_addr是nginx自带的变量，geo指令拿到这个变量对应的ip地址去和下面的default或47.103.215.250匹配，如果满足$remote_addr=47.103.215.250,则把$slow变量设置为1,否则就设置为0。再拿0或者1去map里匹配得到$rate,最后在limit_rate中应用$rate来达到限制速率的要求。 注：geo 和 map都是应用在http段中 测试 ​ 本地下载测试为40KB/s ​ 47.103.215.250上测试为80KB/s 当然limit_rate只能限制单个ip的一个请求： The limit is set per a request, and so if a client simultaneously opens two connections, the overall rate will be twice as much as the specified limit. 如果客户端同时发起了两个链接，这个下载速度会变成限值的2倍。所以对于迅雷这种多线程的下载器，设置这样的限值的无效的。 解决的方法就是配合ngx_http_limit_conn_module模块，让服务器每秒只响应一个ip的一个请求，其余请求全部丢弃。 limit_conn_zone $binary_remote_addr zone=one:10m; server { listen 80; server_name localhost; location / { limit_conn one 1; limit_rate 80k; root /usr/share/nginx/html/; } } 在测试端下载test.html [root@k8s-node ~]# wget http://192.168.0.106/test.html --2020-05-06 00:00:43-- http://192.168.0.106/test.html Connecting to 192.168.0.106:80... connected. HTTP request sent, awaiting response... 200 OK Length: 536870912 (512M) [text/html] Saving to: ‘test.html.2’ test.html.2 0%[ ] 1.29M 79.6KB/s eta 1h 48m 复制终端会话再次请求： Last login: Tue May 5 23:40:24 2020 from 192.168.0.101 [root@k8s-node ~]# wget http://192.168.0.106/test.html --2020-05-06 00:01:18-- http://192.168.0.106/test.html Connecting to 192.168.0.106:80... connected. HTTP request sent, awaiting response... 503 Service Temporarily Unavailable 2020-05-06 00:01:19 ERROR 503: Service Temporarily Unavailable. nginx直接返回了503，这样就达到了限制单个ip的请求速率的目的。 ","date":"2020-05-05","objectID":"/posts/nginx-limit%E6%A8%A1%E5%9D%97/:0:3","tags":["Nginx"],"title":"Nginx-Limit模块","uri":"/posts/nginx-limit%E6%A8%A1%E5%9D%97/"},{"categories":["Redis"],"content":"每隔一段时间Redis会把数据持久化到硬盘进行数据的保存。其中持久化方案包括RDB和AOF模式 ，其中RDB全称为Redis DataBase，AOF全称为Append Only File。Redis默认使用的是RDB方式进行数据的持久化。 ","date":"2020-04-25","objectID":"/posts/redis%E6%8C%81%E4%B9%85%E5%8C%96/:0:0","tags":["Redis"],"title":"Redis持久化","uri":"/posts/redis%E6%8C%81%E4%B9%85%E5%8C%96/"},{"categories":["Redis"],"content":"RDB 从英文的全称可以看出区别，RDB是把所有的数据进行以快照的方式进行持久化。快照文件的生成可以使用两个命令来完成，即save和bgsave。 127.0.0.1:6379\u003e help save SAVE - summary: Synchronously save the dataset to disk since: 1.0.0 group: server 127.0.0.1:6379\u003e help bgsave BGSAVE - summary: Asynchronously save the dataset to disk since: 1.0.0 group: server 从summary可以看出，save是以同步的方式把数据以快照方式写入磁盘，而bgsave是异步的写入。所谓同步即redis在执行rdb备份时，期间redis不能响应外部的请求直到完成本次备份。而异步是redis调用Linux的内核函数fork()生成一个子进程，并且由该子进程来完成数据的持久化，所以备份时redis仍然可以响应外部的请求。 持久化配置 配置文件redis.conf中的save即为bgsave模式，官方配置文件中有save三种默认的选项配置： 218 save 900 1 #每900秒(15分钟)有1个键更新就执行一次bgsave 219 save 300 10 #每300秒(5分钟)有10个键更新就执行一次bgsave 220 save 60 10000 #每60秒(1分钟)有10000个键更新就执行一次bgsave 三种配置不是互斥的，只要有一个满足了条件就会触发备份操作。同时设置多个save是为了匹配不同的场景需要。若按照上面配置save 60 10000进行，并且redis在一分钟内持续更新了10000个键，但是服务器却在第59.99秒发生宕机，那后果可想而知:这一分钟内更新的所有键将全部丢失并且无法恢复。生产环境需要根据请求的调整合适的配置。 备份文件配置 253 dbfilename dump.rdb 263 dir ./ 默认情况下备份保存在./dump.rdb中,默认在配置文件redis.conf所在的目录。可以手动更改配置文件备份的地址和文件名重启redis，也可以在redis命令行工具中热变更，比如将备份调整为/var/redis/redis-6379.rdb: 127.0.0.1:6379\u003e CONFIG SET dir /var/redis OK 127.0.0.1:6379\u003e CONFIG SET dbfilename resid-6379.rdb OK 完成后可以使用CONFIG REWRITE将变更合并到配置文件中。 其它选项 241 rdbcompression yes 将备份出的rdb文件按照LZF算法进行压缩，可以大大减少备份文件的体积，此选项是默认开启的。当然为了节省CPU时间，可以改为no,但是快照文件的大小会大大增加。 250 rdbchecksum yes 默认情况下，一串CRC64d的检验和会追加到快照文件的尾部，这个可以使得快照文件更耐破坏。但是在保存和加载快照文件会损失约10%的性能。 ","date":"2020-04-25","objectID":"/posts/redis%E6%8C%81%E4%B9%85%E5%8C%96/:0:1","tags":["Redis"],"title":"Redis持久化","uri":"/posts/redis%E6%8C%81%E4%B9%85%E5%8C%96/"},{"categories":["Redis"],"content":"AOF 与RDB模式不同的是，AOF仅仅保存修改命令，类似mysql的bin-log日志。也就是说RDB保存的是数据快照，AOF保存的是修改命令。默认情况下AOF是关闭的，Redis仅使用RDB方式持久化数据。 699 appendonly no 在命令行中使用 CONFIG SET appendonly yes来开启AOF持久化模式，同样的可以使用CONFIG REWRITE将变更合并到配置文件中。 相应的持久化数据会默认保存到appendonly.aof中，路径即是dir中定义的路径。如果变更在写入过程中服务器发生了宕机，变更的命令位于来不及写入到**.aof**文件,也会造成数据丢失。Redis通过fsync()系统调用将执行的命令写入磁盘，这里Redis也提供了三种模式用于控制fsync的速度。 持久化配置 728 appendfsync always 729 appendfsync everysec 730 appendfsync no always:Redis每次操作都写入**.aof**文件中，最慢但也是最安全的选项，几乎不存在数据的丢失 no:Redis从不执行fsync()系统调用，而是让操作系统决定什么时候将数据写入磁盘，最快但数据存在数据丢失风险 everysecond:每秒调用一次fsync()，折中方案也是默认配置 日志重写 如果生产环境开启了AOF模式，那么生成的.aof文件会变得越来越大。aof重写的主要包括1.删除抵消的命令 2.合并重复的命令，比如说一个Python Django项目使用Redis来作为计数器，该计数器在5分钟发生了1000次的值更新，但是数据集中包含最终值的键只有一个，而在AOF中却包含1000个条目，不需要其中的999个条目来重建当前状态。所以这999个条目可以在重建时可以全部删除。 Redis支持一个有趣的功能：它可以在后台fork()出一个子进程重建AOF，而不会中断对客户端的服务。每当您发出BGREWRITEAOF，Redis都会编写最短的命令序列来重建内存中的当前数据集。除了手动的在Redis命令行中手动执行外，还可以在配置文件中声明自动重建的配置。 770 auto-aof-rewrite-percentage 100 771 auto-aof-rewrite-min-size 64mb auto-aof-rewrite-min-size 64mb指的是数据量达到64mb就触发执行一次重写操作，如果redis启动时数据量大于64mb，则会自动进行一次重写操作。此后这条配置项将不再有效。 auto-aof-rewrite-percentage 100此第一次重写后后当数据再次达到100%，即128mb时，会再次执行一次重写操作，以此类推，不断执行自动重写。 如果auto-aof-rewrite-percentage 0,则会禁用自动重写 auto-aof-rewrite-min-size的值不能太小，否则会频繁重写造成性能降低 ","date":"2020-04-25","objectID":"/posts/redis%E6%8C%81%E4%B9%85%E5%8C%96/:0:2","tags":["Redis"],"title":"Redis持久化","uri":"/posts/redis%E6%8C%81%E4%B9%85%E5%8C%96/"},{"categories":["Redis"],"content":"总结 RDB持久性按指定的时间间隔执行数据集的时间点快照 AOF持久性会记录服务器接收的每个写入操作，这些操作将在服务器启动时再次播放，以重建原始数据集。使用与Redis协议本身相同的格式记录命令，并且仅采用追加方式。当日志太大时，Redis可以在后台重写日志 如果希望，只要您的数据在服务器运行时就一直存在，则可以完全禁用持久性。当做memcache使用 可以在同一实例中同时合并AOF和RDB。请注意，在这种情况下，当Redis重新启动时，AOF文件将用于重建原始数据集，因为它可以保证是最完整的 ","date":"2020-04-25","objectID":"/posts/redis%E6%8C%81%E4%B9%85%E5%8C%96/:0:3","tags":["Redis"],"title":"Redis持久化","uri":"/posts/redis%E6%8C%81%E4%B9%85%E5%8C%96/"},{"categories":["Docker"],"content":"Dockerfile中的RUN，CMD，ENTRTPOINT三个指令均可以用来指明容器中所运行的指令，但这三者存在的细微的区别。 简单来说： ","date":"2020-03-25","objectID":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/:0:0","tags":["Docker"],"title":"Dockerfile中RUN CMD以及ENTRYPOINT的区别","uri":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Docker"],"content":"RUN RUN指令一般用于在容器内安装软件包或者是执行其他的命令，如 RUN yum install -y telnet RUN touch web.xml ","date":"2020-03-25","objectID":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/:0:1","tags":["Docker"],"title":"Dockerfile中RUN CMD以及ENTRYPOINT的区别","uri":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Docker"],"content":"CMD CMD指令主要用来指明生成的Docker镜像在启动时的命令及参数，这个指令可以被docker run后面的命令所取代，比如下面这个Dockerfile文件 FROM busybox CMD echo \"hello world\" CMD指明了Docker镜像在运行时的输出一个\"hello world\" [root@bochs Docker]# docker build -t test . Sending build context to Docker daemon 2.048kB Step 1/2 : FROM busybox ---\u003e 83aa35aa1c79 Step 2/2 : CMD echo \"hello world\" ---\u003e Running in a1a4d74137d2 Removing intermediate container a1a4d74137d2 ---\u003e 651b45b58fe9 Successfully built 651b45b58fe9 Successfully tagged test:latest [root@bochs Docker]# docker run -it test hello world 但是如果在docker run后添加其他指令。那么CMD将直接被替换 [root@bochs Docker]# docker run -it test ls bin dev etc home proc root sys tmp usr var ","date":"2020-03-25","objectID":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/:0:2","tags":["Docker"],"title":"Dockerfile中RUN CMD以及ENTRYPOINT的区别","uri":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Docker"],"content":"ENTRYPOINT ENTRYPOINT与CMD类似，区别在于ENTRYPOINT一定会被执行。如果一个Dockerfile中同时存在ENTRYPOINT和 CMD，CMD中的参数会被当做额外参数传给ENTRYPOINT。 [root@bochs Docker]# cat Dockerfile FROM busybox ENTRYPOINT [\"/bin/echo\",\"hello\"] CMD [\"world\"] 通过docker run 来运行，CMD变成了ENTRYPOINT的参数： [root@bochs Docker]# docker run -it test2 hello world 但是如果指明docker run 的参数china，那么输出就会变为： [root@bochs Docker]# docker run -it test2 china hello china 原本CMD中带的参数world被docker run中的china所替换，但ENTRYPOINT自带的hello依然正常输出 ","date":"2020-03-25","objectID":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/:0:3","tags":["Docker"],"title":"Dockerfile中RUN CMD以及ENTRYPOINT的区别","uri":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Docker"],"content":"Shell与Exec格式 CMD，RUN，ENTRYPOINT可以用两种格式来传递命令和参数，Shell一般表示为指令+命令，如: RUN yum install -y telnet CMD echo \"hello world\" 第一个大写的单词是Dockerfile的指令。后面跟的就是命令，可以拿到shell中单独执行 Exec格式可以表示为:指令+[“命令”,“命令参数1”,“命令参数2”,…],比如: RUN [\"yum\",\"install\",\"telnet\"] ENTRYPOINT [\"/bin/bash\",\"-c\",\"echo hello world\"] 对于这两种格式来说，CMD和ENTRYPOINT最好使用Exec格式，命令和参数分开，层次性较强，而RUN则都可以。 注意:ENTRYPOINT的Shell格式和Exec格式差异很大 比如下面这个Shell格式的ENTRYPOINT FROM busybox ENTRYPOINT echo \"hello\" CMD \"world\" 在运行所生成的容器时，仅会输出hello，而CMD带的\"world\"会被忽略。同样的docker run带的参数也同样会被忽略 [root@bochs Docker]# docker run -it test hello [root@bochs Docker]# docker run -it test china hello ","date":"2020-03-25","objectID":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/:0:4","tags":["Docker"],"title":"Dockerfile中RUN CMD以及ENTRYPOINT的区别","uri":"/posts/dockerfile%E4%B8%ADrun-cmd%E4%BB%A5%E5%8F%8Aentrypoint%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Docker"],"content":"Prometheus是非常优秀的监控工具，准确的说是一套完整的监控方案。提供了数据收集，存储，处理，加工展示，告警等一系列完整解决方案 ","date":"2020-03-06","objectID":"/posts/prometheus-grafana%E7%9B%91%E6%8E%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA/:0:0","tags":["Docker","Monitoring"],"title":"Prometheus+Grafana监控宿主机","uri":"/posts/prometheus-grafana%E7%9B%91%E6%8E%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA/"},{"categories":["Docker"],"content":"关键组件 Prometheus关键组件包括：Prometheus Server,Exporter,可视化组件，Alertmanager四大模块 Prometheus Server ​ Prometheus Server负责从Exporter拉取及存储数据，并且提供了一套查询语句（PromQL）供用户使用 Exporter Exporter负责收集目标系统的各类性能数据，目标系统可以使host也可以是container容器。并且通过HTTP接口提供给Prometheus Server获取 Alertmanager Alertmanager提供了基于监控数据的告警规则，一旦Alertmanager收到告警就会通过预先定义的方式发出告警通知 可视化组件 数据的可视化是监控工具至关重要的，Prometheus有自己开发的展示方案，后来被更加优秀的开源产品Grafana替代，Grafana能与Prometheus完美结合提供完美的数据展示能力 ","date":"2020-03-06","objectID":"/posts/prometheus-grafana%E7%9B%91%E6%8E%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA/:0:1","tags":["Docker","Monitoring"],"title":"Prometheus+Grafana监控宿主机","uri":"/posts/prometheus-grafana%E7%9B%91%E6%8E%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA/"},{"categories":["Docker"],"content":"实战 1.环境搭建 实例通过prometheus监控两台宿主机，监控host单个层面，也选择适当的DashBoard监控container环境。 本实验将以容器方式运行以下的组件： （1）Prometheus Server会以容器方式运行在192.168.0.102上 （2）Exporter Node Exporter 负责收集宿主机的硬件以及操作系统的数据，以容器方式运行在宿主机上 cAdvisor 负责收集容器数据，以容器方式运行在宿主机上 （3）Grafana：用于展示监控数据 ip Node Exporter cAdvisor Prometheus Server Grafana 192.168.0.101 √ √ 192.168.0.102 √ √ √ √ 2.运行Node Exporter 在两个host上执行如下的命令: docker run -d --name node-exporter -p 9100:9100 \\ -v \"/proc:/host/proc:ro\" -v \"/sys:/host/sys:ro\" -v \"/:/rootfs:ro\" \\ --restart=always --net=\"host\" \\ prom/node-exporter \\ --path.procfs /host/proc --path.sysfs /host/sys \\ --collector.filesystem.ignored-mount-points \"^/(sys|proc|dev|host|etc)($|/)\" 使用–net=“host\"目的是为了Node Exporter和Prometheus Server直接通信 运行完上述命令后就可以访问任意ip的:9100来获取收集到的数据了。在浏览器上输入http://192.168.0.101:9100/metrics测试 3.运行cAdvisor 在两个host上运行如下命令启动cAdvisor容器 docker run -v /:/rootfs:ro -v /var/run:/var/run:rw \\ -v /sys:/sys:ro -v /var/lib/docker:/var/lib/docker:ro \\ -p8080:8080 -t --name cadvisor --net=\"host\" \\ google/cadvisor:latest 同样使用–net=“host\"使cAdvisor与Prometheus Server直接通信 4.运行Prometheus Server 在192.168.0.102主机上执行如下命令启动Prometheus Server docker run -d -p 9090:9090 \\ -v /root/prometheus.yml:/etc/prometheus/prometheus.yml \\ --name prometheus --net host \\ prom/prometheus 其中prometheus.yml是Prometheus Server的配置文件，如下配置所示，最重要的配置就是最下面的static_config段，它指定了从哪些exporter抓取数据 # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - \"first_rules.yml\" # - \"second_rules.yml\" # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=\u003cjob_name\u003e` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. scrape_interval: 5s static_configs: - targets: ['localhost:9090','localhost:8080','localhost:9100','192.168.0.101:9100','192.168.0.101:8080'] 打开http://192.168.0.102:9090,点击菜单栏的Status切换到Targets，输出如下： 所有State均为UP，表示Prometheus Server正常获取到所有监控数据 5.安装Grafana 在192.168.0.102上执行如下命令运行Grafana docker run -id -p 3000:3000 \\ -e \"GF_SERVER_ROOT_URL=http://grafana.server.com\" \\ -e \"GF_SECURITY_ADMIN_PASSWORD=admin@123\" \\ --net=\"host\" grafana/grafana 这里使用-e“GF_SECURITY_ADMIN_PASSWORD=admin@123\"指定Grafana admin用户密码为admin@123 访问http://192.168.0.102:3000进入Grafana界面，Grafana将会引导我们配置Data Source 配置完Data Source后需要通过Dashboard展示数据，配置展示数据很困难，可以使用开源社区的力量 使用现成的Dashboard,下载现成的Dashboard的json文件导入到Grafana就可以展示监控数据了，这里我选择了只监控host的Dashboard ","date":"2020-03-06","objectID":"/posts/prometheus-grafana%E7%9B%91%E6%8E%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA/:0:2","tags":["Docker","Monitoring"],"title":"Prometheus+Grafana监控宿主机","uri":"/posts/prometheus-grafana%E7%9B%91%E6%8E%A7%E5%AE%BF%E4%B8%BB%E6%9C%BA/"},{"categories":["Performance"],"content":"sar是强大的linux系统活动状况收集、报告命令。可以收集CPU，内存，磁盘I/O，网络等诸多数据。对于性能分析是个可靠的利器，本文介绍sar命令的各种用法。 ","date":"2020-02-29","objectID":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/:0:0","tags":["Performance"],"title":"sar命令详解","uri":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/"},{"categories":["Performance"],"content":"安装 sar命令是sysstat下的一个工具，所以安装sar需要首先安装sysstat命令，可以考虑yum安装或者使用源码包编译安装等。yum 安装十分便捷，不需要任何复杂的调试就可以使用 #ubuntu sudo apt-get install -y sysstat #centos sudo yum install -y sysstat yum仓库目前安装的版本为10.1.5，相对于最新版12.3.1要旧很多，所以新的特性可能会无法使用，我推荐下载 最新版本源码包编译。 #安装gcc等重要环境 sudo yum install -y gcc gcc-c++ #centos sudo apt-get install -y gcc gcc-c++ #ubuntu #下载安装包至/tmp目录 wget http://pagesperso-orange.fr/sebastien.godard/sysstat-12.3.1.tar.gz -O /tmp #进入/tmp目录并解压 cd /tmp \u0026\u0026 tar -xzvf sysstat-12.3.1.tar.gz #进入解压目录编译安装 cd sysstat-12.3.1 \u0026\u0026 ./configure \u0026\u0026 make \u0026\u0026 make install #最后查看任一命令的版本即可得到sysstat版本 Ξ (bochs) /tmp/sysstat-12.3.1 → mpstat -V sysstat version 12.3.1 (C) Sebastien Godard (sysstat \u003cat\u003e orange.fr) 初次使用sar命令会遇到如下报错 Ξ (bochs) ~ → sar Cannot open /var/log/sa/sa29: No such file or directory Please check if data collecting is enabled 这是因为sar找不到记录数据的源文件，这时只需要使用-o参数生成即可sar -o 2 3。 ","date":"2020-02-29","objectID":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/:0:1","tags":["Performance"],"title":"sar命令详解","uri":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/"},{"categories":["Performance"],"content":"常用用法 sar [command] 2 5 : 每2秒输出一次sar [command],总计输入五次，省略5表示持续输出 sar -n DEV 1 -e 22:26:00 \u003e/tmp/123 \u0026:每秒采样一次网络情况直到22:26并把采样数据输出到/tmp/123 sar -f /var/log/sa/sa27 -s 23:00:00 -e 00:00:00 -r:本月27日23点至0点的内存数据，需要通过crontab设置定时任务 ","date":"2020-02-29","objectID":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/:0:2","tags":["Performance"],"title":"sar命令详解","uri":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/"},{"categories":["Performance"],"content":"CPU篇 -p -P {CPU_LIST | ALL}:用于分析多核CPU的性能状况，可以使用CPU_LIST分析指定核心的CPU状态，可以使用离散值和连续值，也可以使用ALL分析所有CPU核心状态。 Ξ (bochs) ~ → sar -P 0 1 3 Linux 4.15.0-87-generic (test) 02/29/20 _x86_64_ (1 CPU) 16:31:33 CPU %user %nice %system %iowait %steal %idle 16:31:34 0 0.00 0.00 0.00 0.00 0.00 100.00 16:31:35 0 0.00 0.00 0.00 0.00 0.00 100.00 16:31:36 0 0.00 0.00 0.00 0.00 0.00 100.00 Average: 0 0.00 0.00 0.00 0.00 0.00 100.00 表示每秒采集**0**号CPU状态，总共采样3次。 前两列不必多言，%user指运行非特权用户进程时间百分率 %nice是指运行特权用户进程时间百分率 %system是指运行内核进程时间，这个时间包括了CPU处理软硬中断的时间 %iowait是指等待I/O完成的时间 %steal是指运行虚拟机的时间百分率，steal意味着被偷走的时间 %idle是指cpu空闲时间百分率，我的机器上并未运行任何程序，所以此列一直为100% -u -u[ALL]:报告cpu使用情况，与-p不同的是，-u只能报告所有cpu。ALL选项输出详细信息 Ξ (bochs) ~ → sar -u ALL 1 3 Linux 4.15.0-87-generic (test) 02/29/20 _x86_64_ (1 CPU) 17:23:29 CPU %usr %nice %sys %iowait %steal %irq %soft %guest %gnice %idle 17:23:30 all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 17:23:31 all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 17:23:32 all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 Average: all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 这里的%usr和-P的%user的区别在于%usr不包括虚拟机运行的时间 这里的%sys和-P的%system的区别在于%sys不包括各种软硬中断时间 %irq是指处理硬中断的cpu时间百分率 %soft是指处理软中断的cpu时间百分率 %guest和%gnice分别指运行普通虚拟程序和特权虚拟程序的时间百分率 -q -q:用于报告队列长度以及平均负载 Ξ (bochs) ~ → sar -q 1 3 Linux 4.15.0-87-generic (test) 02/29/20 _x86_64_ (1 CPU) 16:46:47 runq-sz plist-sz ldavg-1 ldavg-5 ldavg-15 blocked 16:46:48 0 126 0.00 0.00 0.00 0 16:46:49 0 126 0.00 0.00 0.00 0 16:46:50 0 126 0.00 0.00 0.00 0 Average: 0 126 0.00 0.00 0.00 0 runq-sz:等待cpu调度的任务数 plist-sz :处于任务列表的任务总数 ldavg-1，ldavg-5，ldavg-15分别指1分钟，5分钟，15分钟内cpu的负载 blocked:表示等待I/O完成而被阻塞的任务总数，不为0则需要留意I/O是否存在性能瓶颈 -w -w:报告进程上下文切换的次数 Ξ (bochs) ~ → sar -w 1 3 Linux 4.15.0-87-generic (test) 02/29/20 _x86_64_ (1 CPU) 17:39:43 proc/s cswch/s 17:39:44 0.00 83.00 17:39:45 0.00 103.00 17:39:46 0.00 96.00 Average: 0.00 94.00 proc/s:指每秒创建的进程数 cswch/s:指每秒自愿上下文切换的次数，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。 还有一个非自愿的上下文切换次数nvcswch/s表示则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。非自愿次数明显升高意味着cpu产生了性能瓶颈。非自愿上下文切换可以使用pidstat加上-w选项来输出 ","date":"2020-02-29","objectID":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/:0:3","tags":["Performance"],"title":"sar命令详解","uri":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/"},{"categories":["Performance"],"content":"内存篇 -r -r [-h]:输出内存使用率统计信息，-h输出更加利于阅读的结果 Ξ (bochs) ~ → sar -r -h 1 3 Linux 4.15.0-87-generic (test) 02/29/20 _x86_64_ (1 CPU) 21:58:21 kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty 21:58:22 602.6M 1.6G 165.3M 8.3% 196.8M 878.7M 743.3M 37.3% 830.7M 379.7M 4.0k 21:58:23 602.6M 1.6G 165.3M 8.3% 196.8M 878.7M 743.3M 37.3% 830.7M 379.7M 4.0k 21:58:24 602.6M 1.6G 165.3M 8.3% 196.8M 878.7M 743.3M 37.3% 830.7M 379.7M 4.0k Average: 602.6M 1.6G 165.3M 8.3% 196.8M 878.7M 743.3M 37.3% 830.7M 379.7M 4.0k kbmemfree:剩余内存总量 kbavail:可用内存总量，可用内存≈剩余内存+buffer+cache kbmemused:使用的内存总量，使用量=总内存-剩余内存-buffer-cache-slab kbbuffers:被内核用来作为buffer的内存量 kbcached:被内核用来作为cache的内存量 kbcommit:当前工作负载下，可以保证不出现内存不足的内存量 %commit:指kbcommit占内存/swap的百分率 kbactive:当前活跃内存量。除非万不得已，这部分内存才会被回收 kbinact:当前非活跃内存总量，当内存不足时，这部分内存最容易被内核收回 kbdirty：脏页大小，脏页指的是暂存于内存还没来得及持久化到硬盘的数据。可以使用sync刷入硬盘 -B -B:报告系统中分页统计信息 λ bochs ~ → sar -B 1 3 Linux 4.4.213-1.el7.elrepo.x86_64 (bochs) 03/02/2020 _x86_64_ (1 CPU) 10:43:36 PM pgpgin/s pgpgout/s fault/s majflt/s pgfree/s pgscank/s pgscand/s pgsteal/s %vmeff 10:43:37 PM 0.00 60.00 18.00 0.00 5.00 0.00 0.00 0.00 0.00 10:43:38 PM 0.00 20.00 158.00 0.00 153.00 0.00 0.00 0.00 0.00 10:43:39 PM 0.00 60.00 53.00 0.00 71.00 0.00 0.00 0.00 0.00 Average: 0.00 46.67 76.33 0.00 76.33 0.00 0.00 0.00 0.00 pgpgin/s:表示每秒从磁盘中换入内存的字节数 pgpgout/s:表示每秒从内存换出到磁盘的字节数 fault/s:表示系统每秒产生的缺页异常（报告主缺页和次缺页），这不是产生I/O的缺页中断的次数，因为部分缺页中断不需要I/O就能处理 majflt/s:表示每秒产生的主缺页异常 pgfree/s:每秒被系统放到空闲列表的分页数量 pgscank/s:每秒被内核线程[kswapd]扫描的分页数量 pgscand/s:每秒被直接扫描的数量 pgsteal/s:系统为满足其内存需求声明每秒从cache(分页缓存和swap缓存)回收的页的数量 %vmeff:这个指标由pgsteal/(pgscand+pgscank)得到，这是一个衡量页面回收效率的指标 -S -s [h]:输出swap空间的使用率统计信息 Ξ (bochs) ~ → sar -S 1 1 Linux 4.15.0-87-generic (test) 02/29/20 _x86_64_ (1 CPU) 22:20:48 kbswpfree kbswpused %swpused kbswpcad %swpcad 22:20:49 0 0 0.00 0 0.00 Average: 0 0 0.00 0 0.00 我的这台机器上未开启swap，所有的统计信息均为0 kbswpfree:未使用的swap量 kbswpused：使用中的swap内存量 %swpused：使用中的swap内存量占总swap的百分率 kbswpcad:被swap缓存的内存量，这部分内存曾经被换出，现在又被换入但仍然位于swap空间 %swpcad:kbswpcad占kbswpused的百分率 -W -W统计输出swap换入换出信息 Ξ (bochs) ~ → sar -W 1 2 Linux 4.15.0-87-generic (test) 02/29/20 _x86_64_ (1 CPU) 22:43:36 pswpin/s pswpout/s 22:43:37 0.00 0.00 22:43:38 0.00 0.00 Average: 0.00 0.00 pswpin/s:每秒换入swap的内存量 pswpout/s:每秒换出swap的内存量 若这两项数值很高，表示内存短缺导致需要频繁换入换出。 ","date":"2020-02-29","objectID":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/:0:4","tags":["Performance"],"title":"sar命令详解","uri":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/"},{"categories":["Performance"],"content":"I/O篇 -b -b:报告I/O及传输速率统计信息 Ξ (bochs) ~ → sar -b 1 3 Linux 4.4.213-1.el7.elrepo.x86_64 (bochs) 03/03/2020 _x86_64_ (1 CPU) 12:52:28 PM tps rtps wtps dtps bread/s bwrtn/s bdscd/s 12:52:29 PM 10.00 0.00 10.00 0.00 0.00 280.00 0.00 12:52:30 PM 18.00 0.00 18.00 0.00 0.00 256.00 0.00 12:52:31 PM 9.00 0.00 9.00 0.00 0.00 144.00 0.00 Average: 12.33 0.00 12.33 0.00 0.00 226.67 0.00 tps:每秒钟加到物理设备上的传输总量。一次传输就是加到物理设备的一次I/O请求，由于多次逻辑请求可以合并为单次的I/O请求，所以一次传输的大小是不确定的 rtps：每秒加到物理设备的读请求总数 wtps：每秒加到物理设备上的写请求总数 dtps:每秒丢弃的请求总数 bread/s:以块为单位每秒钟从磁盘读取的数据总量，块的大小等同于一个扇区的大小，512字节 bwrtn/s：以块为单位每秒钟写入磁盘的数据总量 bdscd/s以块为单位丢弃的数据总量 -d -d -h[--dev=dev_list]:报告块设备的活动情况 Ξ (bochs) ~ → sar -d 1 1 Linux 4.4.213-1.el7.elrepo.x86_64 (bochs) 03/03/2020 _x86_64_ (1 CPU) 08:06:35 PM DEV tps rkB/s wkB/s dkB/s areq-sz aqu-sz await %util 08:06:36 PM vda 9.00 0.00 100.00 0.00 11.11 0.02 1.89 1.70 tps:和-b的tps含义一样，都表示每秒钟加到物理设备上的传输总量 rkB/s:每秒从设备读到的字节数 wkB/s:每秒写入设备的字节数 areq-sz:加到设备上I/O请求平均大小(以字节为大小) aqu-sz:加到设备上请求长度的平均值 await:加到设备上I/O请求的平均响应时间，这个时间包括了请求处于等待队列中的时间 %util:加到设备上I/O请求所用的时间百分比，对于串行设备，接近100%意味着设备出现了性能瓶颈，但是对于并行设备比如RAID或者SSD，这个值实际上并不能反映出设备的极限 -v -v:报告inode，文件以及其他内核表状态 Ξ (bochs) ~ → sar -v 1 1 Linux 4.4.213-1.el7.elrepo.x86_64 (bochs) 03/03/2020 _x86_64_ (1 CPU) 08:35:59 PM dentunusd file-nr inode-nr pty-nr 08:36:00 PM 1692 1504 9458 1 Average: 1692 1504 9458 1 dentunusd:目录缓存中未使用的缓存项数。 file-nr:系统使用的文件句柄数，查看目前使用的文件句柄数lsof|awk '{print $2}'|wc -l inode-nr:系统使用的inode处理程序数 pty-nr:打开的伪终端数，即几个人登陆了系统 ","date":"2020-02-29","objectID":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/:0:5","tags":["Performance"],"title":"sar命令详解","uri":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/"},{"categories":["Performance"],"content":"网络篇 -n -n DEV [--iface=face_list] Ξ (bochs) ~ → sar -n DEV 1 1 Linux 4.4.213-1.el7.elrepo.x86_64 (bochs) 03/03/2020 _x86_64_ (1 CPU) 09:04:07 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil 09:04:08 PM lo 20.00 20.00 1.19 1.19 0.00 0.00 0.00 0.00 09:04:08 PM docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 09:04:08 PM br-f3a0301d37ae 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 09:04:08 PM docker_gwbridge 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 09:04:08 PM eth0 9.00 7.00 0.68 0.78 0.00 0.00 0.00 0.00 IFACE:网络接口 rxpck/s:每秒接收的报文数 txpck/s:每秒发送的报文数 rxkB/s:每秒接收的字节数,``rxkB/s*1024/rxpck/s`\u003c60B,意味着收到的是小包 txkB/s:每秒发送的字节数 rxcmp/s:每秒接收的压缩数据包数 txcmp/s:每秒发送的压缩数据包数 rxmcst/s:每秒接收的多播数据包数 %ifutil:网络接口的利用率百分比，对于半双工接口，利用率使用rxkB/s和txkB/s之和作为接口速度的百分比计算。对于全双工，这是rxkB/s或txkB/s中的较大值。 ","date":"2020-02-29","objectID":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/:0:6","tags":["Performance"],"title":"sar命令详解","uri":"/posts/sar%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3/"},{"categories":["Linux"],"content":"Nginx不像Apache那样自带rotatelogs进行日志的回滚，默认配置的日志文件会越来越大造成无法阅读，必须手动为Nginx配置日志回滚的方式。可以使用自定义脚本或是借助Linux自带的logrotate命令实现日志回滚。 ","date":"2020-02-17","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E5%9B%9E%E6%BB%9A/:0:0","tags":["Linux"],"title":"Nginx日志回滚","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E5%9B%9E%E6%BB%9A/"},{"categories":["Linux"],"content":"脚本分割 脚本分割日志的方法比较容易理解，获取昨天的日期并将日志文件命名为带有昨天的日期的名称，重命名结束后向Nginx发送USR1信号,Nginx在收到USR1信号后重新打开日志文件并写入。 #!/usr/bin/env bash #anthor ltzhang #description split_nginx_log #获取日志文件的路径 LOGS_PATH=/opt/openresty/nginx/logs/ #获取该路径下所有以.log结尾日志 LOGS_LIST=$(ls $LOGS_PATH/*.log) #获取昨天的日期 YESTERDAY=$(date -d \"yesterday\" +%Y-%m-%d) #for循环遍历日志列表，将所有日志名称重命名为*.log.$(YESTERDAY) for logs in ${LOGS_LIST[@]};do varlog=$(basename $logs) mv -f $LOGS_PATH/$varlog $LOGS_PATH/$varlog.$YESTERDAY done #向Nginx的master进程发送USR1信号,Nginx重新打开*.log并写入日志 kill -USR1 $(cat ${LOGS_PATH}/../run/nginx.pid) 优点：灵活，文件名及路径可以自定义 缺点：需要在crontab中设置计划任务，较不方便管理阅读 ","date":"2020-02-17","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E5%9B%9E%E6%BB%9A/:0:1","tags":["Linux"],"title":"Nginx日志回滚","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E5%9B%9E%E6%BB%9A/"},{"categories":["Linux"],"content":"logrorate分割 logrotate是linux自带的日志回滚命令，可以实现日志回滚。logrotate借助crond实现日志回滚的计划任务,所在的路径为/etc/cron.daily/logrotate，即每天执行一次logrotate脚本。 打开脚本，可以发现内容定义 root 00:13:32 /etc \u003e\u003e\u003e cat /etc/cron.daily/logrotate #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 每天按照/etc/logrotate.conf配置文件中的定义执行一次日志回滚操作，并将该日志的状态写入/var/lib/logrotate/logrotate.status,如果执行失败，则错误状态码写入message日志中。 继续打开主配置文件可以看到许多配置 root 00:19:05 /etc \u003e\u003e\u003e grep -v '^$' /etc/logrotate.conf | grep -v '^#' weekly rotate 4 create dateext include /etc/logrotate.d /var/log/wtmp { monthly create 0664 root utmp minsize 1M rotate 1 } /var/log/btmp { missingok monthly create 0600 root utmp rotate 1 } weekly：每周执行日志回滚 rotate：历史日志保留4天 create：回滚旧日志后创建新文件。可带权限，改变属主属组。如:create 0664 nginx root dateext：以日期作为回滚文件的后缀。默认格式YYYYMMDD，其他的格式可用dateformat format_string自行定义。 除此以外还有些常用的参数： compress：压缩回滚文件，默认后缀.gz ifempty：若日志为空也执行回滚操作 maxsize size：除按照时间回滚外还支持文件最大大小回滚，当日志超过定义值时执行一次日志回滚操作，适用于每天日志较大的情况，如Nginx日志等 olddir directory：指定回滚文件的存放路径，必须与日志文件位于同一文件系统中 prerotate/endscript:定义回滚操作结束后执行的动作 配置Nginx回滚只需要按照上面的参数设置自己的配置段即可，例子如下： root 01:33:21 /opt/openresty/nginx/logs \u003e\u003e\u003e cat /etc/logrotate.d/openrestry /opt/openresty/nginx/logs/*log { create 0664 nginx root daily maxsize 20M rotate 14 ifempty nocompress sharedscripts postrotate /bin/kill -USR1 `cat /opt/openresty/nginx/run/nginx.pid 2\u003e/dev/null` 2\u003e/dev/null || true endscript } 如果发现没有正常回滚，可以使用logrotate -dv /etc/logrotate.conf观察输出 优点：1）可以定义最大大小，实现单日日志分割 2）可以实现指定数量的历史日志（脚本也可配置但较复杂）3）不用单独写入计划任务 缺点：无 ","date":"2020-02-17","objectID":"/posts/nginx%E6%97%A5%E5%BF%97%E5%9B%9E%E6%BB%9A/:0:2","tags":["Linux"],"title":"Nginx日志回滚","uri":"/posts/nginx%E6%97%A5%E5%BF%97%E5%9B%9E%E6%BB%9A/"},{"categories":["Apache"],"content":"apache的重定向主要是用RewriteCond及RewriteRule来完成，前者用于判断匹配条件，条件符合的将会转到下一条的Rewrite进行处理。 先看一个例子 #打开重写模块 RewriteEngine On #定义重写的基准路径 RewriteBase / #定义判断，若\"%{http_host}\"==\"dmesg.top\",则继续执行下面的RewriteRule RewriteCond %{http_host} ^dmesg.top$ [NC] #定义重写规则，将所有访问dmesg.top中的请求重定向到www.dmesg.top的相同路径 RewriteRule ^(.*)$ www.dmesg.top/$1 [R=301] ","date":"2020-02-17","objectID":"/posts/apache%E9%87%8D%E5%AE%9A%E5%90%91/:0:0","tags":["Apache"],"title":"Apache重定向","uri":"/posts/apache%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Apache"],"content":"RewriteCond 语法 RewriteCond String Pattern [flags] 定义一个条件，当String所规定的内容与Pattern规则匹配时，才会执行RewriteRule规定重写。 string （1）$N：RewriteRule后向引用。$N引用紧跟在RewriteCond之后的RewriteRule中Pattern的小括号中的规则在当前URL中匹配的内容。N是0 \u003c= N \u003c= 9之间的整数。 （2）%N：RewriteCond后向引用 。%N引用最后一个RewriteCond的Pattern中的小括号中的规则在当前URL中匹配的内容。N是0 \u003c= N \u003c= 9之间的整数。 pattern (1) !：表示TestString不与当前正则匹配；格式是!CondPattern。 (2) \u003e: 将condPattern作为普通字符串与String比较，String大于Pattern为真；格式是\u003ePattern。 (3) =：将condPattern作为普通字符串与String比较，String与Pattern相同时为真；格式是=Pattern。 (4) -d：将String当作一个目录名，检查它是否存在以及是否是一个目录；格式是-d。 (5) -f：将String当作一个文件名，检查它是否存在以及是否是一个文件；格式是-f。 (6) -s：将String当作一个文件名，检查它是否存在以及是否是一个长度大于0的文件；格式是-s。 (7) -l： 将String当作一个文件名，检查它是否存在以及是否是一个 symbolic link；格式是-l。 (8) -F：检查String是否是一个合法的文件，而且通过服务器范围内的当前设置的访问控制进行访问。检查通过一个内部subrequest完成的, 因此需要谨慎使用，以防止降低服务器的性能。 (9) -U：检查String是否是一个合法的URL，而且通过服务器范围内的当前设置的访问控制进行访问。检查通过一个内部subrequest完成的, 因此需要谨慎使用，以防止降低服务器的性能。 [flags] （1）NC：表示不区分大小写。 （2）OR：默认的情况下，二个RewriteCond之间是AND的关系，用这个标志将关系改为OR,即满足任一的RewriteCond即执行RewriteRule。 RewriteEngine On RewriteCond [regex1] [OR] RewrireCond [regex2] RewriteRule [rule] ","date":"2020-02-17","objectID":"/posts/apache%E9%87%8D%E5%AE%9A%E5%90%91/:0:1","tags":["Apache"],"title":"Apache重定向","uri":"/posts/apache%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Apache"],"content":"RewriteRule RewriteRule Pattern Substitution [flags] pattern 作用于当前URL的正则表达式；此url不包含协议、域名和查询字符串部分。 Substitution 当RewriteCond满足时，用来替换原始URL指定内容的字符串，还可以包括以下扩展： （1）$N：RewriteRule后向引用。$N引用紧跟在RewriteCond之后的RewriteRule中Pattern的小括号中的规则在当前URL中匹配的内容。N是0 \u003c= N \u003c= 9之间的整数。 （2）%N：RewriteCond后向引用 。%N引用最后一个RewriteCond的Pattern中的小括号中的规则在当前URL中匹配的内容。N是0 \u003c= N \u003c= 9之间的整数。 [flags] (1) R：表示重定性，[R=301]表示301重定向，默认是302重定向。 (2) F：强制当前URL为被禁止的，即，立即反馈一个HTTP响应代码403(被禁止的)。 (3) G：强制当前URL为已废弃的，即，立即反馈一个HTTP响应代码410(已废弃的)。 (4) L：立即停止重写操作，并不再应用其他重写规则。 (5) N：重新执行重写操作(从第一个规则重新开始). 这时再次进行处理的URL已经不是原始的URL了，而是最后一个重写规则处理的URL。 (6) C：此标记使当前规则与下一个(其本身又可以与其后继规则相链接的， 并可以如此反复的)规则相链接。 它产生这样一个效果: 如果一个规则被匹配，通常会继续处理其后继规则， 即，这个标记不起作用；如果规则不能被匹配，则其后继的链接的规则会被忽略。 (7) NC：忽略大小写。 (8) QSA：此标记强制重写引擎在已有的替换串中追加一个请求串，而不是简单的替换。 ","date":"2020-02-17","objectID":"/posts/apache%E9%87%8D%E5%AE%9A%E5%90%91/:0:2","tags":["Apache"],"title":"Apache重定向","uri":"/posts/apache%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Apache"],"content":"示例 1、强制https跳转 #开启Rewrite重写功能 RewriteEngine on #判断规则为端口非443结尾 RewriteCond %{SERVER_PORT} !^443$ #强制访问到https的相应路径，响应状态码是301永久重定向 RewriteRule ^(.*)?$ https://%{SERVER_NAME}%{REQUEST_URI} [L,R] 2、404页面 #开启Rewrite重写功能 RewriteEngine on #请求的文件名为一个常规文件但不存在，默认是AND，所以此处的flags不用设置 RewriteCond %{REQUEST_FILENAME} !-f #请求的文件名为一个目录但不存在 RewriteCond %{REQUEST_FILENAME} !-d #如果既不是常规文件，也不是常规目录，那么就跳转到404页面。 RewriteRule (.*) /404.htm 3、基于User-Agent的访问控制 #开启Rewrite重写功能 RewriteEngine on #判断客户端user-agent信息是否为IE浏览器 RewriteCond %{HTTP_USER_AGENT} ^.*MSIE.* [NC] #满足是IE浏览器，跳转到404页面，。表示访问任何URL RewriteRule . /404.html [L] 4、QSA RewriteEngine On RewriteRule /pages/(.+) /page.php?page=$1 假如现在访问/pages/88?count=1页面，只会映射到：/page.php?page=88。 因为默认条件下，不会获取到查询字符串部分，(.+)只能匹配到88。 RewriteEngine On RewriteRule /pages/(.+) /page.php?page=$1 [QSA] 访问/pages/88?count=1页面，将会映射到：/page.php?page=88\u0026count=1 （1）[QSA]和正则表达式的子表达式配合使用。 （2）88?count=1中的问号被更换为$1 5、伪静态 RewriteRule ^article-([0-9]+)-([0-9]+)\\.html$ portal.php?mod=view\u0026aid=$1\u0026page=$2\u0026%1 输入article-8342-1.html实质访问portal.php?mod=view\u0026aid=8342\u0026page=1 6、访问路径添加/结尾 #开启Rewrite重写功能 RewriteEngine on #如果当前路径不是完整的文件路径，则不生效 RewriteCond ${REQUEST_FILENAME} ! -f #如果请求路径是以/结尾，则不生效 RewriteCond ${REQUEST_URI} !(.*)/$ #在路径后加上/,返回状态码301 RewriteRule ^(.*)$ $1/ [L,R=301] 7、防盗链 RewriteEngine on RewriteCond %{HTTP_REFERER} !^http://DomainName/.*$ [NC] RewriteCond %{HTTP_REFERER} !^http://DomainName$ [NC] RewriteCond %{HTTP_REFERER} !^DomainName/.*$ [NC] ","date":"2020-02-17","objectID":"/posts/apache%E9%87%8D%E5%AE%9A%E5%90%91/:0:3","tags":["Apache"],"title":"Apache重定向","uri":"/posts/apache%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Linux"],"content":"CentOS7默认的内核版本是3.10，低版本内核无法使用高版本才适用的软件包，如bcc等。本文将介绍内核升级的两种方法并做出比较。 ","date":"2020-02-17","objectID":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/:0:0","tags":["Linux"],"title":"CentOS7内核升级","uri":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"方法比较 内核升级可以选用基于内核软件包的编译方式也可以选择用yum进行升级，前者运行时间长，并且需要消耗大量的磁盘空间，这一点对于个人学习下使用的云服务器来说很不友好，如果你剩余磁盘已经不够20G，很可能会导致编译终止。第二种方式使用yum安装，安装速度十分快捷，15分钟内就可以得到一个全新内核的linux服务器，所以个人使用更推荐后者。当然在磁盘不紧张的情况下，也可以选用前者练练手，但找kernel-devel包又会让人很头疼。 ","date":"2020-02-17","objectID":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/:0:1","tags":["Linux"],"title":"CentOS7内核升级","uri":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"编译方式升级内核 首先去linux内核官方网站 下载你想升级的内核版本，这里有三个主要的版本，mainline即为现在开发的主线版本，stable为稳定版本，longterm为长期支持版本。其中longterm是最为稳妥的选择。这里以4.14.170版本为列，并且假定为root用户，如果当前用户不是root用户可以su -切换到root用户或者在关键命令前加上sudo 安装开发软件包 yum groupinstall -y \"Development tools\" 下载安装包至服务器 mkdir /kernelupdate \u0026\u0026 wget -O /kernelupgrade/linux4.14.170.tar.xz https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.14.170.tar.xz 下载完成后解压 cd /kernelupdate \u0026\u0026 tar -xvf linux4.14.170.tar.xz 解压完成后进入目录通过make menuconfig来进行内核配置 配置完成选择save然后退出即可，此时会在目录下生成.config文件。或者拷贝当前内核的默认配置到解压目录 cp /boot/config-$(uname -r) .config 当然没了加快整个的编译过程。可以选择多核心同时编译，使用lscpu或者cat /proc/cpuinfo|grep \"cpu cores\"|wc -l来查看cpu核数 make -jn #n换成上一步查询到的cpu核数 接下来就是漫长的编译过程，便已完成后需要进一步编译及安装内核模块 make modules_install 最后就是把编译好的内核安装到系统中 make install 此步骤完成后请编辑/boot/grub2/grub.cfg将多余的内核选项删除，确保不会启动到旧内核 reboot 最后重启进入新系统就行 ","date":"2020-02-17","objectID":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/:0:2","tags":["Linux"],"title":"CentOS7内核升级","uri":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"YUM方式升级内核 导入elrepo的证书文件 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org 导入elrepo文件 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 查询可用的内核升级包 yum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list avaliable 结果如下： Ξ (bochs) ~ → yum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" list available Loaded plugins: fastestmirror, langpacks Repository epel is listed more than once in the configuration Loading mirror speeds from cached hostfile * elrepo-kernel: mirrors.tuna.tsinghua.edu.cn Available Packages kernel-lt-doc.noarch 4.4.213-1.el7.elrepo elrepo-kernel kernel-lt-headers.x86_64 4.4.213-1.el7.elrepo elrepo-kernel kernel-lt-devel.x86_64 4.4.213-1.el7.elrepo elrepo-kernel kernel-lt-tools.x86_64 4.4.213-1.el7.elrepo elrepo-kernel kernel-lt-tools-libs.x86_64 4.4.213-1.el7.elrepo elrepo-kernel kernel-lt-tools-libs-devel.x86_64 4.4.213-1.el7.elrepo elrepo-kernel kernel-ml.x86_64 5.5.2-1.el7.elrepo elrepo-kernel kernel-ml-devel.x86_64 5.5.2-1.el7.elrepo elrepo-kernel kernel-ml-doc.noarch 5.5.2-1.el7.elrepo elrepo-kernel kernel-ml-headers.x86_64 5.5.2-1.el7.elrepo elrepo-kernel kernel-ml-tools.x86_64 5.5.2-1.el7.elrepo elrepo-kernel kernel-ml-tools-libs.x86_64 5.5.2-1.el7.elrepo elrepo-kernel kernel-ml-tools-libs-devel.x86_64 5.5.2-1.el7.elrepo elrepo-kernel 这里的lt即为刚才提到的longterm版,ml为mainline版。这里我还是安装lt版本，顺便安装kernel-devel，美滋滋！ yum --disablerepo=\"*\" --enablerepo=\"elrepo-kernel\" install kernel-lt-* 安装完成后生成grub配置文件 grub2-mkconfig -o /boot/grub2/grub.cfg 最后编辑/boot/grub2/grub.cfg项，将其中的多余内核加载项删除并重启即可 最后是升级好的结果 Ξ (bochs) ~ → cat /etc/redhat-release CentOS Linux release 7.7.1908 (Core) Ξ (bochs) ~ → uname -r 4.4.213-1.el7.elrepo.x86_64 ","date":"2020-02-17","objectID":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/:0:3","tags":["Linux"],"title":"CentOS7内核升级","uri":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/"},{"categories":["Linux"],"content":"总结 对比上述两种内核升级方法，使用yum安装要简单许多，但yum安装许多内核的模块无法手动配置，这就导致很多的模块可能是用不了的，反正看情况选择合适的方法。最后，内核升级仅推荐在自己的服务器上升级，谨慎在生产服务器上操作！ ","date":"2020-02-17","objectID":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/:0:4","tags":["Linux"],"title":"CentOS7内核升级","uri":"/posts/centos7%E5%86%85%E6%A0%B8%E5%8D%87%E7%BA%A7/"},{"categories":["TroubleShooting"],"content":"学习zabbix报警媒介，尝试调用shell脚本来完成邮件的发送操作时，在触发动作后，报警邮件顺利发出，但我所在的邮箱却一直没有收到报警邮件。 ","date":"2020-02-02","objectID":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/:0:0","tags":["Monitoring","TroubleShooting"],"title":"记一次zabbix报警邮件无法接收的问题","uri":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["TroubleShooting"],"content":"现象 在测试多次后查看zabbix审计日志时发现zabbix均已成功发送邮件，但邮箱中一直没有收到报警邮件。 ","date":"2020-02-02","objectID":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/:0:1","tags":["Monitoring","TroubleShooting"],"title":"记一次zabbix报警邮件无法接收的问题","uri":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["TroubleShooting"],"content":"分析 在服务器上尝试调用mailx程序，邮件顺利发送并在邮箱中可以看到报警邮件。 Ξ (bochs) ~ → echo \"test content\" | mailx -s \"test title\" 17551094687@163.com Ξ (bochs) ~ → 服务器可以顺利的发送邮件，但为什么zabbix发送的邮件迟迟收不到？我转换了思路，不在使用最新版的4.4.5版本，改换成4.0LTS版本，发现问题依然存在，这时就能判断不是zabbix程序的问题。 继续想，会不会是zabbix没有调用到脚本呢，我在脚本中又加入了时间： #!/bin/bash export LANG=\"zh_CN.utf8\" echo `date \"+%Y-%m-%d %H:%M:%S\"` 进入脚本\u003e\u003e /tmp/mailx.log echo $1 echo $2 echo $3 SEND_TO=$1 SEND_SUBJECT=$2 FILE=/tmp/mailtmp.txt echo \"$3\" \u003e$FILE dos2unix -k $FILE /usr/bin/mailx -s \"$SEND_SUBJECT\" $SEND_TO \u003c $FILE echo `date \"+%Y-%m-%d %H:%M:%S\"` 退出脚本\u003e\u003e /tmp/mailx.log 运行测试并查看/tmp/mailx.log，发现zabbix顺利进入了脚本，并且脚本中的参数也正确传递了 2020-02-02 09:44:22 进入脚本 ************@163.com 故障PROBLEM,服务器:aliyun发生: 无法telnet zabbix_server的10051端口;宏变量函数测试:宏变量测试成功故障! 事件ID:93PROBLEM:1tzabbix server的10051端口:1宏变量函数测试:宏变量测试成功 2020-02-02 09:44:22 邮件成功发送 将脚本中的参数拿到命令行执行参数测试，发现邮件顺利发送。zabbix顺利进入了脚本但是没发送邮件，命令行确是能正常发送。 ","date":"2020-02-02","objectID":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/:0:2","tags":["Monitoring","TroubleShooting"],"title":"记一次zabbix报警邮件无法接收的问题","uri":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["TroubleShooting"],"content":"解决 重新梳理了一遍思路后推测，会不会是zabbix无权限调用mailx程序呢？为了验证这个问题，我先将zabbix的默认shell切换为/bin/bash,以使我能顺利的登录到zabbix用户 usermod -s /bin/bash zabbix su - zabbix #切换到zabbix用户 调用脚本进行测试发现重要的报错信息 -bash-4.2$ echo \"11\" | mailx -s \"22\" ############@163.com -bash-4.2$ Error initializing NSS: Unknown error -8015. . . . message not sent. 这个报错是之前在root账户时从未发生的报错，这时可以断定是权限问题了。查看mailx程序的权限是755，也就是说zabbix可以顺利的调用到mailx程序进行邮件的发送。 后来，终于找到了问题的所在，原来我的mailx配置文件/etc/mail.rc中的秘钥文件位于/root目录下 set from=\"###########@163.com\" set smtp=\"smtps://smtp.163.com:465\" set smtp-auth-user=\"###########@163.com\" set smtp-auth-password=\"############\" set smtp-auth=\"login\" set ssl-verify=\"ignore\" set nss-config-dir=\"/root/.certs\" 我们知道普通用户是肯定不能进入root用户的家目录的，这就导致了zabbix无法调用到证书，进而无法发送邮件的 将秘钥文件拷贝到其他的目录下，并将这个目录授予777权限，更改mailx的配置文件。 cp -R /root/.certs /etc/zabbix \u0026\u0026 chmod 777 /etc/zabbix/.certs 这时测试邮件可以顺利的发送，在尝试用zabbix触发报警邮件，报警邮件顺利发送，邮箱中也有报警邮件了。 最后，将zabbix的默认shell换回/sbin/nolgin,防止恶意利用 usermod -s /sbin/nologin zabbix ","date":"2020-02-02","objectID":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/:0:3","tags":["Monitoring","TroubleShooting"],"title":"记一次zabbix报警邮件无法接收的问题","uri":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["TroubleShooting"],"content":"总结 问题终于厘清，邮箱中顺利的接受到了邮件。追根溯源才发现是权限问题，下次有多用户调用的文件时应避免放到/root目录防止普通用户无法调用。 ","date":"2020-02-02","objectID":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/:0:4","tags":["Monitoring","TroubleShooting"],"title":"记一次zabbix报警邮件无法接收的问题","uri":"/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1zabbix%E6%8A%A5%E8%AD%A6%E9%82%AE%E4%BB%B6%E6%97%A0%E6%B3%95%E6%8E%A5%E6%94%B6%E7%9A%84%E9%97%AE%E9%A2%98/"},{"categories":["MySQL"],"content":"编译安装mysql8.0.18作为测试。顺便记录下安装过程。 ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:0","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"GCC版本 mysql8.0要求gcc版本要5.5以上，CentOS7默认的gcc版本为4.8.5，CentOS8默认gcc版本为8.1.0。为了方便，本次选用CentOS8.0安装mysql8.0。 ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:1","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"下载mysql8.0 为了方便，直接下载boost版本 wget https://cdn.mysql.com//Downloads/MySQL-8.0/mysql-boost-8.0.18.tar.gz #大小为185MB左右 ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:2","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"安装配套软件 yum install -y gcc gcc-c++ cmake openssl openssl-devel ncurses ncurses-devel libaio-devel ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:3","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"关于缺少rpcgen问题的解决 1）下载rpc.tar.gz 至/usr/include解压 2）下载rpcsvc-proto-1.4.tar.gz 执行以下操作 tar -xzvf rpcsvc-proto-1.4.tar.gz \u0026\u0026cd rpcsvc-proto-1.4 \u0026\u0026 ./configure \u0026\u0026 make \u0026\u0026 make install ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:4","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"编译安装 cmake -DCMAKE_INSTALL_PREFIX=/usr/local/mysql -DMYSQL_DATADIR=/data/mysql/ -DWITH_BOOST=boost -DFORCE_INSOURCE_BUILD=ON #安装路径为/usr/local/mysql 数据文件目录/data/mysql/ make \u0026\u0026 make install （make过程长达一小时左右） ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:5","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"增加配置文件 cat /etc/my.cnf [mysqld] server-id=1 port=3306 basedir=/usr/local/mysql datadir=/data/mysql #有其他需要可以另行添加 ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:6","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"权限修改 chown -R mysql:mysql /usr/local/mysql chown -R mysql:mysql /data/mysql chmod -R 755 /usr/local/mysql chmod -R 755 /data/mysql ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:7","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"mysql初始化 /usr/local/mysql/bin/mysqld --initialize --user=mysql --datadir=/data/mysql/ 此步骤执行完会产生一个临时的密码用于登录mysql，请保存此密码。 ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:8","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"启动并登录mysql /usr/local/mysql/bin/mysqld_safe --user=mysql \u0026 /usr/local/mysql/bin/mysql -u root -p\"初始密码\"; ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:9","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"修改密码 登录进mysql后执行show databases会提示修改密码。为了安全考虑。mysql5.7初始化完成会允许空密码登录，而mysql8.0会生成默认的密码进行登录 在mysql下执行 alter user 'root'@'localhost' identified by \"你要改的密码\"; ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:10","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["MySQL"],"content":"添加到启动 cp support-files/mysql.server /etc/init.d/ 可用选项: service mysql.server start service mysql.server stop service mysql.server restart service mysql.server reload #重载配置文件 service mysql.server status ","date":"2019-12-09","objectID":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/:0:11","tags":["MySQL"],"title":"CentOS编译安装MySQL8.0","uri":"/posts/centos%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql8.0/"},{"categories":["Linux"],"content":"vim编辑器是Linux系统自带的编辑器，其操作较一般的编辑器要复杂许多，但熟练运用vim会极大地提升Linux系统下编辑文本的速度。看完本文赶紧去试下吧！ ","date":"2019-11-18","objectID":"/posts/vim%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E8%AF%A6%E7%BB%86%E7%94%A8%E6%B3%95/:0:0","tags":["Linux"],"title":"Vim编辑器的详细用法","uri":"/posts/vim%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E8%AF%A6%E7%BB%86%E7%94%A8%E6%B3%95/"},{"categories":["Linux"],"content":"vim模式介绍 vim有三种模式，分别为命令模式(command mode)，编辑模式(insert mode)及末行模式(last line mode)。我们在刚进入vim编辑器时为命令模式，在按下a时会进入编辑模式，同时屏幕的左下角会显示INSERT，此时就可以输入你想要的文字。在完成输入操作后，按下Esc键会退回到命令模式，此时按下 : 就会进入末行模式,其实就是vim的命令行模式，在此模式按下wq就可以保存文件并退出了。 下文中命令前出现 : 均表示在末行模式中执行 ","date":"2019-11-18","objectID":"/posts/vim%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E8%AF%A6%E7%BB%86%E7%94%A8%E6%B3%95/:0:1","tags":["Linux"],"title":"Vim编辑器的详细用法","uri":"/posts/vim%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E8%AF%A6%E7%BB%86%E7%94%A8%E6%B3%95/"},{"categories":["Linux"],"content":"vim的基本操作 打开文件 打开单个文件或者新建单个文件: vim FILENAME 不保存对文章的改动重新编辑文件 = :q!+vim FILENAME: :e! 编辑文件1时不保存退出编辑文件2: :open FILENAME2 多文件 :sp file 新建窗口打开 filename 并水平分割窗口 :vsp file 新缓窗口打开 filename 并垂直分割窗口 Ctrl + ws 水平分割窗口 Ctrl + wv 垂直分割窗口 Ctrl + ww 在窗口间切换 Ctrl + wq 关闭窗口 Ctrl + wh 切换到右侧窗口 Ctrl + wl 切换到左侧窗口 Ctrl + wj 切换到下侧窗口 Ctrl + wk 切换到上侧窗口 :qall! 不保存退出所有窗口 插入 在命令模式下键入a，A，i，I，o，O均会使vim进入编辑模式，区别在于： i 在当前位置的前插入新的字符 I 在当前行的行首插入新的字符 a 在当前位置的后面插入新的字符 A 在当前行的行尾插入新的字符 o 在当前行的下面插入一个新行 O 在当前行的上面插入一个新行 查找 /pattern 向下搜索pattern ?pattern 向上搜索pattern vim中搜索特殊字符需要进行转义包括 . \\ * [] / ? ~ $ % ^ *等 光标选择某个单词,按住Shift+3，即可把所有的单词标注出来 n 切换下一个搜索结果 N 切换上一个搜索结果 :noh 取消搜索结果的高亮显示 替换命令 :s/old/new 用new去替换当前行匹配到的第一个old :%s/old/new 用new去替换全文各行的第一个old :s/old/new/g 用new替换当前行的所有old :s/old/new/g 将全文中的old用new替换 此外还可以指定行数进行替换： 如要替换11-23行所有的old，可以键入如下指令: :11,23 s/old/new/g 若要在23-35行所有行前加4个空格以示缩进，可以键入如下指令： :23,35 s/^/ /g 撤销与重做 u 撤销上一步操作 Ctrl+r 重做，即对撤销的撤销 删除 x 删除当前字符，即光标在哪个位置就删除那个位置的字符 5x 从当前光标开始往后删除5个字符 X 删除当前字符前的一个字符 5X 从当前光标往前删除5个字符（不包括当前字符） d$ 删除当前字符至行尾所有单词 d^ 删除当前字符至行首前的所有字符（不包括当前字符） dw 删除当前字符至单词尾部 daw 删除光标所在的单词 dd 删除光标所在行的整行 ndd 删除光标所在行及其下共5行 S 删除当前行并进入编辑模式 dgg 删除当前行至文档首部的所有行 效果等同于末行模式下的:1,.d dG 删除当前行至文档尾部的所有行 效果等同于末行模式下的:.,$d($表示最后一行) 末行模式下还可以进行更加复杂的操作而不需要考虑光标的位置 如删除第11行至34行的操作就可以这样 :11,34d 复制粘贴 yy 复制光标所在的当前行 5yy 复制光标及下面共5行 yG 复制当前行至文档开始的所有行 ygg 复制当前行至文档结束的所有行 yw 复制当前光标所在处的一个单词 J 将下一行内容移动到本行的末尾 同样的在末行模式下也可以跨行复制，如复制11至34行 :11,34y 把10至23行的内容移动到56行之后，可以使用 :10,23 m 56 使用p把dd删除或yy复制的内容粘贴出来 p 在当前行的下面粘贴 P 在当前行的上面粘贴 快速操作： ddp 快速交换当前行及下一行 yyp 快速复制当前行并粘贴 保存及退出 :wq 保存并退出 :x 保存并退出 :q 直接退出 :q! 强制退出不做更改 zz 保存并退出 移动 移动操作并不常用，毕竟↑↓←→键就可以完成而且无需记忆 h 光标向←移动1个字符 10h 光标向←移动10个字符 l 光标向→移动1个字符 10l 光标向→移动10个字符 j 光标向↑移动1个字符 10j 光标向↑移动10个字符 k 光标向↓移动1个字符 10k 光标向↓移动10个字符 gg 移动到文档头部，等同于:1 G 移动到文档尾部，等同于:$ 当然你也可以在打开文件时就指定行数，比如想打开文件时直接到最后一行，可以使用 vim + FILENAME 打开文件直接到哪一行 vim +n FILENAME #打开文件直接到第n行，而不用打开文件后在移动光标 可视化 v 进入可视化，左下角显示可视化或者VISUAL V 进入可视行，左下角显示可视行或者VISUAL LINE Ctrl+v 进入可视块，左下角显示可视块或者VISUAL BLOCK 上述选择使用上下左右键，非鼠标 d 删除选中文本 y 复制选中文本 gu 选中区域转为小写 gU 选中区域转为大写 g~ 大小写互调 \u003e 向右缩进一个单位 \u003c 向左缩进一个单位 其他操作 :!COMMAND执行命令，如:!ls会列出本文档当前目录的所有文件而不用退出文档 :suspend或Crtl-Z挂起vim，回到shell，可以使用fg返回vim编辑器 :set nu 设置行号，可将此命令写入vim的配置文件/etc/vimrc或者~/.vimrc，就可以保证每次打开都显示行数 :set ff=unix 设置文档的dos编码为unix风格，在vim打开乱码时可以尝试使用该命令修正 :set syntax=? 设置文档的语言为？，如c，python，sh :set syntax获得目前的文档语言 :set paste 解决复制文字来时缩进乱的问题 复杂注释 :3,10 s/^/#/g 注释3至5行 :3,10 s/^#//g 取消注释3至10行 :1,$ s/^/#/g 注释整个文档 :1,$ s/^#//g 取消注释整个文档 #全选整篇文档也可使用%来替换 ","date":"2019-11-18","objectID":"/posts/vim%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E8%AF%A6%E7%BB%86%E7%94%A8%E6%B3%95/:0:2","tags":["Linux"],"title":"Vim编辑器的详细用法","uri":"/posts/vim%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E8%AF%A6%E7%BB%86%E7%94%A8%E6%B3%95/"}]